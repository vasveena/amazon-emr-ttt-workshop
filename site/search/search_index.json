{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Amazon EMR Train-The-Trainer Workshop \u00b6 This workshop contains exercises for the 3-day Amazon EMR Workshop Training. Exercises \u00b6 Day 1 \u00b6 Launch EMR cluster with Managed Scaling and Fleets Run Spark ETLs and Spark structured streaming workloads EMR Studio Dive Deep Build transactional data lakes using Apache Hudi and Apache Iceberg Day 2 \u00b6 Orchestrate analytical workloads using EMR Studio and Amazon MWAA Run Spark workloads using Amazon EMR on EKS Run Spark and Hive workloads on EMR Serverless (preview) Day 3 \u00b6 Migration methodologies Run Hive batch ETLs and interactive queries Orchestrate analytical workloads using AWS Step Functions Ad-hoc querying with Presto Store and process data using HBase on S3 with migration tips Integrate with AWS LakeFormation and Apache Ranger","title":"Introduction"},{"location":"#welcome-to-amazon-emr-train-the-trainer-workshop","text":"This workshop contains exercises for the 3-day Amazon EMR Workshop Training.","title":"Welcome to Amazon EMR Train-The-Trainer Workshop"},{"location":"#exercises","text":"","title":"Exercises"},{"location":"#day-1","text":"Launch EMR cluster with Managed Scaling and Fleets Run Spark ETLs and Spark structured streaming workloads EMR Studio Dive Deep Build transactional data lakes using Apache Hudi and Apache Iceberg","title":"Day 1"},{"location":"#day-2","text":"Orchestrate analytical workloads using EMR Studio and Amazon MWAA Run Spark workloads using Amazon EMR on EKS Run Spark and Hive workloads on EMR Serverless (preview)","title":"Day 2"},{"location":"#day-3","text":"Migration methodologies Run Hive batch ETLs and interactive queries Orchestrate analytical workloads using AWS Step Functions Ad-hoc querying with Presto Store and process data using HBase on S3 with migration tips Integrate with AWS LakeFormation and Apache Ranger","title":"Day 3"},{"location":"setup/","text":"Setup \u00b6 Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). Enter the hash provided to you. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation template \"dayone\" is created.","title":"Setup"},{"location":"setup/#setup","text":"Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). Enter the hash provided to you. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation template \"dayone\" is created.","title":"Setup"},{"location":"day1/fleet/exercise/","text":"Exercise 1 \u2013 Launching an EMR cluster \u00b6 This exercise is meant to show you different options and features available while trying to create an EMR cluster. EMR clusters required are already created in your event engine accounts which we will use for our exercises. Create EMR Cluster with Instance Fleets \u00b6 Go to the EMR Web Console from AWS Management Console Click on \u201cCreate cluster\u201d Click on \u201cGo to advanced options\u201d Explore the options on all 4 Steps. Step 1: Software and Steps \u00b6 Choose latest release label: EMR 6.5.0. Look at the applications available. Choose Spark, Hive and Hue for example. You can choose Use multiple master nodes to improve cluster availability which will launch 3 X EMR Leader Nodes. You can use AWS Glue Data Catalog for Hive and Spark tables. Under Software Configurations, you can provide a JSON config to override default values. For example, you can use below JSON: [{ \"Classification\" : \"spark\" , \"Properties\" : { \"maximizeResourceAllocation\" : \"true\" } }, { \"classification\" : \"hive-site\" , \"properties\" : { \"hive.blobstore.use.output-committer\" : \"true\" } }, { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [{ \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_DATANODE_HEAPSIZE\" : \"2048\" , \"HADOOP_NAMENODE_OPTS\" : \"-XX:GCTimeRatio=19\" }, \"Configurations\" : [ ] }] } ] Select \"Run multiple steps at the same time to improve cluster utilization\". You can change the step concurrency as well which is defaulted to 10. You can submit a Spark step during cluster creation. But for now, we can leave it as is. We can also submit steps to a running EMR cluster. For transient or short-lived EMR clusters, you can add steps during cluster creation and choose to auto-terminate your clusters after last step completion using option \"Cluster auto-terminates\". For now, we can leave it at default \"Clusters enters waiting state\". Click on Next. Step 2: Hardware \u00b6 There are two types of Cluster Compositions: Uniform instance groups and Instance fleets. Uniform instance groups will allow you to provision only one instance type within a single node group. Also, this option will only look at a single subnet while provisioning clusters. Let's choose instance fleets which provide us more flexibility in terms of hardware configuration. Under Networking, leave it at default VPC and choose all the 6 EC2 subnets in that VPC. Allocation strategy option is an improved method of launching clusters with lowest-priced On-Demand instances and capacity-optimized Spot instances. This option is recommended for faster cluster provisioning, more accurate Spot instance allocation, and fewer Spot instance interruptions compared to default EMR instance fleet allocation. Select \"Apply allocation strategy\". Under Cluster Nodes and Instances, for leader node type, click on \"Add / remove instance types to fleet\" and choose m5.xlarge, m4.xlarge, c5.xlarge, c4.xlarge. Choose on-demand. For core node type, choose 8 on-demand units and 8 spot units and select m5.xlarge, m5.2xlarge, m4.xlarge, m4.2xlarge. Under Provisioning timeout after \"60\" minutes Spot unavailability, change 60 mins to 30 mins and choose \"Switch to On-Demand instances\" from the drop down. For task node type, choose 8 spot units and select r5.xlarge, r5.2xlarge, r4.xlarge, r4.2xlarge. You can choose up to 30 different instance types for each node type. Click on \"Enable cluster scaling\" and choose core and task units. Choose minimum=16, maximum=32, on-demand limit=16, maximum core node=16. You can also enable scaling after the cluster has been launched. Enabling auto-termination helps you save cost by terminating idle clusters. You can leave this enabled since we will not be using this cluster. You can change the EBS root volume. You can increase this value if you are installing many different applications on your cluster. For now, you do not need to change the value. Click on Next. Step 3: General Cluster Settings \u00b6 Choose a friendly name for your cluster. Keep logging, debugging and termination protection enabled. Add a type with key named \"type\" and value \"DEV\". You have the option to customize your EC2 AMI and specify the customized image during your cluster launch. This is especially useful for applying security patches or applying CIS/STIG compliance. For now, we can use default EC2 AMI for EMR. You can specify a custom bootstrap action to run a script on all your cluster nodes. For now, we can leave it empty. Click on Next. Step 4: Security \u00b6 Under Security Options, choose EC2 key pair \"ee-default-keypair\" which is the key pair we downloaded during event engine setup. You can define custom EMR Service IAM Role which will be used by the EMR control plane and custom EC2 IAM role to be assumed by all the nodes in your cluster. If you leave these values at default, the default IAM roles (EMR_DefaultRole and EMR_EC2_DefaultRole) will be automatically created during cluster creation. Leave it as is. You can create a new Security Configuration in your EMR Web Console and use it in the \"Security Configuration\" section. This is where you define encryption, authentication and authorization for your cluster. We will look into this in detail on Day 3. For now, you can leave it at default. You can provide custom EC2 security groups for your leader and worker node types. You can configure up to 5 security groups per node type. If you do not specify, default EC2 security groups will be automatically created for leader and worker node types. Click on \"Create Cluster\". After about 10 mins, you can observe that the EMR cluster is created and is in \"WAITING\" state. Check all the tabs to see the cluster configurations.","title":"1 - Launching EMR cluster with Instance Fleets"},{"location":"day1/fleet/exercise/#exercise-1-launching-an-emr-cluster","text":"This exercise is meant to show you different options and features available while trying to create an EMR cluster. EMR clusters required are already created in your event engine accounts which we will use for our exercises.","title":"Exercise 1 \u2013 Launching an EMR cluster"},{"location":"day1/fleet/exercise/#create-emr-cluster-with-instance-fleets","text":"Go to the EMR Web Console from AWS Management Console Click on \u201cCreate cluster\u201d Click on \u201cGo to advanced options\u201d Explore the options on all 4 Steps.","title":"Create EMR Cluster with Instance Fleets"},{"location":"day1/fleet/exercise/#step-1-software-and-steps","text":"Choose latest release label: EMR 6.5.0. Look at the applications available. Choose Spark, Hive and Hue for example. You can choose Use multiple master nodes to improve cluster availability which will launch 3 X EMR Leader Nodes. You can use AWS Glue Data Catalog for Hive and Spark tables. Under Software Configurations, you can provide a JSON config to override default values. For example, you can use below JSON: [{ \"Classification\" : \"spark\" , \"Properties\" : { \"maximizeResourceAllocation\" : \"true\" } }, { \"classification\" : \"hive-site\" , \"properties\" : { \"hive.blobstore.use.output-committer\" : \"true\" } }, { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [{ \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_DATANODE_HEAPSIZE\" : \"2048\" , \"HADOOP_NAMENODE_OPTS\" : \"-XX:GCTimeRatio=19\" }, \"Configurations\" : [ ] }] } ] Select \"Run multiple steps at the same time to improve cluster utilization\". You can change the step concurrency as well which is defaulted to 10. You can submit a Spark step during cluster creation. But for now, we can leave it as is. We can also submit steps to a running EMR cluster. For transient or short-lived EMR clusters, you can add steps during cluster creation and choose to auto-terminate your clusters after last step completion using option \"Cluster auto-terminates\". For now, we can leave it at default \"Clusters enters waiting state\". Click on Next.","title":"Step 1: Software and Steps"},{"location":"day1/fleet/exercise/#step-2-hardware","text":"There are two types of Cluster Compositions: Uniform instance groups and Instance fleets. Uniform instance groups will allow you to provision only one instance type within a single node group. Also, this option will only look at a single subnet while provisioning clusters. Let's choose instance fleets which provide us more flexibility in terms of hardware configuration. Under Networking, leave it at default VPC and choose all the 6 EC2 subnets in that VPC. Allocation strategy option is an improved method of launching clusters with lowest-priced On-Demand instances and capacity-optimized Spot instances. This option is recommended for faster cluster provisioning, more accurate Spot instance allocation, and fewer Spot instance interruptions compared to default EMR instance fleet allocation. Select \"Apply allocation strategy\". Under Cluster Nodes and Instances, for leader node type, click on \"Add / remove instance types to fleet\" and choose m5.xlarge, m4.xlarge, c5.xlarge, c4.xlarge. Choose on-demand. For core node type, choose 8 on-demand units and 8 spot units and select m5.xlarge, m5.2xlarge, m4.xlarge, m4.2xlarge. Under Provisioning timeout after \"60\" minutes Spot unavailability, change 60 mins to 30 mins and choose \"Switch to On-Demand instances\" from the drop down. For task node type, choose 8 spot units and select r5.xlarge, r5.2xlarge, r4.xlarge, r4.2xlarge. You can choose up to 30 different instance types for each node type. Click on \"Enable cluster scaling\" and choose core and task units. Choose minimum=16, maximum=32, on-demand limit=16, maximum core node=16. You can also enable scaling after the cluster has been launched. Enabling auto-termination helps you save cost by terminating idle clusters. You can leave this enabled since we will not be using this cluster. You can change the EBS root volume. You can increase this value if you are installing many different applications on your cluster. For now, you do not need to change the value. Click on Next.","title":"Step 2: Hardware"},{"location":"day1/fleet/exercise/#step-3-general-cluster-settings","text":"Choose a friendly name for your cluster. Keep logging, debugging and termination protection enabled. Add a type with key named \"type\" and value \"DEV\". You have the option to customize your EC2 AMI and specify the customized image during your cluster launch. This is especially useful for applying security patches or applying CIS/STIG compliance. For now, we can use default EC2 AMI for EMR. You can specify a custom bootstrap action to run a script on all your cluster nodes. For now, we can leave it empty. Click on Next.","title":"Step 3: General Cluster Settings"},{"location":"day1/fleet/exercise/#step-4-security","text":"Under Security Options, choose EC2 key pair \"ee-default-keypair\" which is the key pair we downloaded during event engine setup. You can define custom EMR Service IAM Role which will be used by the EMR control plane and custom EC2 IAM role to be assumed by all the nodes in your cluster. If you leave these values at default, the default IAM roles (EMR_DefaultRole and EMR_EC2_DefaultRole) will be automatically created during cluster creation. Leave it as is. You can create a new Security Configuration in your EMR Web Console and use it in the \"Security Configuration\" section. This is where you define encryption, authentication and authorization for your cluster. We will look into this in detail on Day 3. For now, you can leave it at default. You can provide custom EC2 security groups for your leader and worker node types. You can configure up to 5 security groups per node type. If you do not specify, default EC2 security groups will be automatically created for leader and worker node types. Click on \"Create Cluster\". After about 10 mins, you can observe that the EMR cluster is created and is in \"WAITING\" state. Check all the tabs to see the cluster configurations.","title":"Step 4: Security"},{"location":"day1/hudi/exercise/","text":"Exercise 4 - Apache Hudi on Amazon EMR \u00b6 In this exercise you will build incremental data lakes on EMR using Apache Hudi. You can build data lakes using Apache Hudi using Spark Datasource APIs, Hudi Deltastreamer utility and SparkSQL. You will also build a real-time live incremental data lake with Spark Structured Streaming + Amazon Managed Streaming for Apache Kafka (MSK) + Apache Hudi. In the previous EMR Studio exercise, we linked the Git repository in the Jupyter interface. We will continue to use the same repository to run these exercises. SSH into the EMR leader node of the cluster \"EMR-Spark-Hive-Presto\" or open a session using AWS Session Manager for the EMR leader node since we will be running a few commands directly on the leader node. Apache Hudi with Spark Datasource APIs \u00b6 Open the file workshop-repo -> files -> notebook -> apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb in the Jupyter. Make sure the Kernel is set to PySpark. All the instructions required to run the notebook are within the notebook itself. Download the file workshop-repo -> schema -> schema.avsc to your local desktop and upload this file into the following S3 location (replace \"youraccountID\" with your event engine AWS account ID): s3://mrworkshop-youraccountID-dayone/schema/schema.avsc Alternatively, you can run the following commands from the leader node of your EMR cluster. Replace \"youraccountID\" with your event engine AWS account ID. We will be using this schema AVRO file to run compaction on Merge-On-Read tables. sudo su hadoop cd ~ curl -o schema.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/schema.avsc aws s3 cp schema.avsc s3://mrworkshop-youraccountID-dayone/schema/schema.avsc Run the blocks of the notebook \"apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb\". Replace \"youraccountID\" in the S3 paths within the notebook with your AWS event engine account ID. Apache Hudi with SparkSQL DMLs \u00b6 From EMR 6.5.0, you can write Hudi datasets using simple SQL statements. Let's look at an example. From the EMR Studio workspace Jupyterlab session, go to workshop-repo -> files -> notebook -> apache-hudi-on-amazon-emr-dml.ipynb. Run all the blocks of this notebook. Replace \"youraccountID\" in the S3 paths within the notebook with your AWS event engine account ID. Detailed instructions are within the notebook. Apache Hudi with Spark Deltastreamer \u00b6 Hudi provides a utility called Deltastreamer for creating and manipulating Hudi datasets without the need to write any Spark code. For this activity, let us copy a few files to the S3 location. Run the following commands in your EMR leader node session created using Session Manager or SSH. sudo su hadoop cd ~ curl -o source-schema-json.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/source-schema-json.avsc curl -o target-schema-json.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/target-schema-json.avsc curl -o json-deltastreamer.properties https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/properties/json-deltastreamer.properties curl -o json-deltastreamer_upsert.properties https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/properties/json-deltastreamer_upsert.properties curl -o apache-hudi-on-amazon-emr-deltastreamer-python-demo.py https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/script/apache-hudi-on-amazon-emr-deltastreamer-python-demo.py Replace youraccountID with event engine AWS account ID in the files json-deltastreamer.properties, json-deltastreamer_upsert.properties and apache-hudi-on-amazon-emr-deltastreamer-python-demo.py. You can do so using sed command below. Replace 707263692290 with your event engine account ID. sed -i 's|youraccountID|707263692290|g' json-deltastreamer.properties sed -i 's|youraccountID|707263692290|g' json-deltastreamer_upsert.properties sed -i 's|youraccountID|707263692290|g' apache-hudi-on-amazon-emr-deltastreamer-python-demo.py Now, copy the four files to your S3 location. Replace youraccountID with event engine AWS account ID. aws s3 cp source-schema-json.avsc s3://mrworkshop-youraccountID-dayone/hudi-ds/config/ aws s3 cp target-schema-json.avsc s3://mrworkshop-youraccountID-dayone/hudi-ds/config/ aws s3 cp json-deltastreamer.properties s3://mrworkshop-youraccountID-dayone/hudi-ds/config/ aws s3 cp json-deltastreamer_upsert.properties s3://mrworkshop-youraccountID-dayone/hudi-ds/config/ Now let's generate some Fake data for the purpose of this workshop. We will use Faker library for that. Install Faker with the below command. pip3 install Faker pip3 install boto3 Run the Python program to generate Fake data under respective S3 locations. This takes a few minutes to complete. python3 apache-hudi-on-amazon-emr-deltastreamer-python-demo.py Once done, make sure the inputdata and update prefixes are populated with JSON data files. You can copy one file using \u201caws s3 cp\u201d on the EMR leader node session to inspect the data. Replace youraccountID with event engine AWS account ID. aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds/inputdata aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds/updates Copy the Hudi utilities bundle to HDFS. hadoop fs -copyFromLocal /usr/lib/hudi/hudi-utilities-bundle.jar hdfs:///user/hadoop/ Let's submit DeltaStreamer step to the EMR cluster. You can submit this step on EC2 JumpHost or leader node of EMR cluster \"EMR-Spark-Hive-Presto\". Since we have the EMR leader node session active, let us use it to run the command. Modify Add Steps Command for Bulk Insert Operation. Change the --cluster-id's value to your EMR cluster \"EMR-Spark-Hive-Presto\" cluster ID (Obtained from AWS Management Console -> Amazon EMR Console -> EMR-Spark-Hive-Presto -> Summary tab. Looks like j-XXXXXXXXX). Replace youraccountID with event engine AWS account ID. aws emr add - steps -- cluster - id j - XXXXXXXXX -- steps Type = Spark , Name = \"Deltastreamer COW - Bulk Insert\" , ActionOnFailure = CONTINUE , Args = [ -- jars , hdfs: ///user/hadoop/*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///user/hadoop/hudi-utilities-bundle.jar,--props,s3://mrworkshop-youraccountID-dayone/hudi-ds/config/json-deltastreamer.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://mrworkshop-707263692290-dayone/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,BULK_INSERT] --region us-east-1 You will get an EMR Step ID in return. You will see the corresponding Hudi Deltastreamer step being submitted to your cluster (AWS Management Console -> Amazon EMR Console -> EMR-Spark-Hive-Presto -> Steps). It will take about 2 minutes to complete. Check the S3 location for Hudi files. Replace youraccountID with event engine AWS account ID. aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1/ Let's go to the hive CLI on EMR leader node by typing \"hive\". Let's run the following command to create a table. Replace youraccountID with event engine AWS account ID. CREATE EXTERNAL TABLE `profile_cow` ( `_hoodie_commit_time` string , `_hoodie_commit_seqno` string , `_hoodie_record_key` string , `_hoodie_partition_path` string , `_hoodie_file_name` string , `Name` string , `phone` string , `job` string , `company` string , `ssn` string , `street_address` string , `dob` string , `email` string , `ts` string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 's3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1/' ; Select a record from this table and copy the value of hoodie_record_key and street_address to a notepad. select `_hoodie_commit_time` , `_hoodie_record_key` , street_address from profile_cow limit 1 ; Exit from hive. exit ; Now, let's do upsert operation with Hudi Deltastreamer. Change the --cluster-id's value to your EMR cluster \"EMR-Spark-Hive-Presto\" cluster ID (Obtained from AWS Management Console -> Amazon EMR Console -> EMR-Spark-Hive-Presto -> Summary tab. Looks like j-XXXXXXXXX). Replace youraccountID with event engine AWS account ID. aws emr add - steps -- cluster - id j - XXXXXXXXX -- steps Type = Spark , Name = \"Deltastreamer COW - Upsert\" , ActionOnFailure = CONTINUE , Args = [ -- jars , hdfs: ///user/hadoop/*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///user/hadoop/hudi-utilities-bundle.jar,--props,s3://mrworkshop-youraccountID-dayone/hudi-ds/config/json-deltastreamer_upsert.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,UPSERT] --region us-east-1 You will get an EMR Step ID in return. You will see the corresponding Hudi Deltastreamer step being submitted to your cluster (AWS Management Console -> Amazon EMR Console -> EMR-Spark-Hive-Presto -> Steps). Wait for the step to complete (~1 minute). Let us check the street_address for the same _hoodie_record_key. Run the following query in hive CLI on the EMR leader node. Replace value of \"_hoodie_record_key\" in the where clause with the one you obtained from previous select query. select `_hoodie_commit_time` , street_address from profile_cow where `_hoodie_record_key` = '00000b94-1500-4f10-bd10-d6393ba24643' ; Notice the change in commit time and street_address. Change Data Capture with Hudi Deltastreamer \u00b6 Go to RDS Web console and open the database that was created. Copy the endpoint of this database. Login to your EC2 JumpHost using Session Manager or SSH and run the following command to connect to your DB. Replace \"dbendpoint\" value with the endpoint you copied from the RDS console. sudo yum install - y mysql mysql - h dbendpoint - uadmin - pTest123 $ Once you are logged in to your database, run the following commands in the MySQL session to create a DB table. call mysql . rds_set_configuration ( 'binlog retention hours' , 24 ); create table dev . retail_transactions ( tran_id INT , tran_date DATE , store_id INT , store_city varchar ( 50 ), store_state char ( 2 ), item_code varchar ( 50 ), quantity INT , total FLOAT ); Once the table is created, run the below queries to insert data into this table. insert into dev.retail_transactions values(1,'2019-03-17',1,'CHICAGO','IL','XXXXXX',5,106.25); insert into dev.retail_transactions values(2,'2019-03-16',2,'NEW YORK','NY','XXXXXX',6,116.25); insert into dev.retail_transactions values(3,'2019-03-15',3,'SPRINGFIELD','IL','XXXXXX',7,126.25); insert into dev.retail_transactions values(4,'2019-03-17',4,'SAN FRANCISCO','CA','XXXXXX',8,136.25); insert into dev.retail_transactions values(5,'2019-03-11',1,'CHICAGO','IL','XXXXXX',9,146.25); insert into dev.retail_transactions values(6,'2019-03-18',1,'CHICAGO','IL','XXXXXX',10,156.25); insert into dev.retail_transactions values(7,'2019-03-14',2,'NEW YORK','NY','XXXXXX',11,166.25); insert into dev.retail_transactions values(8,'2019-03-11',1,'CHICAGO','IL','XXXXXX',12,176.25); insert into dev.retail_transactions values(9,'2019-03-10',4,'SAN FRANCISCO','CA','XXXXXX',13,186.25); insert into dev.retail_transactions values(10,'2019-03-13',1,'CHICAGO','IL','XXXXXX',14,196.25); insert into dev.retail_transactions values(11,'2019-03-14',5,'CHICAGO','IL','XXXXXX',15,106.25); insert into dev.retail_transactions values(12,'2019-03-15',6,'CHICAGO','IL','XXXXXX',16,116.25); insert into dev.retail_transactions values(13,'2019-03-16',7,'CHICAGO','IL','XXXXXX',17,126.25); insert into dev.retail_transactions values(14,'2019-03-16',7,'CHICAGO','IL','XXXXXX',17,126.25); commit; We will now use AWS DMS to start pushing this data to S3. Go to the DMS Web Console -> Endpoints -> hudidmsource. Check if the connection is successful. If not, test the connection again. Start the Database migration task hudiload. Once the task state changes from Running to \"Load complete, replication ongoing\", check the below S3 location for deposited files. Replace youraccountID with AWS event engine account ID. aws s3 ls s3://mrworkshop-dms-youraccountID-dayone/dmsdata/dev/retail_transactions/ Now login to the EMR leader node of the cluster \"EMR-Spark-Hive-Presto\" using Session Manager or SSH and run the following commands. Replace youraccountID with AWS event engine account ID. sudo su hadoop cd ~ aws s3 mv s3 : // mrworkshop - dms - youraccountID - dayone / dmsdata / dev / retail_transactions / s3 : // mrworkshop - dms - youraccountID - dayone / dmsdata / data - full / dev / retail_transactions / -- exclude \" * \" -- include \" LOAD*.parquet \" -- recursive With the full table dump available in the data-full S3 folder, we will now use the Hudi Deltastreamer utility on the EMR cluster to populate the Hudi dataset on S3. Run the following command directly on leader node. Replace youraccountID with AWS event engine account ID. spark - submit -- class org . apache . hudi . utilities . deltastreamer . HoodieDeltaStreamer \\ -- jars hdfs : /// user / hadoop /*. jar \\ -- master yarn -- deploy - mode client \\ -- conf spark . serializer = org . apache . spark . serializer . KryoSerializer \\ -- conf spark . sql . hive . convertMetastoreParquet = false \\ / usr / lib / hudi / hudi - utilities - bundle . jar \\ -- table - type COPY_ON_WRITE \\ -- source - ordering - field dms_received_ts \\ -- props s3 : // mrworkshop - dms - youraccountID - dayone / properties / dfs - source - retail - transactions - full . properties \\ -- source - class org . apache . hudi . utilities . sources . ParquetDFSSource \\ -- target - base - path s3 : // mrworkshop - dms - youraccountID - dayone / hudi / retail_transactions -- target - table hudiblogdb . retail_transactions \\ -- transformer - class org . apache . hudi . utilities . transform . SqlQueryBasedTransformer \\ -- payload - class org . apache . hudi . common . model . AWSDmsAvroPayload \\ -- schemaprovider - class org . apache . hudi . utilities . schema . FilebasedSchemaProvider \\ -- enable - hive - sync Once this job completes, check the Hudi table by logging into Spark SQL or Athena console. spark-sql --conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\" --conf \"spark.sql.hive.convertMetastoreParquet=false\" --jars hdfs:///user/hadoop/*.jar Run the query: select * from hudiblogdb.retail_transactions order by tran_id You should see the same data in the table as the MySQL database with a few columns added by Hudi deltastreamer. Now let's run some DML statements on our MySQL database and take these changes through to the Hudi dataset. Run the following commands in MySQL session from your EC2 JumpHost. insert into dev.retail_transactions values(15,'2022-03-22',7,'CHICAGO','IL','XXXXXX',17,126.25); update dev.retail_transactions set store_city='SPRINGFIELD' where tran_id=12; delete from dev.retail_transactions where tran_id=2; commit; Exit from the MySQL session. In a few minutes, you see a new .parquet file created under s3://mrworkshop-dms-youraccountID-dayone/dmsdata/dev/retail_transactions/ folder in the S3 bucket. CDC data is being captured by our DMS replication task. You can see the changes in the DMS replication task under \"Table statistics\". Now, lets take the incremental changes we made to Hudi. Run the following command on EMR leader node. Replace youraccountID with your AWS event engine account ID. spark - submit -- class org . apache . hudi . utilities . deltastreamer . HoodieDeltaStreamer \\ -- jars hdfs : /// user / hadoop /*. jar \\ -- master yarn -- deploy - mode client \\ -- conf spark . serializer = org . apache . spark . serializer . KryoSerializer \\ -- conf spark . sql . hive . convertMetastoreParquet = false \\ / usr / lib / hudi / hudi - utilities - bundle . jar \\ -- table - type COPY_ON_WRITE \\ -- source - ordering - field dms_received_ts \\ -- props s3 : // mrworkshop - dms - youraccountID - dayone / properties / dfs - source - retail - transactions - incremental . properties \\ -- source - class org . apache . hudi . utilities . sources . ParquetDFSSource \\ -- target - base - path s3 : // mrworkshop - dms - youraccountID - dayone / hudi / retail_transactions -- target - table hudiblogdb . retail_transactions \\ -- transformer - class org . apache . hudi . utilities . transform . SqlQueryBasedTransformer \\ -- payload - class org . apache . hudi . common . model . AWSDmsAvroPayload \\ -- schemaprovider - class org . apache . hudi . utilities . schema . FilebasedSchemaProvider \\ -- enable - hive - sync \\ -- checkpoint 0 Once finished, check the Hudi table by logging into Spark SQL or Athena console. spark-sql --conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\" --conf \"spark.sql.hive.convertMetastoreParquet=false\" --jars hdfs:///user/hadoop/*.jar Run the query: select * from hudiblogdb.retail_transactions order by tran_id You should see the changes you made in the MySQL table. Apache Hudi with Spark Structured Streaming \u00b6 This exercise will show how you can write real time Hudi data sets using Spark Structured Streaming. For this exercise, we will use real-time NYC Metro Subway data using MTA API . Keep the EMR Session Manager or SSH session active. In a new browser tab, create a new SSM session for EC2 instance \"JumpHost\" (or SSH into EC2 instance \"JumpHost\"). i.e., under the EC2 console select the EC2 instance with name \"JumpHost\". Click on \"Connect\" -> Session Manager -> Connect. Switch to EC2 user and go to home directory sudo su ec2-user cd ~ Run the following commands in EC2 instance to get the values of ZookeeperConnectString and BootstrapBrokerString. clusterArn=`aws kafka list-clusters --region us-east-1 | jq '.ClusterInfoList[0].ClusterArn'` echo $clusterArn bs=$(echo \"aws kafka get-bootstrap-brokers --cluster-arn ${ clusterArn } --region us-east-1\" | bash | jq '.BootstrapBrokerString') bs_update=$(echo $bs | sed \"s|,|\\',\\'|g\" | sed \"s|\\\"|'|g\") zs=$(echo \"aws kafka describe-cluster --cluster-arn $clusterArn \" --region us-east-1 | bash | jq '.ClusterInfo.ZookeeperConnectString') Create two Kafka topics. echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-topics.sh --create --zookeeper $zs --replication-factor 3 --partitions 1 --topic trip_update_topic\" | bash echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-topics.sh --create --zookeeper $zs --replication-factor 3 --partitions 1 --topic trip_status_topic\" | bash Install packages required by Kafka client on the JumpHost instance session (via SSH or AWS SSM). pip3 install protobuf pip3 install kafka-python pip3 install --upgrade gtfs-realtime-bindings pip3 install underground pip3 install pathlib pip3 install requests Run the below command to modify the bootstrap servers in the file train_arrival_producer.py on the JumpHost's /home/ec2-user/ directory. Change sudo sed -i \"s|'bootstrapserverstring'|$bs_update|g\" /home/ec2-user/train_arrival_producer.py Export API key on the same session. export MTA_API_KEY = UskS0iAsK06DtSffbgqNi8hlDvApPR833wydQAHG Run the Kafka producer client and terminate the process using Ctrl + C after 10 seconds. python3 train_arrival_producer.py You can verify that the Kafka topics are being written to using the following commands echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-console-consumer.sh --bootstrap-server $bs --topic trip_update_topic --from-beginning\" | bash After a few seconds exit using Ctrl + C. echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-console-consumer.sh --bootstrap-server $bs --topic trip_status_topic --from-beginning\" | bash After a few seconds exit using Ctrl + C. Now let's configure Spark consumer on EMR leader node using Session Manager or SSH. 9). SSH into the leader node of EMR cluster \"EMR-Spark-Hive-Presto\" (or use AWS Session Manager). Download Spark dependencies in EMR leader node session. sudo su hadoop cd ~ cd /usr/lib/spark/jars sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.12/3.0.1/spark-streaming-kafka-0-10_2.12-3.0.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.1/spark-sql-kafka-0-10_2.12-3.0.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.2.1/kafka-clients-2.2.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.12/3.0.1/spark-streaming-kafka-0-10-assembly_2.12-3.0.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar In same session, provide all access to all HDFS folders. You can scope access per user if desired. hdfs dfs -chmod 777 / Switch to the SSH/SSM session of EC2 instance \u201cJumpHost\u201d. Get the bootstrap string from the below command on the EC2 JumpHost session. echo $bs Example output: \"b-3.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092,b-1.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092,b-2.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092\" Copy your bootstrap servers output to a notepad. Run the Kafka producer program again and keep it running. python3 train_arrival_producer.py Now, go to the EMR Studio's JupyterLab workspace and open workshop-repo/files/notebook/amazon-emr-spark-streaming-apache-hudi-demo.ipynb. Make sure that \"Spark\" kernel is selected. Replace the broker string with the value of bootstrap servers you copied to your notepad. Replace youraccountID with event engine AWS account ID. Instructions are in the notebook. Once you have made the changes, run all the cell blocks. Spark streaming job runs every 30 seconds. You can increase the duration if you want to. Based on the time of the day you run this code, the results may vary. After sometime, query this table using hive or even Athena. Go to the AWS Web Console -> Athena -> Explore the query editor. Since this would be your first time using Athena console, you need to go to the Settings -> Manage and add your Query result location like -> s3://mrworkshop-youraccountID-dayone/athena/. Now, you can run the following queries once in every 2 minutes or so to see the live changes. You can also build live dashboards using Amazon Quicksight. select count(*) from hudi_trips_streaming_table; select tripId, numoffuturestops from hudi_trips_streaming_table; After inspecting, stop your Python Kafka producer on the EC2 JumpHost session using Ctrl + C. On EMR Studio workspace session, click on the stop icon to stop the Spark Structured Streaming job.","title":"4 - Apache Hudi on Amazon EMR"},{"location":"day1/hudi/exercise/#exercise-4-apache-hudi-on-amazon-emr","text":"In this exercise you will build incremental data lakes on EMR using Apache Hudi. You can build data lakes using Apache Hudi using Spark Datasource APIs, Hudi Deltastreamer utility and SparkSQL. You will also build a real-time live incremental data lake with Spark Structured Streaming + Amazon Managed Streaming for Apache Kafka (MSK) + Apache Hudi. In the previous EMR Studio exercise, we linked the Git repository in the Jupyter interface. We will continue to use the same repository to run these exercises. SSH into the EMR leader node of the cluster \"EMR-Spark-Hive-Presto\" or open a session using AWS Session Manager for the EMR leader node since we will be running a few commands directly on the leader node.","title":"Exercise 4 - Apache Hudi on Amazon EMR"},{"location":"day1/hudi/exercise/#apache-hudi-with-spark-datasource-apis","text":"Open the file workshop-repo -> files -> notebook -> apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb in the Jupyter. Make sure the Kernel is set to PySpark. All the instructions required to run the notebook are within the notebook itself. Download the file workshop-repo -> schema -> schema.avsc to your local desktop and upload this file into the following S3 location (replace \"youraccountID\" with your event engine AWS account ID): s3://mrworkshop-youraccountID-dayone/schema/schema.avsc Alternatively, you can run the following commands from the leader node of your EMR cluster. Replace \"youraccountID\" with your event engine AWS account ID. We will be using this schema AVRO file to run compaction on Merge-On-Read tables. sudo su hadoop cd ~ curl -o schema.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/schema.avsc aws s3 cp schema.avsc s3://mrworkshop-youraccountID-dayone/schema/schema.avsc Run the blocks of the notebook \"apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb\". Replace \"youraccountID\" in the S3 paths within the notebook with your AWS event engine account ID.","title":"Apache Hudi with Spark Datasource APIs"},{"location":"day1/hudi/exercise/#apache-hudi-with-sparksql-dmls","text":"From EMR 6.5.0, you can write Hudi datasets using simple SQL statements. Let's look at an example. From the EMR Studio workspace Jupyterlab session, go to workshop-repo -> files -> notebook -> apache-hudi-on-amazon-emr-dml.ipynb. Run all the blocks of this notebook. Replace \"youraccountID\" in the S3 paths within the notebook with your AWS event engine account ID. Detailed instructions are within the notebook.","title":"Apache Hudi with SparkSQL DMLs"},{"location":"day1/hudi/exercise/#apache-hudi-with-spark-deltastreamer","text":"Hudi provides a utility called Deltastreamer for creating and manipulating Hudi datasets without the need to write any Spark code. For this activity, let us copy a few files to the S3 location. Run the following commands in your EMR leader node session created using Session Manager or SSH. sudo su hadoop cd ~ curl -o source-schema-json.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/source-schema-json.avsc curl -o target-schema-json.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/target-schema-json.avsc curl -o json-deltastreamer.properties https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/properties/json-deltastreamer.properties curl -o json-deltastreamer_upsert.properties https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/properties/json-deltastreamer_upsert.properties curl -o apache-hudi-on-amazon-emr-deltastreamer-python-demo.py https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/script/apache-hudi-on-amazon-emr-deltastreamer-python-demo.py Replace youraccountID with event engine AWS account ID in the files json-deltastreamer.properties, json-deltastreamer_upsert.properties and apache-hudi-on-amazon-emr-deltastreamer-python-demo.py. You can do so using sed command below. Replace 707263692290 with your event engine account ID. sed -i 's|youraccountID|707263692290|g' json-deltastreamer.properties sed -i 's|youraccountID|707263692290|g' json-deltastreamer_upsert.properties sed -i 's|youraccountID|707263692290|g' apache-hudi-on-amazon-emr-deltastreamer-python-demo.py Now, copy the four files to your S3 location. Replace youraccountID with event engine AWS account ID. aws s3 cp source-schema-json.avsc s3://mrworkshop-youraccountID-dayone/hudi-ds/config/ aws s3 cp target-schema-json.avsc s3://mrworkshop-youraccountID-dayone/hudi-ds/config/ aws s3 cp json-deltastreamer.properties s3://mrworkshop-youraccountID-dayone/hudi-ds/config/ aws s3 cp json-deltastreamer_upsert.properties s3://mrworkshop-youraccountID-dayone/hudi-ds/config/ Now let's generate some Fake data for the purpose of this workshop. We will use Faker library for that. Install Faker with the below command. pip3 install Faker pip3 install boto3 Run the Python program to generate Fake data under respective S3 locations. This takes a few minutes to complete. python3 apache-hudi-on-amazon-emr-deltastreamer-python-demo.py Once done, make sure the inputdata and update prefixes are populated with JSON data files. You can copy one file using \u201caws s3 cp\u201d on the EMR leader node session to inspect the data. Replace youraccountID with event engine AWS account ID. aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds/inputdata aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds/updates Copy the Hudi utilities bundle to HDFS. hadoop fs -copyFromLocal /usr/lib/hudi/hudi-utilities-bundle.jar hdfs:///user/hadoop/ Let's submit DeltaStreamer step to the EMR cluster. You can submit this step on EC2 JumpHost or leader node of EMR cluster \"EMR-Spark-Hive-Presto\". Since we have the EMR leader node session active, let us use it to run the command. Modify Add Steps Command for Bulk Insert Operation. Change the --cluster-id's value to your EMR cluster \"EMR-Spark-Hive-Presto\" cluster ID (Obtained from AWS Management Console -> Amazon EMR Console -> EMR-Spark-Hive-Presto -> Summary tab. Looks like j-XXXXXXXXX). Replace youraccountID with event engine AWS account ID. aws emr add - steps -- cluster - id j - XXXXXXXXX -- steps Type = Spark , Name = \"Deltastreamer COW - Bulk Insert\" , ActionOnFailure = CONTINUE , Args = [ -- jars , hdfs: ///user/hadoop/*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///user/hadoop/hudi-utilities-bundle.jar,--props,s3://mrworkshop-youraccountID-dayone/hudi-ds/config/json-deltastreamer.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://mrworkshop-707263692290-dayone/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,BULK_INSERT] --region us-east-1 You will get an EMR Step ID in return. You will see the corresponding Hudi Deltastreamer step being submitted to your cluster (AWS Management Console -> Amazon EMR Console -> EMR-Spark-Hive-Presto -> Steps). It will take about 2 minutes to complete. Check the S3 location for Hudi files. Replace youraccountID with event engine AWS account ID. aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1/ Let's go to the hive CLI on EMR leader node by typing \"hive\". Let's run the following command to create a table. Replace youraccountID with event engine AWS account ID. CREATE EXTERNAL TABLE `profile_cow` ( `_hoodie_commit_time` string , `_hoodie_commit_seqno` string , `_hoodie_record_key` string , `_hoodie_partition_path` string , `_hoodie_file_name` string , `Name` string , `phone` string , `job` string , `company` string , `ssn` string , `street_address` string , `dob` string , `email` string , `ts` string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 's3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1/' ; Select a record from this table and copy the value of hoodie_record_key and street_address to a notepad. select `_hoodie_commit_time` , `_hoodie_record_key` , street_address from profile_cow limit 1 ; Exit from hive. exit ; Now, let's do upsert operation with Hudi Deltastreamer. Change the --cluster-id's value to your EMR cluster \"EMR-Spark-Hive-Presto\" cluster ID (Obtained from AWS Management Console -> Amazon EMR Console -> EMR-Spark-Hive-Presto -> Summary tab. Looks like j-XXXXXXXXX). Replace youraccountID with event engine AWS account ID. aws emr add - steps -- cluster - id j - XXXXXXXXX -- steps Type = Spark , Name = \"Deltastreamer COW - Upsert\" , ActionOnFailure = CONTINUE , Args = [ -- jars , hdfs: ///user/hadoop/*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///user/hadoop/hudi-utilities-bundle.jar,--props,s3://mrworkshop-youraccountID-dayone/hudi-ds/config/json-deltastreamer_upsert.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,UPSERT] --region us-east-1 You will get an EMR Step ID in return. You will see the corresponding Hudi Deltastreamer step being submitted to your cluster (AWS Management Console -> Amazon EMR Console -> EMR-Spark-Hive-Presto -> Steps). Wait for the step to complete (~1 minute). Let us check the street_address for the same _hoodie_record_key. Run the following query in hive CLI on the EMR leader node. Replace value of \"_hoodie_record_key\" in the where clause with the one you obtained from previous select query. select `_hoodie_commit_time` , street_address from profile_cow where `_hoodie_record_key` = '00000b94-1500-4f10-bd10-d6393ba24643' ; Notice the change in commit time and street_address.","title":"Apache Hudi with Spark Deltastreamer"},{"location":"day1/hudi/exercise/#change-data-capture-with-hudi-deltastreamer","text":"Go to RDS Web console and open the database that was created. Copy the endpoint of this database. Login to your EC2 JumpHost using Session Manager or SSH and run the following command to connect to your DB. Replace \"dbendpoint\" value with the endpoint you copied from the RDS console. sudo yum install - y mysql mysql - h dbendpoint - uadmin - pTest123 $ Once you are logged in to your database, run the following commands in the MySQL session to create a DB table. call mysql . rds_set_configuration ( 'binlog retention hours' , 24 ); create table dev . retail_transactions ( tran_id INT , tran_date DATE , store_id INT , store_city varchar ( 50 ), store_state char ( 2 ), item_code varchar ( 50 ), quantity INT , total FLOAT ); Once the table is created, run the below queries to insert data into this table. insert into dev.retail_transactions values(1,'2019-03-17',1,'CHICAGO','IL','XXXXXX',5,106.25); insert into dev.retail_transactions values(2,'2019-03-16',2,'NEW YORK','NY','XXXXXX',6,116.25); insert into dev.retail_transactions values(3,'2019-03-15',3,'SPRINGFIELD','IL','XXXXXX',7,126.25); insert into dev.retail_transactions values(4,'2019-03-17',4,'SAN FRANCISCO','CA','XXXXXX',8,136.25); insert into dev.retail_transactions values(5,'2019-03-11',1,'CHICAGO','IL','XXXXXX',9,146.25); insert into dev.retail_transactions values(6,'2019-03-18',1,'CHICAGO','IL','XXXXXX',10,156.25); insert into dev.retail_transactions values(7,'2019-03-14',2,'NEW YORK','NY','XXXXXX',11,166.25); insert into dev.retail_transactions values(8,'2019-03-11',1,'CHICAGO','IL','XXXXXX',12,176.25); insert into dev.retail_transactions values(9,'2019-03-10',4,'SAN FRANCISCO','CA','XXXXXX',13,186.25); insert into dev.retail_transactions values(10,'2019-03-13',1,'CHICAGO','IL','XXXXXX',14,196.25); insert into dev.retail_transactions values(11,'2019-03-14',5,'CHICAGO','IL','XXXXXX',15,106.25); insert into dev.retail_transactions values(12,'2019-03-15',6,'CHICAGO','IL','XXXXXX',16,116.25); insert into dev.retail_transactions values(13,'2019-03-16',7,'CHICAGO','IL','XXXXXX',17,126.25); insert into dev.retail_transactions values(14,'2019-03-16',7,'CHICAGO','IL','XXXXXX',17,126.25); commit; We will now use AWS DMS to start pushing this data to S3. Go to the DMS Web Console -> Endpoints -> hudidmsource. Check if the connection is successful. If not, test the connection again. Start the Database migration task hudiload. Once the task state changes from Running to \"Load complete, replication ongoing\", check the below S3 location for deposited files. Replace youraccountID with AWS event engine account ID. aws s3 ls s3://mrworkshop-dms-youraccountID-dayone/dmsdata/dev/retail_transactions/ Now login to the EMR leader node of the cluster \"EMR-Spark-Hive-Presto\" using Session Manager or SSH and run the following commands. Replace youraccountID with AWS event engine account ID. sudo su hadoop cd ~ aws s3 mv s3 : // mrworkshop - dms - youraccountID - dayone / dmsdata / dev / retail_transactions / s3 : // mrworkshop - dms - youraccountID - dayone / dmsdata / data - full / dev / retail_transactions / -- exclude \" * \" -- include \" LOAD*.parquet \" -- recursive With the full table dump available in the data-full S3 folder, we will now use the Hudi Deltastreamer utility on the EMR cluster to populate the Hudi dataset on S3. Run the following command directly on leader node. Replace youraccountID with AWS event engine account ID. spark - submit -- class org . apache . hudi . utilities . deltastreamer . HoodieDeltaStreamer \\ -- jars hdfs : /// user / hadoop /*. jar \\ -- master yarn -- deploy - mode client \\ -- conf spark . serializer = org . apache . spark . serializer . KryoSerializer \\ -- conf spark . sql . hive . convertMetastoreParquet = false \\ / usr / lib / hudi / hudi - utilities - bundle . jar \\ -- table - type COPY_ON_WRITE \\ -- source - ordering - field dms_received_ts \\ -- props s3 : // mrworkshop - dms - youraccountID - dayone / properties / dfs - source - retail - transactions - full . properties \\ -- source - class org . apache . hudi . utilities . sources . ParquetDFSSource \\ -- target - base - path s3 : // mrworkshop - dms - youraccountID - dayone / hudi / retail_transactions -- target - table hudiblogdb . retail_transactions \\ -- transformer - class org . apache . hudi . utilities . transform . SqlQueryBasedTransformer \\ -- payload - class org . apache . hudi . common . model . AWSDmsAvroPayload \\ -- schemaprovider - class org . apache . hudi . utilities . schema . FilebasedSchemaProvider \\ -- enable - hive - sync Once this job completes, check the Hudi table by logging into Spark SQL or Athena console. spark-sql --conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\" --conf \"spark.sql.hive.convertMetastoreParquet=false\" --jars hdfs:///user/hadoop/*.jar Run the query: select * from hudiblogdb.retail_transactions order by tran_id You should see the same data in the table as the MySQL database with a few columns added by Hudi deltastreamer. Now let's run some DML statements on our MySQL database and take these changes through to the Hudi dataset. Run the following commands in MySQL session from your EC2 JumpHost. insert into dev.retail_transactions values(15,'2022-03-22',7,'CHICAGO','IL','XXXXXX',17,126.25); update dev.retail_transactions set store_city='SPRINGFIELD' where tran_id=12; delete from dev.retail_transactions where tran_id=2; commit; Exit from the MySQL session. In a few minutes, you see a new .parquet file created under s3://mrworkshop-dms-youraccountID-dayone/dmsdata/dev/retail_transactions/ folder in the S3 bucket. CDC data is being captured by our DMS replication task. You can see the changes in the DMS replication task under \"Table statistics\". Now, lets take the incremental changes we made to Hudi. Run the following command on EMR leader node. Replace youraccountID with your AWS event engine account ID. spark - submit -- class org . apache . hudi . utilities . deltastreamer . HoodieDeltaStreamer \\ -- jars hdfs : /// user / hadoop /*. jar \\ -- master yarn -- deploy - mode client \\ -- conf spark . serializer = org . apache . spark . serializer . KryoSerializer \\ -- conf spark . sql . hive . convertMetastoreParquet = false \\ / usr / lib / hudi / hudi - utilities - bundle . jar \\ -- table - type COPY_ON_WRITE \\ -- source - ordering - field dms_received_ts \\ -- props s3 : // mrworkshop - dms - youraccountID - dayone / properties / dfs - source - retail - transactions - incremental . properties \\ -- source - class org . apache . hudi . utilities . sources . ParquetDFSSource \\ -- target - base - path s3 : // mrworkshop - dms - youraccountID - dayone / hudi / retail_transactions -- target - table hudiblogdb . retail_transactions \\ -- transformer - class org . apache . hudi . utilities . transform . SqlQueryBasedTransformer \\ -- payload - class org . apache . hudi . common . model . AWSDmsAvroPayload \\ -- schemaprovider - class org . apache . hudi . utilities . schema . FilebasedSchemaProvider \\ -- enable - hive - sync \\ -- checkpoint 0 Once finished, check the Hudi table by logging into Spark SQL or Athena console. spark-sql --conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\" --conf \"spark.sql.hive.convertMetastoreParquet=false\" --jars hdfs:///user/hadoop/*.jar Run the query: select * from hudiblogdb.retail_transactions order by tran_id You should see the changes you made in the MySQL table.","title":"Change Data Capture with Hudi Deltastreamer"},{"location":"day1/hudi/exercise/#apache-hudi-with-spark-structured-streaming","text":"This exercise will show how you can write real time Hudi data sets using Spark Structured Streaming. For this exercise, we will use real-time NYC Metro Subway data using MTA API . Keep the EMR Session Manager or SSH session active. In a new browser tab, create a new SSM session for EC2 instance \"JumpHost\" (or SSH into EC2 instance \"JumpHost\"). i.e., under the EC2 console select the EC2 instance with name \"JumpHost\". Click on \"Connect\" -> Session Manager -> Connect. Switch to EC2 user and go to home directory sudo su ec2-user cd ~ Run the following commands in EC2 instance to get the values of ZookeeperConnectString and BootstrapBrokerString. clusterArn=`aws kafka list-clusters --region us-east-1 | jq '.ClusterInfoList[0].ClusterArn'` echo $clusterArn bs=$(echo \"aws kafka get-bootstrap-brokers --cluster-arn ${ clusterArn } --region us-east-1\" | bash | jq '.BootstrapBrokerString') bs_update=$(echo $bs | sed \"s|,|\\',\\'|g\" | sed \"s|\\\"|'|g\") zs=$(echo \"aws kafka describe-cluster --cluster-arn $clusterArn \" --region us-east-1 | bash | jq '.ClusterInfo.ZookeeperConnectString') Create two Kafka topics. echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-topics.sh --create --zookeeper $zs --replication-factor 3 --partitions 1 --topic trip_update_topic\" | bash echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-topics.sh --create --zookeeper $zs --replication-factor 3 --partitions 1 --topic trip_status_topic\" | bash Install packages required by Kafka client on the JumpHost instance session (via SSH or AWS SSM). pip3 install protobuf pip3 install kafka-python pip3 install --upgrade gtfs-realtime-bindings pip3 install underground pip3 install pathlib pip3 install requests Run the below command to modify the bootstrap servers in the file train_arrival_producer.py on the JumpHost's /home/ec2-user/ directory. Change sudo sed -i \"s|'bootstrapserverstring'|$bs_update|g\" /home/ec2-user/train_arrival_producer.py Export API key on the same session. export MTA_API_KEY = UskS0iAsK06DtSffbgqNi8hlDvApPR833wydQAHG Run the Kafka producer client and terminate the process using Ctrl + C after 10 seconds. python3 train_arrival_producer.py You can verify that the Kafka topics are being written to using the following commands echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-console-consumer.sh --bootstrap-server $bs --topic trip_update_topic --from-beginning\" | bash After a few seconds exit using Ctrl + C. echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-console-consumer.sh --bootstrap-server $bs --topic trip_status_topic --from-beginning\" | bash After a few seconds exit using Ctrl + C. Now let's configure Spark consumer on EMR leader node using Session Manager or SSH. 9). SSH into the leader node of EMR cluster \"EMR-Spark-Hive-Presto\" (or use AWS Session Manager). Download Spark dependencies in EMR leader node session. sudo su hadoop cd ~ cd /usr/lib/spark/jars sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.12/3.0.1/spark-streaming-kafka-0-10_2.12-3.0.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.1/spark-sql-kafka-0-10_2.12-3.0.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.2.1/kafka-clients-2.2.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.12/3.0.1/spark-streaming-kafka-0-10-assembly_2.12-3.0.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar In same session, provide all access to all HDFS folders. You can scope access per user if desired. hdfs dfs -chmod 777 / Switch to the SSH/SSM session of EC2 instance \u201cJumpHost\u201d. Get the bootstrap string from the below command on the EC2 JumpHost session. echo $bs Example output: \"b-3.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092,b-1.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092,b-2.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092\" Copy your bootstrap servers output to a notepad. Run the Kafka producer program again and keep it running. python3 train_arrival_producer.py Now, go to the EMR Studio's JupyterLab workspace and open workshop-repo/files/notebook/amazon-emr-spark-streaming-apache-hudi-demo.ipynb. Make sure that \"Spark\" kernel is selected. Replace the broker string with the value of bootstrap servers you copied to your notepad. Replace youraccountID with event engine AWS account ID. Instructions are in the notebook. Once you have made the changes, run all the cell blocks. Spark streaming job runs every 30 seconds. You can increase the duration if you want to. Based on the time of the day you run this code, the results may vary. After sometime, query this table using hive or even Athena. Go to the AWS Web Console -> Athena -> Explore the query editor. Since this would be your first time using Athena console, you need to go to the Settings -> Manage and add your Query result location like -> s3://mrworkshop-youraccountID-dayone/athena/. Now, you can run the following queries once in every 2 minutes or so to see the live changes. You can also build live dashboards using Amazon Quicksight. select count(*) from hudi_trips_streaming_table; select tripId, numoffuturestops from hudi_trips_streaming_table; After inspecting, stop your Python Kafka producer on the EC2 JumpHost session using Ctrl + C. On EMR Studio workspace session, click on the stop icon to stop the Spark Structured Streaming job.","title":"Apache Hudi with Spark Structured Streaming"},{"location":"day1/icebrg/exercise/","text":"Exercise 5 - Apache Iceberg on Amazon EMR \u00b6 In this exercise you will build incremental data lakes on EMR using Apache Iceberg. You will learn about the most important features of Iceberg like schema evolution, time traveling and also S3 traffic scaling using Object Store File Layout . Apache Iceberg Features \u00b6 From the EMR Studio workspace Jupyterlab session, go to workshop-repo -> files -> notebook -> apache-iceberg-on-amazon-emr.ipynb. Run all the blocks of this notebook. Replace \"youraccountID\" in the S3 paths within the notebook with your AWS event engine account ID. Detailed instructions are within the notebook. Stop the session once you are done.","title":"5 - Apache Iceberg on Amazon EMR"},{"location":"day1/icebrg/exercise/#exercise-5-apache-iceberg-on-amazon-emr","text":"In this exercise you will build incremental data lakes on EMR using Apache Iceberg. You will learn about the most important features of Iceberg like schema evolution, time traveling and also S3 traffic scaling using Object Store File Layout .","title":"Exercise 5 - Apache Iceberg on Amazon EMR"},{"location":"day1/icebrg/exercise/#apache-iceberg-features","text":"From the EMR Studio workspace Jupyterlab session, go to workshop-repo -> files -> notebook -> apache-iceberg-on-amazon-emr.ipynb. Run all the blocks of this notebook. Replace \"youraccountID\" in the S3 paths within the notebook with your AWS event engine account ID. Detailed instructions are within the notebook. Stop the session once you are done.","title":"Apache Iceberg Features"},{"location":"day1/smstudio/exercise/","text":"Exercise 4 - Sagemaker Studio Integration with Amazon EMR \u00b6 Amazon Sagemaker provides native integration with Amazon EMR from Sagemaker Studio so that you can run data preparation tasks for your machine learning workloads using EMR from Sagemaker. Login to the Sagemaker Studio \u00b6 Go to the Amazon Sagemaker Web Console -> Get started -> Sagemaker Studio You should see the Sagemaker domain ready status. Launch the Sagemaker Studio from Launch app -> Studio. It will take about 2 minutes to initialize after which you will be taken to the Sagemaker Studio interface. Once you are in, carry on with rest of the steps. Explore EMR clusters in Sagemaker Studio \u00b6 Click on the icon and choose Clusters from the Sagemaker resources drop down. You will be able to see the EMR clusters. You can filter the EMR clusters and also create a new one with a cluster template created from AWS Service Catalog. Go to Clusters -> Create cluster. You will be able to see a template. When you select the template, it will show you the blueprint for your EMR cluster creation. For now, let's use our existing cluster. Connect to EMR cluster from Sagemaker Studio and run data processing jobs \u00b6 Go to Git repository section and click on Clone the repository. Specify the repository to clone: https://github.com/vasveena/amazon-emr-ttt-workshop.git Make sure that the repository is cloned. Go to Files section (folder icon on the left hand side pane). Go to the directory files -> notebook -> smstudio-pyspark-hive-sentiment-analysis.ipynb. Choose the SparkMagic Kernel when prompted and click \"Select\". It will take a few minutes for the kernel to initialize. Once the kernel starts, go to Cluster on the top right corner and choose the EMR cluster \"EMR-Spark-Hive-Presto\". When it asks for credential type, choose \"No credential\" and Connect. Now, a Spark application will be created. Now, you can run the remaining code blocks of the notebook which will perform data transformations and explorations using the EMR cluster and create and host an ML model using Sagemaker.","title":"** Exercise 4 - Sagemaker Studio Integration with Amazon EMR **"},{"location":"day1/smstudio/exercise/#exercise-4-sagemaker-studio-integration-with-amazon-emr","text":"Amazon Sagemaker provides native integration with Amazon EMR from Sagemaker Studio so that you can run data preparation tasks for your machine learning workloads using EMR from Sagemaker.","title":"Exercise 4 - Sagemaker Studio Integration with Amazon EMR"},{"location":"day1/smstudio/exercise/#login-to-the-sagemaker-studio","text":"Go to the Amazon Sagemaker Web Console -> Get started -> Sagemaker Studio You should see the Sagemaker domain ready status. Launch the Sagemaker Studio from Launch app -> Studio. It will take about 2 minutes to initialize after which you will be taken to the Sagemaker Studio interface. Once you are in, carry on with rest of the steps.","title":"Login to the Sagemaker Studio"},{"location":"day1/smstudio/exercise/#explore-emr-clusters-in-sagemaker-studio","text":"Click on the icon and choose Clusters from the Sagemaker resources drop down. You will be able to see the EMR clusters. You can filter the EMR clusters and also create a new one with a cluster template created from AWS Service Catalog. Go to Clusters -> Create cluster. You will be able to see a template. When you select the template, it will show you the blueprint for your EMR cluster creation. For now, let's use our existing cluster.","title":"Explore EMR clusters in Sagemaker Studio"},{"location":"day1/smstudio/exercise/#connect-to-emr-cluster-from-sagemaker-studio-and-run-data-processing-jobs","text":"Go to Git repository section and click on Clone the repository. Specify the repository to clone: https://github.com/vasveena/amazon-emr-ttt-workshop.git Make sure that the repository is cloned. Go to Files section (folder icon on the left hand side pane). Go to the directory files -> notebook -> smstudio-pyspark-hive-sentiment-analysis.ipynb. Choose the SparkMagic Kernel when prompted and click \"Select\". It will take a few minutes for the kernel to initialize. Once the kernel starts, go to Cluster on the top right corner and choose the EMR cluster \"EMR-Spark-Hive-Presto\". When it asks for credential type, choose \"No credential\" and Connect. Now, a Spark application will be created. Now, you can run the remaining code blocks of the notebook which will perform data transformations and explorations using the EMR cluster and create and host an ML model using Sagemaker.","title":"Connect to EMR cluster from Sagemaker Studio and run data processing jobs"},{"location":"day1/spark/exercise/","text":"Exercise 2 - Apache Spark on Amazon EMR \u00b6 Initial Setup \u00b6 SSH to the leader node of the EMR cluster \"EMR-Spark-Hive-Presto\". The key pair you downloaded in the Setup can be used to SSH via terminal or using an SSH client like Putty. To make it easy, ssm-agent has been installed on all EMR nodes via a bootstrap action so that you can use AWS Systems Manager to login to your client EC2 instance and EMR leader node. Go to AWS Management Console on your browser -> Amazon EMR Console -> Hardware Tab. You will see MASTER and CORE fleets. Click on the MASTER instance fleet id (looks like if-XXXXXXXXXX). Click on the EC2 instance ID (looks like i-xxxxxxxxxxx). It should take you to the EC2 management console. You will be navigated to the EC2 management console. Click on \u201cConnect\u201d and go to the tab \u201cSession Manager\u201d. Click on Connect. You will be navigated to the AWS Session Manager session. In the session, type the following commands to log in as hadoop user (default OS user for EMR). sudo su hadoop cd ~ You will use the same login method to log in to other EC2 instances in this workshop as well. Using spark-submit and spark-shell \u00b6 Once you are logged into the EMR cluster using SSH or SSM agent, type \u201cspark-shell\u201d on your EMR Leader Node. Once spark-shell opens and the Spark session object is created, run the commands below - import org.apache.spark.sql.types. { IntegerType , StringType , StructType , StructField , DoubleType } val schema = StructType ( Array ( StructField ( \"s_suppkey\" , IntegerType , true ), StructField ( \"s_name\" , StringType , true ), StructField ( \"s_address\" , StringType , true ), StructField ( \"s_nationkey\" , StringType , true ), StructField ( \"s_phone\" , StringType , true ), StructField ( \"s_acctbal\" , DoubleType , true ), StructField ( \"s_comment\" , StringType , true ))) val df = spark . read . option ( \"delimiter\" , \"|\" ) . schema ( schema ) . csv ( \"s3://redshift-downloads/TPC-H/3TB/supplier/\" ) df . show ( 5 ) val df2 = df . filter ( $ \"s_acctbal\" > lit ( 0.0 )) . withColumn ( \"randdom\" , lit ( \"insert random column\" )) df2 . show ( 5 ) You will see the results in the spark-shell session. Investigate Spark UI \u00b6 While the Spark session is still active, you can check the Spark UI. You will need to install AWS CLI and Session Manager plugin on your local desktop to do this. You will also need to update your PATH variable if it is not done automatically following this document. Otherwise you may get the error \"SessionManagerPlugin is not found\". Replace --target with your leader node instance ID in the following command. Replace the environmental variables with the values from the Team Dashboard. For Windows, you will need to use \"set\" instead of \"export\". export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > Run the below command. Replace target with your leader node instance ID aws ssm start - session -- target i - 00785e8946 b4ff636 -- document - name AWS - StartPortForwardingSession -- parameters '{ \"portNumber\" : [ \"18080\" ], \"localPortNumber\" : [ \"8158\" ]}' -- region us - east - 1 Following image shows the commands run in macOS terminal. 18080 is the Spark History Server Port and 8157 is the local port. Now open http://localhost:8158 in your browser. Click on \"Show incomplete applications\" -> App ID (for eg: application_1647720368860_0002). Check out all the tabs especially the SQL tab. Click on \"show\" (Spark action) in the SQL tab to see the query plan. Alternative approach - Local SSH tunneling \u00b6 Please note that with this approach, you cannot access YARN Resource Manager UI. You can access it via local port forwarding by running the following command in your local desktop's terminal or using Putty for Windows. Replace leaderNodePublicDNS with your leader node public DNS (obtained from EMR Web Console -> EMR-Spark-Hive-Presto -> Summary tab -> Master public DNS). ssh - i ~/ ee - default - keypair . pem - N - L 8157 : leaderNodePublicDNS : 8088 hadoop @leaderNodePublicDNS Now enter http://localhost:8157 on your browser to see the Resource Manager UI. You can use this method as well to access Spark UI (replace port 8088 in the above command with port 18080). Optimization exercise (Optional) \u00b6 Converting inefficient join types Run the below code block in spark-shell. Set driver memory to 4G while starting the shell. spark-shell --driver-memory=4G import org.apache.spark.sql.types._ val liSchema = StructType ( Array ( StructField ( \"l_orderkey\" , StringType , true ), StructField ( \"l_partkey\" , IntegerType , true ), StructField ( \"l_suppkey\" , IntegerType , true ), StructField ( \"l_linenumber\" , IntegerType , true ), StructField ( \"l_quantity\" , IntegerType , true ), StructField ( \"l_extendedprice\" , IntegerType , true ), StructField ( \"l_discount\" , DoubleType , true ), StructField ( \"l_tax\" , DoubleType , true ), StructField ( \"l_returnflag\" , StringType , true ), StructField ( \"l_linestatus\" , StringType , true ), StructField ( \"l_shipdate\" , StringType , true ), StructField ( \"l_commitdate\" , StringType , true ), StructField ( \"l_receiptdate\" , StringType , true ), StructField ( \"l_shipinstruct\" , StringType , true ), StructField ( \"l_shipmode\" , StringType , true ), StructField ( \"l_comment\" , StringType , true ) ) ) val orSchema = StructType ( Array ( StructField ( \"o_orderkey\" , StringType , true ), StructField ( \"o_custkey\" , IntegerType , true ), StructField ( \"o_orderstatus\" , StringType , true ), StructField ( \"o_totalprice\" , DoubleType , true ), StructField ( \"o_orderdate\" , StringType , true ), StructField ( \"o_orderpriority\" , StringType , true ), StructField ( \"o_clerk\" , StringType , true ), StructField ( \"o_shippriority\" , IntegerType , true ), StructField ( \"o_comment\" , StringType , true ) ) ) val df1 = spark . read . schema ( liSchema ) . option ( \"delimiter\" , \"|\" ) . csv ( \"s3://redshift-downloads/TPC-H/2.18/3TB/lineitem/\" ) val df2 = spark . read . schema ( orSchema ) . option ( \"delimiter\" , \"|\" ) . csv ( \"s3://redshift-downloads/TPC-H/2.18/3TB/orders/\" ) val lineitem = df1 . limit ( 1000 ) val orders = df2 . limit ( 1000 ) val nestedLoopDF = lineitem . join ( orders , lineitem ( \"l_orderkey\" ) === orders ( \"o_orderkey\" ) || lineitem ( \"l_receiptdate\" ) === orders ( \"o_orderdate\" )) nestedLoopDF . show ( 5 , truncate = false ) Check the Spark UI -> SQL tab -> show (Spark action). You will be able to see that the above code uses BroadcastNestedLoopJoin as the join type. BroadcastNestedLoopJoin is an inefficient join that results from bad coding practice. Convert it into SortMergeJoin or BroadcastJoin by changing the code. val result1 = lineitem . join ( orders , lineitem ( \" l_orderkey \" ) === orders ( \" o_orderkey \" )) val result2 = lineitem . join ( orders , lineitem ( \" l_receiptdate \" ) === orders ( \" o_orderdate \" )) val broadcastDF = result1 . union ( result2 ) broadcastDF . show ( 5 , truncate = false ) Check the Spark UI now. You will be able to see that BroadcastHashJoin is being used instead which is the best join type if at least one of the two tables you are going to join is relatively small (<50MB). Default spark.sql.autoBroadcastJoinThreshold is 10 MB. Submit Spark Work to EMR using AddSteps API \u00b6 Let us submit Spark work to the cluster using EMR\u2019s AddSteps API. Copy the EMR Cluster ID in the Summary Tab of your EMR cluster \"EMR-Spark-Hive-Presto\" from EMR Web Console. It looks like \u2018j-XXXXXXXXXX\u2019. You can submit steps to your EMR from your local desktop after exporting the AWS credentials in your Team Dashboard page. For windows, use set instead of export. export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > Run the below command. Replace cluster-id value with your cluster ID. aws emr add - steps -- cluster - id j - XXXXXXXXXX -- steps Name = \"Spark Pi Job\" , Jar = command - runner . jar , Args = [ spark - submit , -- master , yarn , -- num - executors , 2 , -- class , org . apache . spark . examples . SparkPi , / usr / lib / spark / examples / jars / spark - examples . jar , 10 , - v ] -- region us - east - 1 If you cannot use AWS CLI for some reason, you will find an EC2 instance called \"JumpHost\" in the EC2 Web Console. In real life scenario, you can run this command from any machine as long as your IAM user or role has IAM access to invoke EMR AddSteps API. You can connect to that instance using Session Manager and submit step to EMR cluster from that session. Once connected, enter following commands to login as ec2-user (default OS user for EC2 instances). sudo su ec2-user cd ~ Now, run the AddSteps CLI command below. Replace cluster-id value with your cluster ID. You do not need to export any credentials since the IAM role attached to this JumpHost has all accesses required. aws emr add - steps -- cluster - id j - 142 PVKGDZTTXS -- steps Name = \"Spark Pi Job\" , Jar = command - runner . jar , Args = [ spark - submit , -- master , yarn , -- num - executors , 2 , -- class , org . apache . spark . examples . SparkPi , / usr / lib / spark / examples / jars / spark - examples . jar , 10 , - v ] -- region us - east - 1 Now, check the EMR step that was submitted to the cluster. You can look into the stdout logs to see the output.","title":"2 - Apache Spark on Amazon EMR"},{"location":"day1/spark/exercise/#exercise-2-apache-spark-on-amazon-emr","text":"","title":"Exercise 2 - Apache Spark on Amazon EMR"},{"location":"day1/spark/exercise/#initial-setup","text":"SSH to the leader node of the EMR cluster \"EMR-Spark-Hive-Presto\". The key pair you downloaded in the Setup can be used to SSH via terminal or using an SSH client like Putty. To make it easy, ssm-agent has been installed on all EMR nodes via a bootstrap action so that you can use AWS Systems Manager to login to your client EC2 instance and EMR leader node. Go to AWS Management Console on your browser -> Amazon EMR Console -> Hardware Tab. You will see MASTER and CORE fleets. Click on the MASTER instance fleet id (looks like if-XXXXXXXXXX). Click on the EC2 instance ID (looks like i-xxxxxxxxxxx). It should take you to the EC2 management console. You will be navigated to the EC2 management console. Click on \u201cConnect\u201d and go to the tab \u201cSession Manager\u201d. Click on Connect. You will be navigated to the AWS Session Manager session. In the session, type the following commands to log in as hadoop user (default OS user for EMR). sudo su hadoop cd ~ You will use the same login method to log in to other EC2 instances in this workshop as well.","title":"Initial Setup"},{"location":"day1/spark/exercise/#using-spark-submit-and-spark-shell","text":"Once you are logged into the EMR cluster using SSH or SSM agent, type \u201cspark-shell\u201d on your EMR Leader Node. Once spark-shell opens and the Spark session object is created, run the commands below - import org.apache.spark.sql.types. { IntegerType , StringType , StructType , StructField , DoubleType } val schema = StructType ( Array ( StructField ( \"s_suppkey\" , IntegerType , true ), StructField ( \"s_name\" , StringType , true ), StructField ( \"s_address\" , StringType , true ), StructField ( \"s_nationkey\" , StringType , true ), StructField ( \"s_phone\" , StringType , true ), StructField ( \"s_acctbal\" , DoubleType , true ), StructField ( \"s_comment\" , StringType , true ))) val df = spark . read . option ( \"delimiter\" , \"|\" ) . schema ( schema ) . csv ( \"s3://redshift-downloads/TPC-H/3TB/supplier/\" ) df . show ( 5 ) val df2 = df . filter ( $ \"s_acctbal\" > lit ( 0.0 )) . withColumn ( \"randdom\" , lit ( \"insert random column\" )) df2 . show ( 5 ) You will see the results in the spark-shell session.","title":"Using spark-submit and spark-shell"},{"location":"day1/spark/exercise/#investigate-spark-ui","text":"While the Spark session is still active, you can check the Spark UI. You will need to install AWS CLI and Session Manager plugin on your local desktop to do this. You will also need to update your PATH variable if it is not done automatically following this document. Otherwise you may get the error \"SessionManagerPlugin is not found\". Replace --target with your leader node instance ID in the following command. Replace the environmental variables with the values from the Team Dashboard. For Windows, you will need to use \"set\" instead of \"export\". export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > Run the below command. Replace target with your leader node instance ID aws ssm start - session -- target i - 00785e8946 b4ff636 -- document - name AWS - StartPortForwardingSession -- parameters '{ \"portNumber\" : [ \"18080\" ], \"localPortNumber\" : [ \"8158\" ]}' -- region us - east - 1 Following image shows the commands run in macOS terminal. 18080 is the Spark History Server Port and 8157 is the local port. Now open http://localhost:8158 in your browser. Click on \"Show incomplete applications\" -> App ID (for eg: application_1647720368860_0002). Check out all the tabs especially the SQL tab. Click on \"show\" (Spark action) in the SQL tab to see the query plan.","title":"Investigate Spark UI"},{"location":"day1/spark/exercise/#alternative-approach-local-ssh-tunneling","text":"Please note that with this approach, you cannot access YARN Resource Manager UI. You can access it via local port forwarding by running the following command in your local desktop's terminal or using Putty for Windows. Replace leaderNodePublicDNS with your leader node public DNS (obtained from EMR Web Console -> EMR-Spark-Hive-Presto -> Summary tab -> Master public DNS). ssh - i ~/ ee - default - keypair . pem - N - L 8157 : leaderNodePublicDNS : 8088 hadoop @leaderNodePublicDNS Now enter http://localhost:8157 on your browser to see the Resource Manager UI. You can use this method as well to access Spark UI (replace port 8088 in the above command with port 18080).","title":"Alternative approach - Local SSH tunneling"},{"location":"day1/spark/exercise/#optimization-exercise-optional","text":"Converting inefficient join types Run the below code block in spark-shell. Set driver memory to 4G while starting the shell. spark-shell --driver-memory=4G import org.apache.spark.sql.types._ val liSchema = StructType ( Array ( StructField ( \"l_orderkey\" , StringType , true ), StructField ( \"l_partkey\" , IntegerType , true ), StructField ( \"l_suppkey\" , IntegerType , true ), StructField ( \"l_linenumber\" , IntegerType , true ), StructField ( \"l_quantity\" , IntegerType , true ), StructField ( \"l_extendedprice\" , IntegerType , true ), StructField ( \"l_discount\" , DoubleType , true ), StructField ( \"l_tax\" , DoubleType , true ), StructField ( \"l_returnflag\" , StringType , true ), StructField ( \"l_linestatus\" , StringType , true ), StructField ( \"l_shipdate\" , StringType , true ), StructField ( \"l_commitdate\" , StringType , true ), StructField ( \"l_receiptdate\" , StringType , true ), StructField ( \"l_shipinstruct\" , StringType , true ), StructField ( \"l_shipmode\" , StringType , true ), StructField ( \"l_comment\" , StringType , true ) ) ) val orSchema = StructType ( Array ( StructField ( \"o_orderkey\" , StringType , true ), StructField ( \"o_custkey\" , IntegerType , true ), StructField ( \"o_orderstatus\" , StringType , true ), StructField ( \"o_totalprice\" , DoubleType , true ), StructField ( \"o_orderdate\" , StringType , true ), StructField ( \"o_orderpriority\" , StringType , true ), StructField ( \"o_clerk\" , StringType , true ), StructField ( \"o_shippriority\" , IntegerType , true ), StructField ( \"o_comment\" , StringType , true ) ) ) val df1 = spark . read . schema ( liSchema ) . option ( \"delimiter\" , \"|\" ) . csv ( \"s3://redshift-downloads/TPC-H/2.18/3TB/lineitem/\" ) val df2 = spark . read . schema ( orSchema ) . option ( \"delimiter\" , \"|\" ) . csv ( \"s3://redshift-downloads/TPC-H/2.18/3TB/orders/\" ) val lineitem = df1 . limit ( 1000 ) val orders = df2 . limit ( 1000 ) val nestedLoopDF = lineitem . join ( orders , lineitem ( \"l_orderkey\" ) === orders ( \"o_orderkey\" ) || lineitem ( \"l_receiptdate\" ) === orders ( \"o_orderdate\" )) nestedLoopDF . show ( 5 , truncate = false ) Check the Spark UI -> SQL tab -> show (Spark action). You will be able to see that the above code uses BroadcastNestedLoopJoin as the join type. BroadcastNestedLoopJoin is an inefficient join that results from bad coding practice. Convert it into SortMergeJoin or BroadcastJoin by changing the code. val result1 = lineitem . join ( orders , lineitem ( \" l_orderkey \" ) === orders ( \" o_orderkey \" )) val result2 = lineitem . join ( orders , lineitem ( \" l_receiptdate \" ) === orders ( \" o_orderdate \" )) val broadcastDF = result1 . union ( result2 ) broadcastDF . show ( 5 , truncate = false ) Check the Spark UI now. You will be able to see that BroadcastHashJoin is being used instead which is the best join type if at least one of the two tables you are going to join is relatively small (<50MB). Default spark.sql.autoBroadcastJoinThreshold is 10 MB.","title":"Optimization exercise (Optional)"},{"location":"day1/spark/exercise/#submit-spark-work-to-emr-using-addsteps-api","text":"Let us submit Spark work to the cluster using EMR\u2019s AddSteps API. Copy the EMR Cluster ID in the Summary Tab of your EMR cluster \"EMR-Spark-Hive-Presto\" from EMR Web Console. It looks like \u2018j-XXXXXXXXXX\u2019. You can submit steps to your EMR from your local desktop after exporting the AWS credentials in your Team Dashboard page. For windows, use set instead of export. export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > Run the below command. Replace cluster-id value with your cluster ID. aws emr add - steps -- cluster - id j - XXXXXXXXXX -- steps Name = \"Spark Pi Job\" , Jar = command - runner . jar , Args = [ spark - submit , -- master , yarn , -- num - executors , 2 , -- class , org . apache . spark . examples . SparkPi , / usr / lib / spark / examples / jars / spark - examples . jar , 10 , - v ] -- region us - east - 1 If you cannot use AWS CLI for some reason, you will find an EC2 instance called \"JumpHost\" in the EC2 Web Console. In real life scenario, you can run this command from any machine as long as your IAM user or role has IAM access to invoke EMR AddSteps API. You can connect to that instance using Session Manager and submit step to EMR cluster from that session. Once connected, enter following commands to login as ec2-user (default OS user for EC2 instances). sudo su ec2-user cd ~ Now, run the AddSteps CLI command below. Replace cluster-id value with your cluster ID. You do not need to export any credentials since the IAM role attached to this JumpHost has all accesses required. aws emr add - steps -- cluster - id j - 142 PVKGDZTTXS -- steps Name = \"Spark Pi Job\" , Jar = command - runner . jar , Args = [ spark - submit , -- master , yarn , -- num - executors , 2 , -- class , org . apache . spark . examples . SparkPi , / usr / lib / spark / examples / jars / spark - examples . jar , 10 , - v ] -- region us - east - 1 Now, check the EMR step that was submitted to the cluster. You can look into the stdout logs to see the output.","title":"Submit Spark Work to EMR using AddSteps API"},{"location":"day1/studio/exercise/","text":"Exercise 3 - Amazon EMR Studio \u00b6 Log in to EMR Studio \u00b6 In this exercise we will run Spark workflows using EMR Studio with managed Jupyter-based notebooks. We will also cover the most standout features of Amazon EMR Studio. Go to the EMR Web Console and navigate to \"EMR Studio\" on the right hand side. Click on \"Get Started\". You will be able to see an EMR Studio created called \"workshop-studio\". Click on it and in the following page, copy the Studio URL. Open an incognito or a private browser and paste the URL. In the AWS login page, choose \"IAM User\" and enter the account ID retrieved from your event engine's AWS Web console. Click on Next. Under IAM user name, enter \"studiouser\". Under password, enter Test123$. Click on Sign in. You will be logged into the EMR Studio. Users can access this interface without requiring AWS Web Console access. EMR Studio supports both IAM and SSO auth modes. Check EMR clusters from EMR Studio \u00b6 Check the clusters under EMR on EC2. You can filter the clusters. Click on \"EMR-Spark-Hive-Presto\" and go to \"Launch application UI -> Spark History Server\". You will be taken to the EMR Persistent Spark History Server. You can also see the UIs of terminated clusters for up to 60 days after termination. Create a Studio Workspace \u00b6 Go to Workspaces and \"Create Workspace\". Enter a workspace name. For example: \"studio-ws\". Enable \"Allow Workspace Collaboration\". Under \"Advanced Configuration\", select \"Attach Workspace to an EMR cluster\". In the drop down, choose the EMR-Spark-Hive-Presto cluster. Click \"Create Workspace\". It will take about 2 minutes for the Status to change to \"Attached\". Click on the workspace and it will open a managed JupyterLab session. You may need to allow pop-up from this address in your browser to open the JupyterLab. Once opened, you can create a Jupyter notebook with any kernel. Explore EMR Studio Workspace Features \u00b6 Cluster \u00b6 Under cluster tab, check the cluster attachment. Note that you will be able to detach and attach this workspace to a different cluster. For now, you can leave it as is. Git repository \u00b6 Under Git tab, you can add a Git repository by entering the repository name, URL and credentials. You can access public repositories without any credentials. Repository name: workshop-repo Git repository URL: https://github.com/vasveena/amazon-emr-ttt-workshop Branch: main Git credentials: Use a public repository without credentials Once the repository is added, select it from the \"Git repositories\" drop down. You will see that the Git repository will be linked successfully. Once its linked, you can go back to the workspace folder. You will find a folder called \"workshop-repo\". Go to workshop-repo -> files -> notebook to see the notebooks. If you are not able to link repository successfully, download amazon_reviews.ipynb and find_best_sellers.ipynb files to your local desktop (Right click -> Save Link As). Upload these two files from your local desktop to the JupyterLab. Upload icon looks like . Notebook-scoped libraries \u00b6 Run all the cells in amazon-reviews.ipynb notebook. Make sure Pyspark kernel is selected. Notice the notebook scoped libraries installed on SparkContext sc. sc.list_packages() sc.install_pypi_package(\"pandas==1.0.1\") #Install pandas version 1.0.5 sc.install_pypi_package(\"numpy==1.20.2\") #Intall numpy version 1.19.5 sc.install_pypi_package(\"matplotlib==3.2.0\",\"https://pypi.org/simple\") #Install matplotlib from given PyPI repository sc.list_packages() You will use these installed dependencies to plot visualizations on top of Amazon Reviews data. You can have two notebooks within the same workspace with different dependencies. You can even reproduce these dependencies and run the same notebook after your cluster is terminated by attaching it to a different active cluster. When you are done, terminate the kernel by clicking on stop icon . Parameterized notebooks \u00b6 Open the file find_best_sellers.ipynb. Go to View -> Show Right Sidebar. Click on the first cell with comment \"Default parameters\". In the Right Sidebar, click on \"Add tag\" and type \"parameters\" and click \"+\". Now check the \"Advanced Tools\" and make sure that the parameters tag is applied to that cell. Replace OUTPUT_LOCATION \"s3://mrworkshop- -dayone/studio/best_sellers_output/\" with your event engine AWS account ID. Check the S3 Web Console for the bucket name if required. Do not create the S3 prefix \"studio\" before running the notebook cells. Run all the cells in the notebook and make sure the outputs for categories \"Apparel\" and \"Baby\" are created under the S3 output location using AWS CLI or S3 Web Console. Save the notebook. When you are done, terminate the kernel by clicking on stop icon . Notebooks API \u00b6 Let us run the parameterized notebook \"find_best_sellers.ipynb\" using EMR Notebooks API. Run the below command with your JumpHost EC2 instance (connected with Session Manager). aws s3 ls s3://amazon-reviews-pds/parquet/product_category You can see the list of categories. From EMR Studio, we ran analysis for categories \"Apparel\" and \"Baby\". Now let us run this notebook from API for categories \"Furniture\" and \"PC\". You can select whichever categories you want. Run following commands in your EC2 JumpHost to upgrade your AWS CLI. sudo su ec2-user cd ~ pip3 uninstall awscli -y pip3 install awscli --upgrade /home/ec2-user/.local/bin/aws --version Verify that the notebooks APIs are working / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 list - studios Copy the editor-ID from notebook URL. For example: https://e-c9zy9cmd24ccf4f2b4uz7d7ma.emrnotebooks-prod.us-east-1.amazonaws.com/e-C9ZY9CMD24CCF4F2B4UZ7D7MA/lab/tree/workshop-repo/files/notebook/find_best_sellers.ipynb \"e-C9ZY9CMD24CCF4F2B4UZ7D7MA\" is the editor ID. In the following command, replace your editor-id with this value. Cluster ID in --execution-engine should be replaced with your EMR cluster \"EMR-Spark-Hive-Presto\" cluster ID (Obtained from AWS Management Console -> Amazon EMR Console -> Summary tab. Looks like j-XXXXXXXXX) Change \"youraccountID\" in the OUTPUT_LOCATION parameter with your account ID. / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 \\ start - notebook - execution \\ -- editor - id e - XXXXXXXXXXXXXXXXX \\ -- notebook - params '{ \"CATEGORIES\" : [ \"Furniture\" , \"PC\" ], \"FROM_DATE\" : \"2015-08-27\" , \"TO_DATE\" : \"2015-08-31\" , \"OUTPUT_LOCATION\" : \"s3://mrworkshop-youraccountID-dayone/studio/best_sellers_output_fromapi/\" }' \\ -- relative - path workshop - repo / files / notebook / find_best_sellers . ipynb \\ -- notebook - execution - name demo - execution \\ -- execution - engine '{ \"Id\" : \"j-XXXXXXXXXXXXX\" }' \\ -- service - role emrStudioRole { \"NotebookExecutionId\": \"ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV\" } You will get a NotebookExecutionId in return. Use this NotebookExecutionId in the following command to check for status. aws emr -- region us - east - 1 describe - notebook - execution -- notebook - execution - id ex - J02QDLG4TWXSNWLO4OGZ9NNX609MV After about 2-3 minutes, the Status will be FINISHED. aws emr -- region us - east - 1 describe - notebook - execution -- notebook - execution - id ex - J02QDLG4TWXSNWLO4OGZ9NNX609MV { \"NotebookExecution\" : { \"Status\" : \"FINISHED\" , \"ExecutionEngine\" : { \"MasterInstanceSecurityGroupId\" : \"sg-066e6805267d1d69c\" , \"Type\" : \"EMR\" , \"Id\" : \"j-142PVKGDZTTXS\" }, \"NotebookParams\" : \"{ \\\" CATEGORIES \\\" :[ \\\" Furniture \\\" , \\\" PC \\\" ], \\\" FROM_DATE \\\" : \\\" 2015-08-27 \\\" , \\\" TO_DATE \\\" : \\\" 2015-08-31 \\\" , \\\" OUTPUT_LOCATION \\\" : \\\" s3://mrworkshop-352365466794-dayone/studio/best_sellers_output_fromapi/ \\\" }\" , \"Tags\" : [], \"OutputNotebookURI\" : \"s3://studio-352365466794-dayone/notebook/e-C9ZY9CMD24CCF4F2B4UZ7D7MA/executions/ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV/find_best_sellers.ipynb\" , \"NotebookExecutionName\" : \"demo-execution\" , \"LastStateChangeReason\" : \"Execution is finished for cluster j-142PVKGDZTTXS.\" , \"StartTime\" : 1647768150.761 , \"NotebookExecutionId\" : \"ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV\" , \"EndTime\" : 1647768220.416 , \"EditorId\" : \"e-C9ZY9CMD24CCF4F2B4UZ7D7MA\" , \"Arn\" : \"arn:aws:elasticmapreduce:us-east-1:352365466794:notebook-execution/ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV\" , \"NotebookInstanceSecurityGroupId\" : \"sg-05e5e70bcaa4a624f\" } } Now let us check the S3 output path. You will now see a new prefix called \"best_sellers_output_fromapi\" with files generated under categories \"Furniture\" and \"PC\". We will see in tomorrow's exercise how to orchestrate a pipeline with this parameterized notebook using Amazon Managed Workflows for Apache Airflow. SQL Explorer \u00b6 Lets check the new SQL explorer feature which helps you run ad-hoc and interactive queries against your tables. Go to the SQL explorer and select \"default\" database. You will be able to see the four tables created in Glue catalog for the 4 categories apparel, baby, furniture and PC from our previous job runs. Click on \"Open Editor\" and query the tables. select * from default.baby limit 10; Collaborators \u00b6 Workspace collaboration is a new feature introduced in EMR Studio. Currently, we are logged in as studiouser. Go to the root folder and choose studio-ws.ipynb which is the default notebook created for this workspace. You can choose any kernel. Let us choose Python3 kernel for this time. Type the following command on the cell. print(\"hello world\") Now go to the Collaborators section and add the IAM user \"collabuser\". Make sure that the user is added to the workspace collaborators. Open another incognito or private window in your browser and paste the Studio access URL (for example: https://es-8QX8R2BETY6B8HA0Y6QM7G6EC.emrstudio-prod.us-east-1.amazonaws.com?locale=en). Click on Logout and logout as studiouser. Once signed out, do not click on \"Log Back In\". Paste the Studio access URL again in the same window and you will be re-directed to login page. Enter your event engine AWS account ID. Under IAM user name, enter collab user. Under password, enter \"Test123$\". Click on sign in. Once logged in, click on the workspace \"studio-ws\" and open JupyterLab console. Now, open the studio-ws.ipynb file. Open the two private browsers side by side with one browser session for IAM user \"studiouser\" and another one for IAM user \"collabuser\". Hover over the hello world code cell from collabuser's browser and see the user name from the studiouser's browser. Similarly, you can hover over the cell from studiouser's browser and see the user name from the collabuser's browser. You can also edit the same cell as collabuser and see the changes getting reflected from the studiouser's browser. This feature is very useful for collaborating with your team members for live troubleshooting and brainstorming.","title":"3 - Amazon EMR Studio"},{"location":"day1/studio/exercise/#exercise-3-amazon-emr-studio","text":"","title":"Exercise 3 - Amazon EMR Studio"},{"location":"day1/studio/exercise/#log-in-to-emr-studio","text":"In this exercise we will run Spark workflows using EMR Studio with managed Jupyter-based notebooks. We will also cover the most standout features of Amazon EMR Studio. Go to the EMR Web Console and navigate to \"EMR Studio\" on the right hand side. Click on \"Get Started\". You will be able to see an EMR Studio created called \"workshop-studio\". Click on it and in the following page, copy the Studio URL. Open an incognito or a private browser and paste the URL. In the AWS login page, choose \"IAM User\" and enter the account ID retrieved from your event engine's AWS Web console. Click on Next. Under IAM user name, enter \"studiouser\". Under password, enter Test123$. Click on Sign in. You will be logged into the EMR Studio. Users can access this interface without requiring AWS Web Console access. EMR Studio supports both IAM and SSO auth modes.","title":"Log in to EMR Studio"},{"location":"day1/studio/exercise/#check-emr-clusters-from-emr-studio","text":"Check the clusters under EMR on EC2. You can filter the clusters. Click on \"EMR-Spark-Hive-Presto\" and go to \"Launch application UI -> Spark History Server\". You will be taken to the EMR Persistent Spark History Server. You can also see the UIs of terminated clusters for up to 60 days after termination.","title":"Check EMR clusters from EMR Studio"},{"location":"day1/studio/exercise/#create-a-studio-workspace","text":"Go to Workspaces and \"Create Workspace\". Enter a workspace name. For example: \"studio-ws\". Enable \"Allow Workspace Collaboration\". Under \"Advanced Configuration\", select \"Attach Workspace to an EMR cluster\". In the drop down, choose the EMR-Spark-Hive-Presto cluster. Click \"Create Workspace\". It will take about 2 minutes for the Status to change to \"Attached\". Click on the workspace and it will open a managed JupyterLab session. You may need to allow pop-up from this address in your browser to open the JupyterLab. Once opened, you can create a Jupyter notebook with any kernel.","title":"Create a Studio Workspace"},{"location":"day1/studio/exercise/#explore-emr-studio-workspace-features","text":"","title":"Explore EMR Studio Workspace Features"},{"location":"day1/studio/exercise/#cluster","text":"Under cluster tab, check the cluster attachment. Note that you will be able to detach and attach this workspace to a different cluster. For now, you can leave it as is.","title":"Cluster"},{"location":"day1/studio/exercise/#git-repository","text":"Under Git tab, you can add a Git repository by entering the repository name, URL and credentials. You can access public repositories without any credentials. Repository name: workshop-repo Git repository URL: https://github.com/vasveena/amazon-emr-ttt-workshop Branch: main Git credentials: Use a public repository without credentials Once the repository is added, select it from the \"Git repositories\" drop down. You will see that the Git repository will be linked successfully. Once its linked, you can go back to the workspace folder. You will find a folder called \"workshop-repo\". Go to workshop-repo -> files -> notebook to see the notebooks. If you are not able to link repository successfully, download amazon_reviews.ipynb and find_best_sellers.ipynb files to your local desktop (Right click -> Save Link As). Upload these two files from your local desktop to the JupyterLab. Upload icon looks like .","title":"Git repository"},{"location":"day1/studio/exercise/#notebook-scoped-libraries","text":"Run all the cells in amazon-reviews.ipynb notebook. Make sure Pyspark kernel is selected. Notice the notebook scoped libraries installed on SparkContext sc. sc.list_packages() sc.install_pypi_package(\"pandas==1.0.1\") #Install pandas version 1.0.5 sc.install_pypi_package(\"numpy==1.20.2\") #Intall numpy version 1.19.5 sc.install_pypi_package(\"matplotlib==3.2.0\",\"https://pypi.org/simple\") #Install matplotlib from given PyPI repository sc.list_packages() You will use these installed dependencies to plot visualizations on top of Amazon Reviews data. You can have two notebooks within the same workspace with different dependencies. You can even reproduce these dependencies and run the same notebook after your cluster is terminated by attaching it to a different active cluster. When you are done, terminate the kernel by clicking on stop icon .","title":"Notebook-scoped libraries"},{"location":"day1/studio/exercise/#parameterized-notebooks","text":"Open the file find_best_sellers.ipynb. Go to View -> Show Right Sidebar. Click on the first cell with comment \"Default parameters\". In the Right Sidebar, click on \"Add tag\" and type \"parameters\" and click \"+\". Now check the \"Advanced Tools\" and make sure that the parameters tag is applied to that cell. Replace OUTPUT_LOCATION \"s3://mrworkshop- -dayone/studio/best_sellers_output/\" with your event engine AWS account ID. Check the S3 Web Console for the bucket name if required. Do not create the S3 prefix \"studio\" before running the notebook cells. Run all the cells in the notebook and make sure the outputs for categories \"Apparel\" and \"Baby\" are created under the S3 output location using AWS CLI or S3 Web Console. Save the notebook. When you are done, terminate the kernel by clicking on stop icon .","title":"Parameterized notebooks"},{"location":"day1/studio/exercise/#notebooks-api","text":"Let us run the parameterized notebook \"find_best_sellers.ipynb\" using EMR Notebooks API. Run the below command with your JumpHost EC2 instance (connected with Session Manager). aws s3 ls s3://amazon-reviews-pds/parquet/product_category You can see the list of categories. From EMR Studio, we ran analysis for categories \"Apparel\" and \"Baby\". Now let us run this notebook from API for categories \"Furniture\" and \"PC\". You can select whichever categories you want. Run following commands in your EC2 JumpHost to upgrade your AWS CLI. sudo su ec2-user cd ~ pip3 uninstall awscli -y pip3 install awscli --upgrade /home/ec2-user/.local/bin/aws --version Verify that the notebooks APIs are working / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 list - studios Copy the editor-ID from notebook URL. For example: https://e-c9zy9cmd24ccf4f2b4uz7d7ma.emrnotebooks-prod.us-east-1.amazonaws.com/e-C9ZY9CMD24CCF4F2B4UZ7D7MA/lab/tree/workshop-repo/files/notebook/find_best_sellers.ipynb \"e-C9ZY9CMD24CCF4F2B4UZ7D7MA\" is the editor ID. In the following command, replace your editor-id with this value. Cluster ID in --execution-engine should be replaced with your EMR cluster \"EMR-Spark-Hive-Presto\" cluster ID (Obtained from AWS Management Console -> Amazon EMR Console -> Summary tab. Looks like j-XXXXXXXXX) Change \"youraccountID\" in the OUTPUT_LOCATION parameter with your account ID. / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 \\ start - notebook - execution \\ -- editor - id e - XXXXXXXXXXXXXXXXX \\ -- notebook - params '{ \"CATEGORIES\" : [ \"Furniture\" , \"PC\" ], \"FROM_DATE\" : \"2015-08-27\" , \"TO_DATE\" : \"2015-08-31\" , \"OUTPUT_LOCATION\" : \"s3://mrworkshop-youraccountID-dayone/studio/best_sellers_output_fromapi/\" }' \\ -- relative - path workshop - repo / files / notebook / find_best_sellers . ipynb \\ -- notebook - execution - name demo - execution \\ -- execution - engine '{ \"Id\" : \"j-XXXXXXXXXXXXX\" }' \\ -- service - role emrStudioRole { \"NotebookExecutionId\": \"ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV\" } You will get a NotebookExecutionId in return. Use this NotebookExecutionId in the following command to check for status. aws emr -- region us - east - 1 describe - notebook - execution -- notebook - execution - id ex - J02QDLG4TWXSNWLO4OGZ9NNX609MV After about 2-3 minutes, the Status will be FINISHED. aws emr -- region us - east - 1 describe - notebook - execution -- notebook - execution - id ex - J02QDLG4TWXSNWLO4OGZ9NNX609MV { \"NotebookExecution\" : { \"Status\" : \"FINISHED\" , \"ExecutionEngine\" : { \"MasterInstanceSecurityGroupId\" : \"sg-066e6805267d1d69c\" , \"Type\" : \"EMR\" , \"Id\" : \"j-142PVKGDZTTXS\" }, \"NotebookParams\" : \"{ \\\" CATEGORIES \\\" :[ \\\" Furniture \\\" , \\\" PC \\\" ], \\\" FROM_DATE \\\" : \\\" 2015-08-27 \\\" , \\\" TO_DATE \\\" : \\\" 2015-08-31 \\\" , \\\" OUTPUT_LOCATION \\\" : \\\" s3://mrworkshop-352365466794-dayone/studio/best_sellers_output_fromapi/ \\\" }\" , \"Tags\" : [], \"OutputNotebookURI\" : \"s3://studio-352365466794-dayone/notebook/e-C9ZY9CMD24CCF4F2B4UZ7D7MA/executions/ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV/find_best_sellers.ipynb\" , \"NotebookExecutionName\" : \"demo-execution\" , \"LastStateChangeReason\" : \"Execution is finished for cluster j-142PVKGDZTTXS.\" , \"StartTime\" : 1647768150.761 , \"NotebookExecutionId\" : \"ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV\" , \"EndTime\" : 1647768220.416 , \"EditorId\" : \"e-C9ZY9CMD24CCF4F2B4UZ7D7MA\" , \"Arn\" : \"arn:aws:elasticmapreduce:us-east-1:352365466794:notebook-execution/ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV\" , \"NotebookInstanceSecurityGroupId\" : \"sg-05e5e70bcaa4a624f\" } } Now let us check the S3 output path. You will now see a new prefix called \"best_sellers_output_fromapi\" with files generated under categories \"Furniture\" and \"PC\". We will see in tomorrow's exercise how to orchestrate a pipeline with this parameterized notebook using Amazon Managed Workflows for Apache Airflow.","title":"Notebooks API"},{"location":"day1/studio/exercise/#sql-explorer","text":"Lets check the new SQL explorer feature which helps you run ad-hoc and interactive queries against your tables. Go to the SQL explorer and select \"default\" database. You will be able to see the four tables created in Glue catalog for the 4 categories apparel, baby, furniture and PC from our previous job runs. Click on \"Open Editor\" and query the tables. select * from default.baby limit 10;","title":"SQL Explorer"},{"location":"day1/studio/exercise/#collaborators","text":"Workspace collaboration is a new feature introduced in EMR Studio. Currently, we are logged in as studiouser. Go to the root folder and choose studio-ws.ipynb which is the default notebook created for this workspace. You can choose any kernel. Let us choose Python3 kernel for this time. Type the following command on the cell. print(\"hello world\") Now go to the Collaborators section and add the IAM user \"collabuser\". Make sure that the user is added to the workspace collaborators. Open another incognito or private window in your browser and paste the Studio access URL (for example: https://es-8QX8R2BETY6B8HA0Y6QM7G6EC.emrstudio-prod.us-east-1.amazonaws.com?locale=en). Click on Logout and logout as studiouser. Once signed out, do not click on \"Log Back In\". Paste the Studio access URL again in the same window and you will be re-directed to login page. Enter your event engine AWS account ID. Under IAM user name, enter collab user. Under password, enter \"Test123$\". Click on sign in. Once logged in, click on the workspace \"studio-ws\" and open JupyterLab console. Now, open the studio-ws.ipynb file. Open the two private browsers side by side with one browser session for IAM user \"studiouser\" and another one for IAM user \"collabuser\". Hover over the hello world code cell from collabuser's browser and see the user name from the studiouser's browser. Similarly, you can hover over the cell from studiouser's browser and see the user name from the collabuser's browser. You can also edit the same cell as collabuser and see the changes getting reflected from the studiouser's browser. This feature is very useful for collaborating with your team members for live troubleshooting and brainstorming.","title":"Collaborators"},{"location":"day2/","text":"Amazon EMR Deployment Options \u00b6 Amazon EMR offers four deployment options to run your applications. Amazon EMR on EC2 Amazon EMR on EKS Amazon EMR on Outposts Amazon EMR Serverless (Preview) So far we leveraged EMR on EC2 deployment. In the following exercises, we will run the same workloads using EMR on EKS and EMR Serverless deployment options. Note that all the deployment options offer the same optimized runtime Spark engine version for every EMR release label. This means you can build your application code only once and then use any of these deployment modes interchangeably to run your applications.","title":"Introduction"},{"location":"day2/#amazon-emr-deployment-options","text":"Amazon EMR offers four deployment options to run your applications. Amazon EMR on EC2 Amazon EMR on EKS Amazon EMR on Outposts Amazon EMR Serverless (Preview) So far we leveraged EMR on EC2 deployment. In the following exercises, we will run the same workloads using EMR on EKS and EMR Serverless deployment options. Note that all the deployment options offer the same optimized runtime Spark engine version for every EMR release label. This means you can build your application code only once and then use any of these deployment modes interchangeably to run your applications.","title":"Amazon EMR Deployment Options"},{"location":"day2/emroneks/exercise/","text":"Exercise 2.1 - Amazon EMR on EKS \u00b6 In this exercise, you will run Spark applications using EMR on EKS. For this exercise, we are going to use an EKS cluster created using the CloudFormation template. If you are interested in building this environment end-to-end, you can do that following the first 10 steps (until line 131) in this note . Create the EMR on EKS virtual clusters \u00b6 Go to the CloudFormation Web Console (AWS Web Console -> Search for CloudFormation -> Right click -> Open in new tab) and see if you have the CloudFormation stack named \"emr-on-eks\" deployed in your AWS event engine accounts. Now go to the EC2 and search for \"jumphost\". Choose the instance starting with the name \"emr-on-eks-PrepStack\". Connect to this instance using Session Manager. Once you are inside the session manager, do NOT switch to the ec2-user. Export the AWS credentials of your AWS event engine accounts (follow the step 6 of Setup ). export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > Next, run the following command to configure the Kubernetes client (kubectl) and download the certificate from the Amazon EKS control plane for authentication. sudo sed -i 's|<REGION>|us-east-1|g' /tmp/aws-logging-cloudwatch-configmap.yaml sudo sed -i 's|auto_create_group On|auto_create_group true|g' /tmp/aws-logging-cloudwatch-configmap.yaml sh /tmp/kubeconfig.sh It will take about 2 mins to complete. Once done, you will see the 3 nodes of the deployed EKS cluster in ready status. Go to the EKS Web console (AWS Web Console -> Search for EKS -> Right click -> Open in new tab) and check the cluster myEKS. Click on the cluster and look at the 3 EC2 nodes in the Overview section. These are the EC2 instances of the EKS cluster. Go to Configuration -> Compute and scroll down to see the Fargate profile attached to the EKS cluster. Run the following command to register the EKS namespace backed by EC2 instances with an EMR virtual cluster. sh /tmp/emroneks.sh ec2-ns ec2-vc Run the following command to register the EKS namespace backed by Fargate profile with another EMR virtual cluster. sh /tmp/emroneks-fargate.sh fargate-vc You can now go to the EMR Web console. On the bottom left side pane, you will be able to see the \"EMR on EKS\" section. Click on Virtual clusters. You can see the two EMR on EKS virtual clusters created for EC2 and Fargate namespaces respectively. Virtual clusters do not run any resources. Build the Cloud9 workspace \u00b6 We will use Cloud9 to observe Kubernetes dashboards. Go to the Cloud9 Web Console -> Create environment. In Step 1, name your environment like \"emr-on-eks-platform\". In Step 2, choose t3.small for instance type. Leave the values for Environment type, Instance type, Platform defaulted. Choose Network settings (advanced) and choose VPC. Select the VPC starting with the name \"emr-on-eks-EksStack\". Go to Next Step and create environment. This will take a few minutes. Once the environment is created, click on the grey circle button in top right corner and select Manage EC2 Instance. You will be taken to the cloud9 instance in the EC2 console. Select the instance, then choose Actions / Security / Modify IAM Role. Select the IAM role that looks like \"emr-on-eks-PrepStack-XXXX-rJumpHostInstanceProfile\" and Save. Now return to your Cloud9 workspace and click the gear icon in top right corner. Select AWS SETTINGS. Turn off AWS managed temporary credentials. Close the Preferences tab. Install Tools \u00b6 Let's install eksctl and kubectl in our Cloud9 environment. Kubernetes uses a command line utility called kubectl for communicating with the cluster API server. eksctl is a simple CLI tool for creating and managing EKS clusters. Run the following commands in your Cloud9 IDE. You can expand the bash screen for better visibility. Upgrade your AWS CLI. pip3 install awscli --upgrade --user Install eksctl with following commands. curl -- silent -- location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz - C / tmp sudo mv / tmp / eksctl / usr / local / bin eksctl version Install kubectl with following commands and associate it with the EKS cluster. curl - o kubectl https : // amazon - eks . s3 . us - west - 2. amazonaws . com / 1.19 . 6 / 2021 - 01 - 05 / bin / linux / amd64 / kubectl chmod + x ./ kubectl mkdir - p $ HOME / bin && cp ./ kubectl $ HOME / bin / kubectl && export PATH =$ PATH : $ HOME / bin echo 'export PATH=$PATH:$HOME/bin' >> ~/. bashrc kubectl version -- short -- client aws eks update - kubeconfig -- region us - east - 1 -- name myEKS kubectl get nodes - o wide The last command should display the EC2 nodes of your EKS cluster. Similarly, running the eksctl get cluster command should display the myEKS cluster. eksctl get cluster -- region us - east - 1 Run the following command and see if the EMR on EKS clusters we created are getting listed. aws emr - containers list - virtual - clusters -- region us - east - 1 Submit jobs using EMR on EKS \u00b6 Now, lets submit jobs to these clusters. For that, we are going to use EMR Containers API. To make it easy, lets assign variables to be used in these APIs. sudo yum install - y jq ec2_vc = $ ( aws emr - containers list - virtual - clusters -- region us - east - 1 | jq - r .' virtualClusters [] | select (. name == \"ec2-vc\" ) | . id ') fargate_vc = $ ( aws emr - containers list - virtual - clusters -- region us - east - 1 | jq - r .' virtualClusters [] | select (. name == \"fargate-vc\" ) | . id ') emrOnEksExecRoleArn = $ ( aws iam list - roles | jq - r .' Roles [] | select (. RoleName | endswith ( \"-EMRExectionRole\" )) | . Arn ') accountID = $ ( aws sts get - caller - identity -- query Account -- output text ) Submit jobs to EC2 namespace \u00b6 Let us submit a Spark job to the EMR on EKS cluster with namespace attached to the EC2 node group. No need to change anything since we have used variables. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-pi-6.5 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-6.5.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 You should get the below response. { \"virtualClusterId\": \"vaqteerxju21c161kkr7awe00\", \"arn\": \"arn:aws:emr-containers:us-east-1:156321623241:/virtualclusters/vaqteerxju21c161kkr7awe00/jobruns/00000002vtm1d42gmtd\", \"id\": \"00000002vtm1d42gmtd\", \"name\": \"spark-pi-6.5\" } Right after this, execute the below command to see what happens on the EC2 namespace in your EKS cluster. kubectl get pods -n ec2-ns --watch Almost immediately you will see EMR virtual cluster will create driver and executor pods on the EC2 namespace of your EKS cluster. Check the job in the EMR Web Console (EMR Web Console-> EMR on EKS -> Virtual Clusters -> ec2-vc). You will see that the job is completed. Click on \"View Logs\" to open the persistent Spark history server. You can also check the Spark job logs by going to CloudWatch Logs (AWS Web Console -> Search for Cloudwatch -> Open Cloudwatch console -> Log groups). You will see there is a log group called \"emroneks\". You can see the driver and executor logs. Pass Advanced Configs as JSON to the API \u00b6 Let's run a Spark ETL job with Glue metastore integration and configure it to send driver and executor logs to S3 and Cloudwatch log group. This is the ETL job (spark-etl-glue.py) we are going to use which will read data, do some basic transformations and write the output to a table on AWS Glue data catalog. This code is just for your reference. import sys from datetime import datetime from pyspark.sql import SparkSession from pyspark.sql import SQLContext from pyspark.sql.functions import * if __name__ == \"__main__\" : print ( len ( sys . argv )) if ( len ( sys . argv ) != 4 ): print ( \"Usage: spark-etl-glue [input-folder] [output-folder] [dbName]\" ) sys . exit ( 0 ) spark = SparkSession \\ . builder \\ . appName ( \"Python Spark SQL Glue integration example\" ) \\ . enableHiveSupport () \\ . getOrCreate () nyTaxi = spark . read . option ( \"inferSchema\" , \"true\" ) . option ( \"header\" , \"true\" ) . csv ( sys . argv [ 1 ]) updatedNYTaxi = nyTaxi . withColumn ( \"current_date\" , lit ( datetime . now ())) updatedNYTaxi . printSchema () print ( updatedNYTaxi . show ()) print ( \"Total number of records: \" + str ( updatedNYTaxi . count ())) updatedNYTaxi . write . parquet ( sys . argv [ 2 ]) updatedNYTaxi . registerTempTable ( \"ny_taxi_table\" ) dbName = sys . argv [ 3 ] spark . sql ( \"CREATE database if not exists \" + dbName ) spark . sql ( \"USE \" + dbName ) spark . sql ( \"CREATE table if not exists ny_taxi_parquet USING PARQUET LOCATION '\" + sys . argv [ 2 ] + \"' AS SELECT * from ny_taxi_table \" ) We will create a JSON with all the configurations we need using the below command. sudo tee ./emroneks-config.json >/dev/null < <EOF { \"name\": \"spark-glue-integration-and-s3-log\", \"virtualClusterId\": \"${ec2_vc}\", \"executionRoleArn\": \"${emrOnEksExecRoleArn}\", \"releaseLabel\": \"emr-6.5.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/spark-etl-glue.py\", \"entryPointArguments\": [ \"s3://aws-data-analytics-workshops/shared_datasets/tripdata/\",\"s3://mrworkshop-$accountID-dayone/taxi-data-glue/\",\"tripdata\" ], \"sparkSubmitParameters\": \"--conf spark.driver. cores= 1 --conf spark.executor. memory= 2G --conf spark.driver. memory= 2G --conf spark.executor. cores= 2\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.hadoop.hive.metastore.client.factory.class\":\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" } }, { \"classification\": \"spark-log4j\", \"properties\": { \"log4j.rootCategory\": \"DEBUG, console\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"emroneks\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://mrworkshop-$accountID-dayone/emroneks/logs/\" } } } } EOF Now you can pass the config file we created in your API. aws emr - containers start - job - run -- cli - input - json file: //emroneks-config.json --region us-east-1 You can find the Spark driver and executor pod logs in S3 for the above job. You can also see the table \"ny_taxi_parquet\" in database \"tripdata\" created in AWS Glue data log. Go to Athena Web Console -> Explore the query editor and see the table under database \"tripdata\". Since this is the first time we are using Athena, in order to run queries, you need to go to the Settings -> Manage add an S3 location in your account to save your query results. For example: s3://mrworkshop- -dayone/athena/. You can query the table now. select * from tripdata.ny_taxi_parquet limit 10; Change EMR Release Label \u00b6 One of the important features with EMR on EKS is the ability to change major and minor versions per release label. So far we submitted jobs using EMR 6.5.0 label. Let's now submit the jobs to other release labels. Let's submit a job to EMR 5.34.0 label. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-pi-5.34 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Similarly, you can change the minor versions also. Let's submit the same job to EMR 6.2.0. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-pi-6.2 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-6.2.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Notice the jobs in EMR on EKS console. We are consolidating jobs from different versions in the same infrastructure on EKS. Submit serverless Spark jobs using Fargate \u00b6 So far we were using EC2 namespace to submit jobs. We have an EMR on EKS virtual cluster created for Fargate namespace as well. Let's submit serverless Spark jobs using Fargate. Run the below command. Notice that for --virtual-cluster-id we are passing the EMR on EKS cluster mapped to Fargate namespace. aws emr-containers start-job-run --virtual-cluster-id ${ fargate_vc } \\ --name spark-pi-5.34 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 You will see the job being submitted to Fargate virtual cluster. Now go to the EKS Web Console -> Overview. In about a minute, you will see that the fargate resource has come up to run this job. Fargate automatically scales up and down based on your processing requirements. Deploy Kubernetes Dashboard \u00b6 Kubernetes Dashboard is a web-based user interface. You can use Dashboard to get an overview of applications running on your cluster. In this lab we will deploy the official Kubernetes Dashboard. Check the documentation here . To deploy the dashboard, run the following command: export DASHBOARD_VERSION = \"v2.0.0\" kubectl apply - f https : // raw . githubusercontent . com / kubernetes / dashboard /$ { DASHBOARD_VERSION } / aio / deploy / recommended . yaml You can access Dashboard using the kubectl command-line tool by running the following command in your Cloud9 terminal. This will start the proxy on port 8080. kubectl proxy --port=8080 --address=0.0.0.0 --disable-filter=true & In your Cloud9 workspace, click Tools / Preview / Preview Running Application. Scroll to the end of the browser address URL and append the following: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ You will see the Kubernetes dashboard asking for token. Once you have the login screen in the Cloud9 preview browser tab, press the Pop Out button to open the login screen in a regular browser tab, like below: To get the token, you need to create an EKS admin account. Let's do that by running following commands on Cloud9. sudo tee ./eks-admin-service-account.yaml >/dev/null <<EOF apiVersion: v1 kind: ServiceAccount metadata: name: eks-admin namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: eks-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: eks-admin namespace: kube-system EOF kubectl apply -f eks-admin-service-account.yaml Now, run the below command in Cloud9 and paste the \"token\" of the output on to your browser where it prompts you to enter token. kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}') You will now be able to see the dashboard. Submit a job again. aws emr-containers start-job-run --virtual-cluster-id ${ fargate_vc } \\ --name spark-pi-5.34 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Explore the dashboard. Orchestrate jobs on EMR on EKS using Amazon MWAA \u00b6 Let's submit (or orchestrate) jobs to EMR on EKS through Amazon MWAA. Go to the MWAA console. You will find a new environment starting with name \"emr-on-eks-MWAAStack\". Open the airflow UI. You will see two DAGs. One for EC2 namespace and one for fargate namespace. Toggle the DAGs ON and trigger the DAGs manually. This job copies, unzips and transforms files from S3 and then runs some analytics on top of this transformed data. This DAG takes about 10-12 mins end-to-end. You can check the status in EMR on EKS console. Single AZ placement \u00b6 Our EMR on EKS cluster uses two AZs: us-east-1a and us-east-1b. Go to the Nodes section in Kubernetes dashboard to see which EC2 nodes belong to which AZ. This provides resiliency when compared to EMR on EC2 which runs on a single AZ. However, cross-AZ communication may impact performance and cost. You can define topology for your jobs and instruct all pods to launch in a single AZ. Let's see an example. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-single-az-us-east-1a \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.kubernetes.node.selector.topology.kubernetes.io/zone='us-east-1a' --conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Check the Pods section of Kubernetes dashboard. You will see that the pods are launched only on nodes running in us-east-1a. You can test the same command by substituting us-east-1a with us-east-1b and see how the pods are getting placed in your Kubernetes dashboard. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-single-az-us-east-1a \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.kubernetes.node.selector.topology.kubernetes.io/zone='us-east-1b' --conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1","title":"2.1 - Amazon EMR on EKS"},{"location":"day2/emroneks/exercise/#exercise-21-amazon-emr-on-eks","text":"In this exercise, you will run Spark applications using EMR on EKS. For this exercise, we are going to use an EKS cluster created using the CloudFormation template. If you are interested in building this environment end-to-end, you can do that following the first 10 steps (until line 131) in this note .","title":"Exercise 2.1 - Amazon EMR on EKS"},{"location":"day2/emroneks/exercise/#create-the-emr-on-eks-virtual-clusters","text":"Go to the CloudFormation Web Console (AWS Web Console -> Search for CloudFormation -> Right click -> Open in new tab) and see if you have the CloudFormation stack named \"emr-on-eks\" deployed in your AWS event engine accounts. Now go to the EC2 and search for \"jumphost\". Choose the instance starting with the name \"emr-on-eks-PrepStack\". Connect to this instance using Session Manager. Once you are inside the session manager, do NOT switch to the ec2-user. Export the AWS credentials of your AWS event engine accounts (follow the step 6 of Setup ). export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > Next, run the following command to configure the Kubernetes client (kubectl) and download the certificate from the Amazon EKS control plane for authentication. sudo sed -i 's|<REGION>|us-east-1|g' /tmp/aws-logging-cloudwatch-configmap.yaml sudo sed -i 's|auto_create_group On|auto_create_group true|g' /tmp/aws-logging-cloudwatch-configmap.yaml sh /tmp/kubeconfig.sh It will take about 2 mins to complete. Once done, you will see the 3 nodes of the deployed EKS cluster in ready status. Go to the EKS Web console (AWS Web Console -> Search for EKS -> Right click -> Open in new tab) and check the cluster myEKS. Click on the cluster and look at the 3 EC2 nodes in the Overview section. These are the EC2 instances of the EKS cluster. Go to Configuration -> Compute and scroll down to see the Fargate profile attached to the EKS cluster. Run the following command to register the EKS namespace backed by EC2 instances with an EMR virtual cluster. sh /tmp/emroneks.sh ec2-ns ec2-vc Run the following command to register the EKS namespace backed by Fargate profile with another EMR virtual cluster. sh /tmp/emroneks-fargate.sh fargate-vc You can now go to the EMR Web console. On the bottom left side pane, you will be able to see the \"EMR on EKS\" section. Click on Virtual clusters. You can see the two EMR on EKS virtual clusters created for EC2 and Fargate namespaces respectively. Virtual clusters do not run any resources.","title":"Create the EMR on EKS virtual clusters"},{"location":"day2/emroneks/exercise/#build-the-cloud9-workspace","text":"We will use Cloud9 to observe Kubernetes dashboards. Go to the Cloud9 Web Console -> Create environment. In Step 1, name your environment like \"emr-on-eks-platform\". In Step 2, choose t3.small for instance type. Leave the values for Environment type, Instance type, Platform defaulted. Choose Network settings (advanced) and choose VPC. Select the VPC starting with the name \"emr-on-eks-EksStack\". Go to Next Step and create environment. This will take a few minutes. Once the environment is created, click on the grey circle button in top right corner and select Manage EC2 Instance. You will be taken to the cloud9 instance in the EC2 console. Select the instance, then choose Actions / Security / Modify IAM Role. Select the IAM role that looks like \"emr-on-eks-PrepStack-XXXX-rJumpHostInstanceProfile\" and Save. Now return to your Cloud9 workspace and click the gear icon in top right corner. Select AWS SETTINGS. Turn off AWS managed temporary credentials. Close the Preferences tab.","title":"Build the Cloud9 workspace"},{"location":"day2/emroneks/exercise/#install-tools","text":"Let's install eksctl and kubectl in our Cloud9 environment. Kubernetes uses a command line utility called kubectl for communicating with the cluster API server. eksctl is a simple CLI tool for creating and managing EKS clusters. Run the following commands in your Cloud9 IDE. You can expand the bash screen for better visibility. Upgrade your AWS CLI. pip3 install awscli --upgrade --user Install eksctl with following commands. curl -- silent -- location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz - C / tmp sudo mv / tmp / eksctl / usr / local / bin eksctl version Install kubectl with following commands and associate it with the EKS cluster. curl - o kubectl https : // amazon - eks . s3 . us - west - 2. amazonaws . com / 1.19 . 6 / 2021 - 01 - 05 / bin / linux / amd64 / kubectl chmod + x ./ kubectl mkdir - p $ HOME / bin && cp ./ kubectl $ HOME / bin / kubectl && export PATH =$ PATH : $ HOME / bin echo 'export PATH=$PATH:$HOME/bin' >> ~/. bashrc kubectl version -- short -- client aws eks update - kubeconfig -- region us - east - 1 -- name myEKS kubectl get nodes - o wide The last command should display the EC2 nodes of your EKS cluster. Similarly, running the eksctl get cluster command should display the myEKS cluster. eksctl get cluster -- region us - east - 1 Run the following command and see if the EMR on EKS clusters we created are getting listed. aws emr - containers list - virtual - clusters -- region us - east - 1","title":"Install Tools"},{"location":"day2/emroneks/exercise/#submit-jobs-using-emr-on-eks","text":"Now, lets submit jobs to these clusters. For that, we are going to use EMR Containers API. To make it easy, lets assign variables to be used in these APIs. sudo yum install - y jq ec2_vc = $ ( aws emr - containers list - virtual - clusters -- region us - east - 1 | jq - r .' virtualClusters [] | select (. name == \"ec2-vc\" ) | . id ') fargate_vc = $ ( aws emr - containers list - virtual - clusters -- region us - east - 1 | jq - r .' virtualClusters [] | select (. name == \"fargate-vc\" ) | . id ') emrOnEksExecRoleArn = $ ( aws iam list - roles | jq - r .' Roles [] | select (. RoleName | endswith ( \"-EMRExectionRole\" )) | . Arn ') accountID = $ ( aws sts get - caller - identity -- query Account -- output text )","title":"Submit jobs using EMR on EKS"},{"location":"day2/emroneks/exercise/#submit-jobs-to-ec2-namespace","text":"Let us submit a Spark job to the EMR on EKS cluster with namespace attached to the EC2 node group. No need to change anything since we have used variables. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-pi-6.5 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-6.5.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 You should get the below response. { \"virtualClusterId\": \"vaqteerxju21c161kkr7awe00\", \"arn\": \"arn:aws:emr-containers:us-east-1:156321623241:/virtualclusters/vaqteerxju21c161kkr7awe00/jobruns/00000002vtm1d42gmtd\", \"id\": \"00000002vtm1d42gmtd\", \"name\": \"spark-pi-6.5\" } Right after this, execute the below command to see what happens on the EC2 namespace in your EKS cluster. kubectl get pods -n ec2-ns --watch Almost immediately you will see EMR virtual cluster will create driver and executor pods on the EC2 namespace of your EKS cluster. Check the job in the EMR Web Console (EMR Web Console-> EMR on EKS -> Virtual Clusters -> ec2-vc). You will see that the job is completed. Click on \"View Logs\" to open the persistent Spark history server. You can also check the Spark job logs by going to CloudWatch Logs (AWS Web Console -> Search for Cloudwatch -> Open Cloudwatch console -> Log groups). You will see there is a log group called \"emroneks\". You can see the driver and executor logs.","title":"Submit jobs to EC2 namespace"},{"location":"day2/emroneks/exercise/#pass-advanced-configs-as-json-to-the-api","text":"Let's run a Spark ETL job with Glue metastore integration and configure it to send driver and executor logs to S3 and Cloudwatch log group. This is the ETL job (spark-etl-glue.py) we are going to use which will read data, do some basic transformations and write the output to a table on AWS Glue data catalog. This code is just for your reference. import sys from datetime import datetime from pyspark.sql import SparkSession from pyspark.sql import SQLContext from pyspark.sql.functions import * if __name__ == \"__main__\" : print ( len ( sys . argv )) if ( len ( sys . argv ) != 4 ): print ( \"Usage: spark-etl-glue [input-folder] [output-folder] [dbName]\" ) sys . exit ( 0 ) spark = SparkSession \\ . builder \\ . appName ( \"Python Spark SQL Glue integration example\" ) \\ . enableHiveSupport () \\ . getOrCreate () nyTaxi = spark . read . option ( \"inferSchema\" , \"true\" ) . option ( \"header\" , \"true\" ) . csv ( sys . argv [ 1 ]) updatedNYTaxi = nyTaxi . withColumn ( \"current_date\" , lit ( datetime . now ())) updatedNYTaxi . printSchema () print ( updatedNYTaxi . show ()) print ( \"Total number of records: \" + str ( updatedNYTaxi . count ())) updatedNYTaxi . write . parquet ( sys . argv [ 2 ]) updatedNYTaxi . registerTempTable ( \"ny_taxi_table\" ) dbName = sys . argv [ 3 ] spark . sql ( \"CREATE database if not exists \" + dbName ) spark . sql ( \"USE \" + dbName ) spark . sql ( \"CREATE table if not exists ny_taxi_parquet USING PARQUET LOCATION '\" + sys . argv [ 2 ] + \"' AS SELECT * from ny_taxi_table \" ) We will create a JSON with all the configurations we need using the below command. sudo tee ./emroneks-config.json >/dev/null < <EOF { \"name\": \"spark-glue-integration-and-s3-log\", \"virtualClusterId\": \"${ec2_vc}\", \"executionRoleArn\": \"${emrOnEksExecRoleArn}\", \"releaseLabel\": \"emr-6.5.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/spark-etl-glue.py\", \"entryPointArguments\": [ \"s3://aws-data-analytics-workshops/shared_datasets/tripdata/\",\"s3://mrworkshop-$accountID-dayone/taxi-data-glue/\",\"tripdata\" ], \"sparkSubmitParameters\": \"--conf spark.driver. cores= 1 --conf spark.executor. memory= 2G --conf spark.driver. memory= 2G --conf spark.executor. cores= 2\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.hadoop.hive.metastore.client.factory.class\":\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" } }, { \"classification\": \"spark-log4j\", \"properties\": { \"log4j.rootCategory\": \"DEBUG, console\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"emroneks\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://mrworkshop-$accountID-dayone/emroneks/logs/\" } } } } EOF Now you can pass the config file we created in your API. aws emr - containers start - job - run -- cli - input - json file: //emroneks-config.json --region us-east-1 You can find the Spark driver and executor pod logs in S3 for the above job. You can also see the table \"ny_taxi_parquet\" in database \"tripdata\" created in AWS Glue data log. Go to Athena Web Console -> Explore the query editor and see the table under database \"tripdata\". Since this is the first time we are using Athena, in order to run queries, you need to go to the Settings -> Manage add an S3 location in your account to save your query results. For example: s3://mrworkshop- -dayone/athena/. You can query the table now. select * from tripdata.ny_taxi_parquet limit 10;","title":"Pass Advanced Configs as JSON to the API"},{"location":"day2/emroneks/exercise/#change-emr-release-label","text":"One of the important features with EMR on EKS is the ability to change major and minor versions per release label. So far we submitted jobs using EMR 6.5.0 label. Let's now submit the jobs to other release labels. Let's submit a job to EMR 5.34.0 label. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-pi-5.34 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Similarly, you can change the minor versions also. Let's submit the same job to EMR 6.2.0. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-pi-6.2 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-6.2.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Notice the jobs in EMR on EKS console. We are consolidating jobs from different versions in the same infrastructure on EKS.","title":"Change EMR Release Label"},{"location":"day2/emroneks/exercise/#submit-serverless-spark-jobs-using-fargate","text":"So far we were using EC2 namespace to submit jobs. We have an EMR on EKS virtual cluster created for Fargate namespace as well. Let's submit serverless Spark jobs using Fargate. Run the below command. Notice that for --virtual-cluster-id we are passing the EMR on EKS cluster mapped to Fargate namespace. aws emr-containers start-job-run --virtual-cluster-id ${ fargate_vc } \\ --name spark-pi-5.34 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 You will see the job being submitted to Fargate virtual cluster. Now go to the EKS Web Console -> Overview. In about a minute, you will see that the fargate resource has come up to run this job. Fargate automatically scales up and down based on your processing requirements.","title":"Submit serverless Spark jobs using Fargate"},{"location":"day2/emroneks/exercise/#deploy-kubernetes-dashboard","text":"Kubernetes Dashboard is a web-based user interface. You can use Dashboard to get an overview of applications running on your cluster. In this lab we will deploy the official Kubernetes Dashboard. Check the documentation here . To deploy the dashboard, run the following command: export DASHBOARD_VERSION = \"v2.0.0\" kubectl apply - f https : // raw . githubusercontent . com / kubernetes / dashboard /$ { DASHBOARD_VERSION } / aio / deploy / recommended . yaml You can access Dashboard using the kubectl command-line tool by running the following command in your Cloud9 terminal. This will start the proxy on port 8080. kubectl proxy --port=8080 --address=0.0.0.0 --disable-filter=true & In your Cloud9 workspace, click Tools / Preview / Preview Running Application. Scroll to the end of the browser address URL and append the following: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ You will see the Kubernetes dashboard asking for token. Once you have the login screen in the Cloud9 preview browser tab, press the Pop Out button to open the login screen in a regular browser tab, like below: To get the token, you need to create an EKS admin account. Let's do that by running following commands on Cloud9. sudo tee ./eks-admin-service-account.yaml >/dev/null <<EOF apiVersion: v1 kind: ServiceAccount metadata: name: eks-admin namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: eks-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: eks-admin namespace: kube-system EOF kubectl apply -f eks-admin-service-account.yaml Now, run the below command in Cloud9 and paste the \"token\" of the output on to your browser where it prompts you to enter token. kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}') You will now be able to see the dashboard. Submit a job again. aws emr-containers start-job-run --virtual-cluster-id ${ fargate_vc } \\ --name spark-pi-5.34 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Explore the dashboard.","title":"Deploy Kubernetes Dashboard"},{"location":"day2/emroneks/exercise/#orchestrate-jobs-on-emr-on-eks-using-amazon-mwaa","text":"Let's submit (or orchestrate) jobs to EMR on EKS through Amazon MWAA. Go to the MWAA console. You will find a new environment starting with name \"emr-on-eks-MWAAStack\". Open the airflow UI. You will see two DAGs. One for EC2 namespace and one for fargate namespace. Toggle the DAGs ON and trigger the DAGs manually. This job copies, unzips and transforms files from S3 and then runs some analytics on top of this transformed data. This DAG takes about 10-12 mins end-to-end. You can check the status in EMR on EKS console.","title":"Orchestrate jobs on EMR on EKS using Amazon MWAA"},{"location":"day2/emroneks/exercise/#single-az-placement","text":"Our EMR on EKS cluster uses two AZs: us-east-1a and us-east-1b. Go to the Nodes section in Kubernetes dashboard to see which EC2 nodes belong to which AZ. This provides resiliency when compared to EMR on EC2 which runs on a single AZ. However, cross-AZ communication may impact performance and cost. You can define topology for your jobs and instruct all pods to launch in a single AZ. Let's see an example. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-single-az-us-east-1a \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.kubernetes.node.selector.topology.kubernetes.io/zone='us-east-1a' --conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Check the Pods section of Kubernetes dashboard. You will see that the pods are launched only on nodes running in us-east-1a. You can test the same command by substituting us-east-1a with us-east-1b and see how the pods are getting placed in your Kubernetes dashboard. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-single-az-us-east-1a \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.kubernetes.node.selector.topology.kubernetes.io/zone='us-east-1b' --conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1","title":"Single AZ placement"},{"location":"day2/mwaa/exercise/","text":"Exercise 1 - Orchestrating Notebook Pipelines using Amazon MWAA \u00b6 In the previous Amazon EMR Studio exercise, you ran a parameterized notebook programmatically using start-notebook-execution API. In this exercise, we are going to orchestrate a pipeline with the same parameterized notebook find_best_sellers.ipynb using Amazon Managed Workflows for Apache Airflow Go to your EMR Studio Workspace and make sure that you have \"find_best_sellers.ipynb\" under workspace-repo/files/notebook. If you uploaded the file manually, go to the path where you uploaded the file and make sure that the first cell in that notebook is tagged as parameters (View -> Show Right Bar -> Advanced Tools). If this cell is not tagged as \"parameters\", re-do the parameterized notebooks section of Day 1 Exercises. Once you have confirmed that, login to the EMR leader node of the cluster \"EMR-Spark-Hive-Presto\" or EC2 JumpHost Session Manager session (EC2 Web console -> Click on JumpHost -> Connect -> Session Manager -> Connect), run the following commands. Replace youraccountID with your AWS account ID. sudo su ec2 - user cd ~ curl - o test_dag . py https : // raw . githubusercontent . com / vasveena / amazon - emr - ttt - workshop / main / files / dags / test_dag . py instanceProfileRole = $ ( aws iam list - instance - profiles - for - role -- role - name emrEc2InstanceProfileRole | jq . ' InstanceProfiles[].InstanceProfileName ' | sed \" s|\\ \" || g \" ) sed - i \" s|emrEc2InstanceProfileRole|$instanceProfileRole|g \" test_dag . py aws s3 cp test_dag . py s3 : // airflow - youraccountID - dayone / dags / Go to the Managed Apache Airflow Web Console. (AWS Web Console -> Search for MWAA -> Select Managed Apache Airflow). You will be able to see an Managed Airflow environment named \"mwaa\". Click on \"Open Airflow UI\". You will be taken to the Managed Airflow UI. Ignore the DAG Import Errors for now. Create a file called \"variables.json\" like below using a notepad or a vi editor. { \"REGION\": \"us-east-1\", \"SUBNET_ID\": \"<subnet-id>\", \"EMR_LOG_URI\": \"s3://mrworkshop-youraccountID-dayone/\", \"NOTEBOOK_ID\": \"e-XXXXXXXXXXXXXXXXXXX\", \"NOTEBOOK_FILE_NAME\": \"workshop-repo/files/notebook/find_best_sellers.ipynb\", \"CATEGORIES_CSV\": \"Apparel,Automotive,Baby,Beauty,Books\", \"FROM_DATE\": \"2015-08-25\", \"TO_DATE\": \"2015-08-31\", \"OUTPUT_LOCATION\": \"s3://mrworkshop-youraccountID-dayone/mwaa/\" } Replace youraccountID with your AWS Event Engine account ID. For the subnet ID, choose the subnet of the cluster \"EMR-Spark-Hive-Presto\" (AWS Web Console -> EMR -> EMR-Spark-Hive-Presto -> Summary tab -> Network and hardware section). For the values of NOTEBOOK_ID and NOTEBOOK_FILE_NAME, use the same values you used in the Parameterized Notebooks exercise. i.e., take these values from your workspace URL (or from the API command you ran if you have saved it somewhere). For example: https://e-4ac2fwhw1liin22ezilly60j8.emrnotebooks-prod.us-east-1.amazonaws.com/e-4AC2FWHW1LIIN22EZILLY60J8/lab/tree/workshop-repo/files/notebook/find_best_sellers.ipynb An example variables.json file with values entered: { \"REGION\": \"us-east-1\", \"SUBNET_ID\": \"subnet-096759acf495c85df\", \"EMR_LOG_URI\": \"s3://mrworkshop-880847545464-dayone/\", \"NOTEBOOK_ID\": \"e-4AC2FWHW1LIIN22EZILLY60J8\", \"NOTEBOOK_FILE_NAME\": \"workshop-repo/files/notebook/find_best_sellers.ipynb\", \"CATEGORIES_CSV\": \"Apparel,Automotive,Baby,Beauty,Books\", \"FROM_DATE\": \"2015-08-25\", \"TO_DATE\": \"2015-08-31\", \"OUTPUT_LOCATION\": \"s3://mrworkshop-880847545464-dayone/mwaa/\" } Now let's upload this file into the Airflow UI. Go to Airflow UI -> Admin -> Variables. Click on \"Choose File\". Choose the variables.json you created from your local desktop and click on \"Import Variables\" to import the file. Now go to the DAGs on the top left corner and you should be able to see your DAG \"test_dag\". Turn on the DAG by using the Toggle switch. This DAG will execute once at the start of each hour (based on cron schedule: 0 * * ). For now, let us execute the DAG manually. Click on \"Trigger DAG\". Trigger the DAG. Your DAG will start to execute. Click on the DAG runs -> DAG ID -> test_dag. You will see the graph view of this execution. Analyze the steps in this DAG. Also, analyze the DAG code by clicking on \"Code\". Now, if you go to the EMR Web Console, you can see a new cluster called \"Test-Cluster\" being launched. The DAG steps will create a new cluster, submit the notebook API as pipeline and terminate the cluster once the job is finished. From start to finish, this DAG will take about 15-20 mins to complete. Please note that it is not necessary to have your EMR Studio Workspace attached to an EMR cluster to be able to run this notebook pipeline since your notebooks will be persisted in the S3 location of your EMR Studio. After 15 mins, check the DAG execution status. Now, let's check the S3 location you provided as parameter \"OUTPUT_LOCATION\" in your variables.json. This is where your job output is going to be stored. Run the below command on EC2 JumpHost Session Manager session. Replace youraccountID with your AWS event engine account ID. aws s3 ls s3://mrworkshop-156321623241-dayone/mwaa/ Alternatively, you can go to the S3 Web Console and check this location in the console as well. You should be able to see the output files for the 5 categories we passed as parameter in variables.json: \"Apparel,Automotive,Baby,Beauty,Books\" After the job is finished, the cluster \"Test-cluster\" will be automatically terminated. This DAG will be executed once every hour automatically. You can play around by changing the notebook parameters or pipeline schedule. You can stop the DAG by using the toggle switch and turn it OFF. Another way to do this is by using AWS Step Functions. Next day's session, we will orchestrate Hive ETL pipelines using AWS Step Functions to demonstrate this capability.","title":"1 - Orchestration using Amazon MWAA"},{"location":"day2/mwaa/exercise/#exercise-1-orchestrating-notebook-pipelines-using-amazon-mwaa","text":"In the previous Amazon EMR Studio exercise, you ran a parameterized notebook programmatically using start-notebook-execution API. In this exercise, we are going to orchestrate a pipeline with the same parameterized notebook find_best_sellers.ipynb using Amazon Managed Workflows for Apache Airflow Go to your EMR Studio Workspace and make sure that you have \"find_best_sellers.ipynb\" under workspace-repo/files/notebook. If you uploaded the file manually, go to the path where you uploaded the file and make sure that the first cell in that notebook is tagged as parameters (View -> Show Right Bar -> Advanced Tools). If this cell is not tagged as \"parameters\", re-do the parameterized notebooks section of Day 1 Exercises. Once you have confirmed that, login to the EMR leader node of the cluster \"EMR-Spark-Hive-Presto\" or EC2 JumpHost Session Manager session (EC2 Web console -> Click on JumpHost -> Connect -> Session Manager -> Connect), run the following commands. Replace youraccountID with your AWS account ID. sudo su ec2 - user cd ~ curl - o test_dag . py https : // raw . githubusercontent . com / vasveena / amazon - emr - ttt - workshop / main / files / dags / test_dag . py instanceProfileRole = $ ( aws iam list - instance - profiles - for - role -- role - name emrEc2InstanceProfileRole | jq . ' InstanceProfiles[].InstanceProfileName ' | sed \" s|\\ \" || g \" ) sed - i \" s|emrEc2InstanceProfileRole|$instanceProfileRole|g \" test_dag . py aws s3 cp test_dag . py s3 : // airflow - youraccountID - dayone / dags / Go to the Managed Apache Airflow Web Console. (AWS Web Console -> Search for MWAA -> Select Managed Apache Airflow). You will be able to see an Managed Airflow environment named \"mwaa\". Click on \"Open Airflow UI\". You will be taken to the Managed Airflow UI. Ignore the DAG Import Errors for now. Create a file called \"variables.json\" like below using a notepad or a vi editor. { \"REGION\": \"us-east-1\", \"SUBNET_ID\": \"<subnet-id>\", \"EMR_LOG_URI\": \"s3://mrworkshop-youraccountID-dayone/\", \"NOTEBOOK_ID\": \"e-XXXXXXXXXXXXXXXXXXX\", \"NOTEBOOK_FILE_NAME\": \"workshop-repo/files/notebook/find_best_sellers.ipynb\", \"CATEGORIES_CSV\": \"Apparel,Automotive,Baby,Beauty,Books\", \"FROM_DATE\": \"2015-08-25\", \"TO_DATE\": \"2015-08-31\", \"OUTPUT_LOCATION\": \"s3://mrworkshop-youraccountID-dayone/mwaa/\" } Replace youraccountID with your AWS Event Engine account ID. For the subnet ID, choose the subnet of the cluster \"EMR-Spark-Hive-Presto\" (AWS Web Console -> EMR -> EMR-Spark-Hive-Presto -> Summary tab -> Network and hardware section). For the values of NOTEBOOK_ID and NOTEBOOK_FILE_NAME, use the same values you used in the Parameterized Notebooks exercise. i.e., take these values from your workspace URL (or from the API command you ran if you have saved it somewhere). For example: https://e-4ac2fwhw1liin22ezilly60j8.emrnotebooks-prod.us-east-1.amazonaws.com/e-4AC2FWHW1LIIN22EZILLY60J8/lab/tree/workshop-repo/files/notebook/find_best_sellers.ipynb An example variables.json file with values entered: { \"REGION\": \"us-east-1\", \"SUBNET_ID\": \"subnet-096759acf495c85df\", \"EMR_LOG_URI\": \"s3://mrworkshop-880847545464-dayone/\", \"NOTEBOOK_ID\": \"e-4AC2FWHW1LIIN22EZILLY60J8\", \"NOTEBOOK_FILE_NAME\": \"workshop-repo/files/notebook/find_best_sellers.ipynb\", \"CATEGORIES_CSV\": \"Apparel,Automotive,Baby,Beauty,Books\", \"FROM_DATE\": \"2015-08-25\", \"TO_DATE\": \"2015-08-31\", \"OUTPUT_LOCATION\": \"s3://mrworkshop-880847545464-dayone/mwaa/\" } Now let's upload this file into the Airflow UI. Go to Airflow UI -> Admin -> Variables. Click on \"Choose File\". Choose the variables.json you created from your local desktop and click on \"Import Variables\" to import the file. Now go to the DAGs on the top left corner and you should be able to see your DAG \"test_dag\". Turn on the DAG by using the Toggle switch. This DAG will execute once at the start of each hour (based on cron schedule: 0 * * ). For now, let us execute the DAG manually. Click on \"Trigger DAG\". Trigger the DAG. Your DAG will start to execute. Click on the DAG runs -> DAG ID -> test_dag. You will see the graph view of this execution. Analyze the steps in this DAG. Also, analyze the DAG code by clicking on \"Code\". Now, if you go to the EMR Web Console, you can see a new cluster called \"Test-Cluster\" being launched. The DAG steps will create a new cluster, submit the notebook API as pipeline and terminate the cluster once the job is finished. From start to finish, this DAG will take about 15-20 mins to complete. Please note that it is not necessary to have your EMR Studio Workspace attached to an EMR cluster to be able to run this notebook pipeline since your notebooks will be persisted in the S3 location of your EMR Studio. After 15 mins, check the DAG execution status. Now, let's check the S3 location you provided as parameter \"OUTPUT_LOCATION\" in your variables.json. This is where your job output is going to be stored. Run the below command on EC2 JumpHost Session Manager session. Replace youraccountID with your AWS event engine account ID. aws s3 ls s3://mrworkshop-156321623241-dayone/mwaa/ Alternatively, you can go to the S3 Web Console and check this location in the console as well. You should be able to see the output files for the 5 categories we passed as parameter in variables.json: \"Apparel,Automotive,Baby,Beauty,Books\" After the job is finished, the cluster \"Test-cluster\" will be automatically terminated. This DAG will be executed once every hour automatically. You can play around by changing the notebook parameters or pipeline schedule. You can stop the DAG by using the toggle switch and turn it OFF. Another way to do this is by using AWS Step Functions. Next day's session, we will orchestrate Hive ETL pipelines using AWS Step Functions to demonstrate this capability.","title":"Exercise 1 - Orchestrating Notebook Pipelines using Amazon MWAA"},{"location":"day2/serverless/exercise/","text":"Exercise 2.2 - Amazon EMR Serverless \u00b6 In this exercise, you will run Hive and Spark applications using EMR Serverless. To run the exercises in this section, you need to use your own AWS account. Since the feature is still in preview (as of writing this), you will not be charged. However, your AWS account should have been whitelisted for using EMR Serverless feature to be able to run these exercises. If your AWS account is not whitelisted yet, you can sign up for preview of this feature using this sign up form. Once your AWS account is whitelisted, you can come back to this exercise and run them. Since you are using your own AWS accounts, you can run this exercise even after the workshop. You will need latest version of AWS CLI and jq installed to run these exercises. Run all the commands in the terminal of your local desktop (or export your AWS credentials and run them in an EC2 instance). Let's first create an IAM role that we are going to use for job submission. Go to IAM Console -> Roles -> Create Role. Under \"Select Trusted Entity\", choose \"Custom Trust Policy\" and paste the following: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"EMRServerlessTrustPolicy\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"emr-serverless.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] } In the Permissions Policy section, choose a policy that will be used to access resources from your jobs. For time being, you can attach AdministratorAccess managed policy. But it is not recommended to do so other than for testing purpose. Always make sure to give the least privileges possible. In next step, name your role. You can name this role \"sampleJobExecutionRole\". Create the role. Spark on EMR Serverless \u00b6 List the applications. aws emr - serverless list - applications -- region us - east - 1 Create a new Spark application with min and and max limits for vCPU and memory. result = $ ( aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 6.5.0 - preview \\ -- type ' SPARK ' \\ -- initial - capacity '{ \"DRIVER\" : { \"workerCount\" : 5 , \"resourceConfiguration\" : { \"cpu\" : \"2vCPU\" , \"memory\" : \"4GB\" } }, \"EXECUTOR\" : { \"workerCount\" : 50 , \"resourceConfiguration\" : { \"cpu\" : \"4vCPU\" , \"memory\" : \"8GB\" } } }' \\ -- maximum - capacity '{ \"cpu\" : \"400vCPU\" , \"memory\" : \"1024GB\" }' \\ -- name spark - 6.5.0 - demo - application ) echo $result appID = $ ( echo $result | jq - r . applicationId ) Now let's start this application. aws --region us-east-1 emr-serverless start-application \\ --application-id ${ appID } You can get the status of the application with below command. aws --region us-east-1 emr-serverless get-application \\ --application-id ${ appID } The state will be STARTING. It will take about 1-2 minutes for the status to become STARTED. Once the status becomes STARTED, lets submit a job to this application. Get the ARN for the execution role you created. Also, create or use an existing S3 bucket in your own account in the same region where you are running these commands (us-east-1). serverlessArn=$(aws iam get-role --role-name sampleJobExecutionRole | jq -r .'Role | .Arn') s3bucket = 'yours3bucketname' Let's use the following Spark code (same job we ran for EMR on EKS). You don't have to copy this script anywhere. This is just for your reference. import sys from datetime import datetime from pyspark.sql import SparkSession from pyspark.sql import SQLContext from pyspark.sql.functions import * if __name__ == \"__main__\" : print ( len ( sys . argv )) if ( len ( sys . argv ) != 4 ): print ( \"Usage: spark-etl-glue [input-folder] [output-folder] [dbName]\" ) sys . exit ( 0 ) spark = SparkSession \\ . builder \\ . appName ( \"Python Spark SQL Glue integration example\" ) \\ . enableHiveSupport () \\ . getOrCreate () nyTaxi = spark . read . option ( \"inferSchema\" , \"true\" ) . option ( \"header\" , \"true\" ) . csv ( sys . argv [ 1 ]) updatedNYTaxi = nyTaxi . withColumn ( \"current_date\" , lit ( datetime . now ())) updatedNYTaxi . printSchema () print ( updatedNYTaxi . show ()) print ( \"Total number of records: \" + str ( updatedNYTaxi . count ())) updatedNYTaxi . write . parquet ( sys . argv [ 2 ]) updatedNYTaxi . registerTempTable ( \"ny_taxi_table\" ) dbName = sys . argv [ 3 ] spark . sql ( \"CREATE database if not exists \" + dbName ) spark . sql ( \"USE \" + dbName ) spark . sql ( \"CREATE table if not exists ny_taxi_parquet USING PARQUET LOCATION '\" + sys . argv [ 2 ] + \"' AS SELECT * from ny_taxi_table \" ) Submit the job using following command. result=$(echo \"aws --region us-east-1 emr-serverless start-job-run \\ --application-id ${ appID } \\ --execution-role-arn ${ serverlessArn } \\ --job-driver '{ \\\"sparkSubmit\\\": { \\\"entryPoint\\\": \\\"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/spark-etl-glue.py\\\", \\\"entryPointArguments\\\": [ \\\"s3://aws-data-analytics-workshops/shared_datasets/tripdata/\\\",\\\"s3:// $s3bucket /emrserverless/taxi-data-glue/\\\",\\\"tripdata\\\" ], \\\"sparkSubmitParameters\\\": \\\"--conf spark.executor.cores=1 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1\\\" } }' \\ --configuration-overrides '{ \\\"applicationConfiguration\\\": [{ \\\"classification\\\": \\\"spark-defaults\\\", \\\"properties\\\": { \\\"spark.dynamicAllocation.enabled\\\": \\\"false\\\", \\\"spark.hadoop.hive.metastore.client.factory.class\\\": \\\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\\\" } }], \\\"monitoringConfiguration\\\": { \\\"s3MonitoringConfiguration\\\": { \\\"logUri\\\": \\\"s3:// $s3bucket /emrserverless/logs\\\" } } }'\" | bash ) Get the job run ID. jobID=$(echo $result | jq -r .'jobRunId') You can get the status of our job using the following command. aws -- region us - east - 1 emr - serverless get - job - run \\ -- application - id $appID \\ -- job - run - id $jobID You will see the job being scheduled. Wait for the status to go from SCHEDULED to RUNNING to SUCCESS. Now check the S3 location for logs and output. Download the stderr and stdout logs from Spark driver and inspect them. aws s3 cp s3:// $s3bucket /emrserverless/logs/applications/ ${ appID } /jobs/ ${ jobID } /SPARK_DRIVER/stderr.gz . aws s3 cp s3:// $s3bucket /emrserverless/logs/applications/ ${ appID } /jobs/ ${ jobID } /SPARK_DRIVER/stdout.gz . You will see the job output in stdout and driver execution logs in stderr. Similarly, you can check out executor logs as well. Hive on EMR Serverless \u00b6 EMR Serverless supports Hive applications also. Let's start by creating a Hive application. result = $ ( aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 6.5.0 - preview \\ -- initial - capacity '{ \"DRIVER\" : { \"workerCount\" : 5 , \"resourceConfiguration\" : { \"cpu\" : \"2vCPU\" , \"memory\" : \"4GB\" } }, \"TEZ_TASK\" : { \"workerCount\" : 50 , \"resourceConfiguration\" : { \"cpu\" : \"4vCPU\" , \"memory\" : \"8GB\" } } }' \\ -- maximum - capacity '{ \"cpu\" : \"400vCPU\" , \"memory\" : \"1024GB\" }' \\ -- type ' HIVE ' \\ -- name hive - 6.5.0 - demo - application ) echo $result appID = $ ( echo $result | jq - r . applicationId ) Start the application. aws --region us-east-1 emr-serverless start-application \\ --application-id ${ appID } Get application status. aws --region us-east-1 emr-serverless get-application \\ --application-id ${ appID } Once the application status becomes STARTED, submit a Hive job. For this, create a file called \"hive-query.ql\" with following contents: create database if not exists emrserverless ; use emrserverless ; create table if not exists test_table ( id int ) ; drop table if exists Values__Tmp__Table__1 ; insert into test_table values ( 1 ) , ( 2 ) , ( 2 ) , ( 3 ) , ( 3 ) , ( 3 ) ; select id , count ( id ) from test_table group by id order by id desc ; Upload this file to an S3 location. You can use the same S3 bucket with a different prefix. aws s3 cp hive-query.ql s3://$s3bucket/emrserverless/scripts/hive/ Now let's submit a Hive job to this application. result=$(echo \"aws --region us-east-1 emr-serverless start-job-run \\ --application-id ${ appID } \\ --execution-role-arn ${ serverlessArn } \\ --job-driver '{ \\\"hive\\\": { \\\"query\\\": \\\"s3:// $s3bucket /emr-serverless-hive/query/hive-query.ql\\\", \\\"parameters\\\": \\\"--hiveconf hive.root.logger=DEBUG,DRFA\\\" } }' \\ --configuration-overrides '{ \\\"applicationConfiguration\\\": [{ \\\"classification\\\": \\\"hive-site\\\", \\\"properties\\\": { \\\"hive.exec.scratchdir\\\": \\\"s3:// $s3bucket /emr-serverless-hive/hive/scratch\\\", \\\"hive.metastore.warehouse.dir\\\": \"s3:// $s3bucket /emr-serverless-hive/hive/warehouse\\\", \\\"hive.driver.cores\\\": \\\"2\\\", \\\"hive.driver.memory\\\": \\\"4g\\\", \\\"hive.tez.container.size\\\": \\\"4096\\\", \\\"hive.tez.cpu.vcores\\\": \\\"1\\\" } }], \\\"monitoringConfiguration\\\": { \\\"s3MonitoringConfiguration\\\": { \\\"logUri\\\": \\\"s3:// $s3bucket /emr-serverless-hive/logs/\\\" } } }'\" | bash ) Get the job run ID. jobID=$(echo $result | jq -r .'jobRunId') You can get the status of our job using the following command. aws -- region us - east - 1 emr - serverless get - job - run \\ -- application - id $appID \\ -- job - run - id $jobID","title":"2.2 - Amazon EMR Serverless"},{"location":"day2/serverless/exercise/#exercise-22-amazon-emr-serverless","text":"In this exercise, you will run Hive and Spark applications using EMR Serverless. To run the exercises in this section, you need to use your own AWS account. Since the feature is still in preview (as of writing this), you will not be charged. However, your AWS account should have been whitelisted for using EMR Serverless feature to be able to run these exercises. If your AWS account is not whitelisted yet, you can sign up for preview of this feature using this sign up form. Once your AWS account is whitelisted, you can come back to this exercise and run them. Since you are using your own AWS accounts, you can run this exercise even after the workshop. You will need latest version of AWS CLI and jq installed to run these exercises. Run all the commands in the terminal of your local desktop (or export your AWS credentials and run them in an EC2 instance). Let's first create an IAM role that we are going to use for job submission. Go to IAM Console -> Roles -> Create Role. Under \"Select Trusted Entity\", choose \"Custom Trust Policy\" and paste the following: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"EMRServerlessTrustPolicy\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"emr-serverless.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] } In the Permissions Policy section, choose a policy that will be used to access resources from your jobs. For time being, you can attach AdministratorAccess managed policy. But it is not recommended to do so other than for testing purpose. Always make sure to give the least privileges possible. In next step, name your role. You can name this role \"sampleJobExecutionRole\". Create the role.","title":"Exercise 2.2 - Amazon EMR Serverless"},{"location":"day2/serverless/exercise/#spark-on-emr-serverless","text":"List the applications. aws emr - serverless list - applications -- region us - east - 1 Create a new Spark application with min and and max limits for vCPU and memory. result = $ ( aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 6.5.0 - preview \\ -- type ' SPARK ' \\ -- initial - capacity '{ \"DRIVER\" : { \"workerCount\" : 5 , \"resourceConfiguration\" : { \"cpu\" : \"2vCPU\" , \"memory\" : \"4GB\" } }, \"EXECUTOR\" : { \"workerCount\" : 50 , \"resourceConfiguration\" : { \"cpu\" : \"4vCPU\" , \"memory\" : \"8GB\" } } }' \\ -- maximum - capacity '{ \"cpu\" : \"400vCPU\" , \"memory\" : \"1024GB\" }' \\ -- name spark - 6.5.0 - demo - application ) echo $result appID = $ ( echo $result | jq - r . applicationId ) Now let's start this application. aws --region us-east-1 emr-serverless start-application \\ --application-id ${ appID } You can get the status of the application with below command. aws --region us-east-1 emr-serverless get-application \\ --application-id ${ appID } The state will be STARTING. It will take about 1-2 minutes for the status to become STARTED. Once the status becomes STARTED, lets submit a job to this application. Get the ARN for the execution role you created. Also, create or use an existing S3 bucket in your own account in the same region where you are running these commands (us-east-1). serverlessArn=$(aws iam get-role --role-name sampleJobExecutionRole | jq -r .'Role | .Arn') s3bucket = 'yours3bucketname' Let's use the following Spark code (same job we ran for EMR on EKS). You don't have to copy this script anywhere. This is just for your reference. import sys from datetime import datetime from pyspark.sql import SparkSession from pyspark.sql import SQLContext from pyspark.sql.functions import * if __name__ == \"__main__\" : print ( len ( sys . argv )) if ( len ( sys . argv ) != 4 ): print ( \"Usage: spark-etl-glue [input-folder] [output-folder] [dbName]\" ) sys . exit ( 0 ) spark = SparkSession \\ . builder \\ . appName ( \"Python Spark SQL Glue integration example\" ) \\ . enableHiveSupport () \\ . getOrCreate () nyTaxi = spark . read . option ( \"inferSchema\" , \"true\" ) . option ( \"header\" , \"true\" ) . csv ( sys . argv [ 1 ]) updatedNYTaxi = nyTaxi . withColumn ( \"current_date\" , lit ( datetime . now ())) updatedNYTaxi . printSchema () print ( updatedNYTaxi . show ()) print ( \"Total number of records: \" + str ( updatedNYTaxi . count ())) updatedNYTaxi . write . parquet ( sys . argv [ 2 ]) updatedNYTaxi . registerTempTable ( \"ny_taxi_table\" ) dbName = sys . argv [ 3 ] spark . sql ( \"CREATE database if not exists \" + dbName ) spark . sql ( \"USE \" + dbName ) spark . sql ( \"CREATE table if not exists ny_taxi_parquet USING PARQUET LOCATION '\" + sys . argv [ 2 ] + \"' AS SELECT * from ny_taxi_table \" ) Submit the job using following command. result=$(echo \"aws --region us-east-1 emr-serverless start-job-run \\ --application-id ${ appID } \\ --execution-role-arn ${ serverlessArn } \\ --job-driver '{ \\\"sparkSubmit\\\": { \\\"entryPoint\\\": \\\"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/spark-etl-glue.py\\\", \\\"entryPointArguments\\\": [ \\\"s3://aws-data-analytics-workshops/shared_datasets/tripdata/\\\",\\\"s3:// $s3bucket /emrserverless/taxi-data-glue/\\\",\\\"tripdata\\\" ], \\\"sparkSubmitParameters\\\": \\\"--conf spark.executor.cores=1 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1\\\" } }' \\ --configuration-overrides '{ \\\"applicationConfiguration\\\": [{ \\\"classification\\\": \\\"spark-defaults\\\", \\\"properties\\\": { \\\"spark.dynamicAllocation.enabled\\\": \\\"false\\\", \\\"spark.hadoop.hive.metastore.client.factory.class\\\": \\\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\\\" } }], \\\"monitoringConfiguration\\\": { \\\"s3MonitoringConfiguration\\\": { \\\"logUri\\\": \\\"s3:// $s3bucket /emrserverless/logs\\\" } } }'\" | bash ) Get the job run ID. jobID=$(echo $result | jq -r .'jobRunId') You can get the status of our job using the following command. aws -- region us - east - 1 emr - serverless get - job - run \\ -- application - id $appID \\ -- job - run - id $jobID You will see the job being scheduled. Wait for the status to go from SCHEDULED to RUNNING to SUCCESS. Now check the S3 location for logs and output. Download the stderr and stdout logs from Spark driver and inspect them. aws s3 cp s3:// $s3bucket /emrserverless/logs/applications/ ${ appID } /jobs/ ${ jobID } /SPARK_DRIVER/stderr.gz . aws s3 cp s3:// $s3bucket /emrserverless/logs/applications/ ${ appID } /jobs/ ${ jobID } /SPARK_DRIVER/stdout.gz . You will see the job output in stdout and driver execution logs in stderr. Similarly, you can check out executor logs as well.","title":"Spark on EMR Serverless"},{"location":"day2/serverless/exercise/#hive-on-emr-serverless","text":"EMR Serverless supports Hive applications also. Let's start by creating a Hive application. result = $ ( aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 6.5.0 - preview \\ -- initial - capacity '{ \"DRIVER\" : { \"workerCount\" : 5 , \"resourceConfiguration\" : { \"cpu\" : \"2vCPU\" , \"memory\" : \"4GB\" } }, \"TEZ_TASK\" : { \"workerCount\" : 50 , \"resourceConfiguration\" : { \"cpu\" : \"4vCPU\" , \"memory\" : \"8GB\" } } }' \\ -- maximum - capacity '{ \"cpu\" : \"400vCPU\" , \"memory\" : \"1024GB\" }' \\ -- type ' HIVE ' \\ -- name hive - 6.5.0 - demo - application ) echo $result appID = $ ( echo $result | jq - r . applicationId ) Start the application. aws --region us-east-1 emr-serverless start-application \\ --application-id ${ appID } Get application status. aws --region us-east-1 emr-serverless get-application \\ --application-id ${ appID } Once the application status becomes STARTED, submit a Hive job. For this, create a file called \"hive-query.ql\" with following contents: create database if not exists emrserverless ; use emrserverless ; create table if not exists test_table ( id int ) ; drop table if exists Values__Tmp__Table__1 ; insert into test_table values ( 1 ) , ( 2 ) , ( 2 ) , ( 3 ) , ( 3 ) , ( 3 ) ; select id , count ( id ) from test_table group by id order by id desc ; Upload this file to an S3 location. You can use the same S3 bucket with a different prefix. aws s3 cp hive-query.ql s3://$s3bucket/emrserverless/scripts/hive/ Now let's submit a Hive job to this application. result=$(echo \"aws --region us-east-1 emr-serverless start-job-run \\ --application-id ${ appID } \\ --execution-role-arn ${ serverlessArn } \\ --job-driver '{ \\\"hive\\\": { \\\"query\\\": \\\"s3:// $s3bucket /emr-serverless-hive/query/hive-query.ql\\\", \\\"parameters\\\": \\\"--hiveconf hive.root.logger=DEBUG,DRFA\\\" } }' \\ --configuration-overrides '{ \\\"applicationConfiguration\\\": [{ \\\"classification\\\": \\\"hive-site\\\", \\\"properties\\\": { \\\"hive.exec.scratchdir\\\": \\\"s3:// $s3bucket /emr-serverless-hive/hive/scratch\\\", \\\"hive.metastore.warehouse.dir\\\": \"s3:// $s3bucket /emr-serverless-hive/hive/warehouse\\\", \\\"hive.driver.cores\\\": \\\"2\\\", \\\"hive.driver.memory\\\": \\\"4g\\\", \\\"hive.tez.container.size\\\": \\\"4096\\\", \\\"hive.tez.cpu.vcores\\\": \\\"1\\\" } }], \\\"monitoringConfiguration\\\": { \\\"s3MonitoringConfiguration\\\": { \\\"logUri\\\": \\\"s3:// $s3bucket /emr-serverless-hive/logs/\\\" } } }'\" | bash ) Get the job run ID. jobID=$(echo $result | jq -r .'jobRunId') You can get the status of our job using the following command. aws -- region us - east - 1 emr - serverless get - job - run \\ -- application - id $appID \\ -- job - run - id $jobID","title":"Hive on EMR Serverless"}]}