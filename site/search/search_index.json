{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Amazon EMR Train-The-Trainer Workshop \u00b6 This workshop contains exercises for the 3-day Amazon EMR Workshop Training. Exercises \u00b6 Part 1 \u00b6 Launch EMR cluster with Managed Scaling and Fleets Run Spark workloads on Amazon EMR Orchestrate EMR Steps using AWS Step Functions Part 2 \u00b6 Amazon EMR Studio Dive Deep Orchestrate notebook pipelines using Amazon EMR Studio and Amazon MWAA Integrate with Amazon Sagemaker Studio Part 3 \u00b6 Build transactional data lakes using Apache Hudi Build transactional data lakes using Apache Iceberg Part 4 \u00b6 Run Spark workloads using Amazon EMR on EKS Run Spark and Hive workloads on EMR Serverless (preview)","title":"Introduction"},{"location":"#welcome-to-amazon-emr-train-the-trainer-workshop","text":"This workshop contains exercises for the 3-day Amazon EMR Workshop Training.","title":"Welcome to Amazon EMR Train-The-Trainer Workshop"},{"location":"#exercises","text":"","title":"Exercises"},{"location":"#part-1","text":"Launch EMR cluster with Managed Scaling and Fleets Run Spark workloads on Amazon EMR Orchestrate EMR Steps using AWS Step Functions","title":"Part 1"},{"location":"#part-2","text":"Amazon EMR Studio Dive Deep Orchestrate notebook pipelines using Amazon EMR Studio and Amazon MWAA Integrate with Amazon Sagemaker Studio","title":"Part 2"},{"location":"#part-3","text":"Build transactional data lakes using Apache Hudi Build transactional data lakes using Apache Iceberg","title":"Part 3"},{"location":"#part-4","text":"Run Spark workloads using Amazon EMR on EKS Run Spark and Hive workloads on EMR Serverless (preview)","title":"Part 4"},{"location":"setup/","text":"Setup \u00b6 Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). Enter the hash provided to you. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation templates \"dayone\", \"emr-on-eks\" and \"smstudio\" are created.","title":"Setup"},{"location":"setup/#setup","text":"Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). Enter the hash provided to you. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation templates \"dayone\", \"emr-on-eks\" and \"smstudio\" are created.","title":"Setup"},{"location":"day1/fleet/exercise/","text":"Launching an EMR cluster \u00b6 This exercise is meant to show you different options and features available while trying to create an EMR cluster. EMR clusters required are already created in your event engine accounts which we will use for our exercises. Create EMR Cluster with Instance Fleets \u00b6 Go to the EMR Web Console (Right click -> Open Link in New Tab). Click on \u201cCreate cluster\u201d Click on \u201cGo to advanced options\u201d Explore the options on all 4 Steps. Step 1: Software and Steps \u00b6 Choose latest release label: EMR 6.5.0. Look at the applications available. Choose Spark, Hive and Hue for example. You can choose Use multiple master nodes to improve cluster availability which will launch 3 X EMR Leader Nodes. You can use AWS Glue Data Catalog for Hive and Spark tables. Under Software Configurations, you can provide a JSON config to override default values. For example, you can use below JSON: [{ \"Classification\" : \"spark\" , \"Properties\" : { \"maximizeResourceAllocation\" : \"true\" } }, { \"classification\" : \"hive-site\" , \"properties\" : { \"hive.blobstore.use.output-committer\" : \"true\" } }, { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [{ \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_DATANODE_HEAPSIZE\" : \"2048\" , \"HADOOP_NAMENODE_OPTS\" : \"-XX:GCTimeRatio=19\" }, \"Configurations\" : [ ] }] } ] Select \"Run multiple steps at the same time to improve cluster utilization\". You can change the step concurrency as well which is defaulted to 10. You can submit a Spark step during cluster creation. But for now, we can leave it as is. We can also submit steps to a running EMR cluster. For transient or short-lived EMR clusters, you can add steps during cluster creation and choose to auto-terminate your clusters after last step completion using option \"Cluster auto-terminates\". For now, we can leave it at default \"Clusters enters waiting state\". Click on Next. Step 2: Hardware \u00b6 There are two types of Cluster Compositions: Uniform instance groups and Instance fleets. Uniform instance groups will allow you to provision only one instance type within a single node group. Also, this option will only look at a single subnet while provisioning clusters. Let's choose instance fleets which provide us more flexibility in terms of hardware configuration. Under Networking, leave it at default VPC and choose all the 6 EC2 subnets in that VPC. Allocation strategy option is an improved method of launching clusters with lowest-priced On-Demand instances and capacity-optimized Spot instances. This option is recommended for faster cluster provisioning, more accurate Spot instance allocation, and fewer Spot instance interruptions compared to default EMR instance fleet allocation. Select \"Apply allocation strategy\". Under Cluster Nodes and Instances, for leader node type, click on \"Add / remove instance types to fleet\" and choose m5.xlarge, m4.xlarge, c5.xlarge, c4.xlarge. Choose on-demand. For core node type, choose 8 on-demand units and 8 spot units and select m5.xlarge, m5.2xlarge, m4.xlarge, m4.2xlarge. Under Provisioning timeout after \"60\" minutes Spot unavailability, change 60 mins to 30 mins and choose \"Switch to On-Demand instances\" from the drop down. For task node type, choose 8 spot units and select r5.xlarge, r5.2xlarge, r4.xlarge, r4.2xlarge. You can choose up to 30 different instance types for each node type. Click on \"Enable cluster scaling\" and choose core and task units. Choose minimum=16, maximum=32, on-demand limit=16, maximum core node=16. You can also enable scaling after the cluster has been launched. Enabling auto-termination helps you save cost by terminating idle clusters. You can leave this enabled since we will not be using this cluster. You can change the EBS root volume. You can increase this value if you are installing many different applications on your cluster. For now, you do not need to change the value. Click on Next. Step 3: General Cluster Settings \u00b6 Choose a friendly name for your cluster. Keep logging, debugging and termination protection enabled. Add a type with key named \"type\" and value \"DEV\". You have the option to customize your EC2 AMI and specify the customized image during your cluster launch. This is especially useful for applying security patches or applying CIS/STIG compliance. For now, we can use default EC2 AMI for EMR. You can specify a custom bootstrap action to run a script on all your cluster nodes. For now, we can leave it empty. Click on Next. Step 4: Security \u00b6 Under Security Options, choose EC2 key pair \"ee-default-keypair\" which is the key pair we downloaded during event engine setup. You can define custom EMR Service IAM Role which will be used by the EMR control plane and custom EC2 IAM role to be assumed by all the nodes in your cluster. If you leave these values at default, the default IAM roles (EMR_DefaultRole and EMR_EC2_DefaultRole) will be automatically created during cluster creation. Leave it as is. You can create a new Security Configuration in your EMR Web Console and use it in the \"Security Configuration\" section. This is where you define encryption, authentication and authorization for your cluster. We will look into this in detail on Day 3. For now, you can leave it at default. You can provide custom EC2 security groups for your leader and worker node types. You can configure up to 5 security groups per node type. If you do not specify, default EC2 security groups will be automatically created for leader and worker node types. Click on \"Create Cluster\". After about 10 mins, you can observe that the EMR cluster is created and is in \"WAITING\" state. Check all the tabs to see the cluster configurations.","title":"1 - Launching EMR cluster with Instance Fleets"},{"location":"day1/fleet/exercise/#launching-an-emr-cluster","text":"This exercise is meant to show you different options and features available while trying to create an EMR cluster. EMR clusters required are already created in your event engine accounts which we will use for our exercises.","title":"Launching an EMR cluster"},{"location":"day1/fleet/exercise/#create-emr-cluster-with-instance-fleets","text":"Go to the EMR Web Console (Right click -> Open Link in New Tab). Click on \u201cCreate cluster\u201d Click on \u201cGo to advanced options\u201d Explore the options on all 4 Steps.","title":"Create EMR Cluster with Instance Fleets"},{"location":"day1/fleet/exercise/#step-1-software-and-steps","text":"Choose latest release label: EMR 6.5.0. Look at the applications available. Choose Spark, Hive and Hue for example. You can choose Use multiple master nodes to improve cluster availability which will launch 3 X EMR Leader Nodes. You can use AWS Glue Data Catalog for Hive and Spark tables. Under Software Configurations, you can provide a JSON config to override default values. For example, you can use below JSON: [{ \"Classification\" : \"spark\" , \"Properties\" : { \"maximizeResourceAllocation\" : \"true\" } }, { \"classification\" : \"hive-site\" , \"properties\" : { \"hive.blobstore.use.output-committer\" : \"true\" } }, { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [{ \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_DATANODE_HEAPSIZE\" : \"2048\" , \"HADOOP_NAMENODE_OPTS\" : \"-XX:GCTimeRatio=19\" }, \"Configurations\" : [ ] }] } ] Select \"Run multiple steps at the same time to improve cluster utilization\". You can change the step concurrency as well which is defaulted to 10. You can submit a Spark step during cluster creation. But for now, we can leave it as is. We can also submit steps to a running EMR cluster. For transient or short-lived EMR clusters, you can add steps during cluster creation and choose to auto-terminate your clusters after last step completion using option \"Cluster auto-terminates\". For now, we can leave it at default \"Clusters enters waiting state\". Click on Next.","title":"Step 1: Software and Steps"},{"location":"day1/fleet/exercise/#step-2-hardware","text":"There are two types of Cluster Compositions: Uniform instance groups and Instance fleets. Uniform instance groups will allow you to provision only one instance type within a single node group. Also, this option will only look at a single subnet while provisioning clusters. Let's choose instance fleets which provide us more flexibility in terms of hardware configuration. Under Networking, leave it at default VPC and choose all the 6 EC2 subnets in that VPC. Allocation strategy option is an improved method of launching clusters with lowest-priced On-Demand instances and capacity-optimized Spot instances. This option is recommended for faster cluster provisioning, more accurate Spot instance allocation, and fewer Spot instance interruptions compared to default EMR instance fleet allocation. Select \"Apply allocation strategy\". Under Cluster Nodes and Instances, for leader node type, click on \"Add / remove instance types to fleet\" and choose m5.xlarge, m4.xlarge, c5.xlarge, c4.xlarge. Choose on-demand. For core node type, choose 8 on-demand units and 8 spot units and select m5.xlarge, m5.2xlarge, m4.xlarge, m4.2xlarge. Under Provisioning timeout after \"60\" minutes Spot unavailability, change 60 mins to 30 mins and choose \"Switch to On-Demand instances\" from the drop down. For task node type, choose 8 spot units and select r5.xlarge, r5.2xlarge, r4.xlarge, r4.2xlarge. You can choose up to 30 different instance types for each node type. Click on \"Enable cluster scaling\" and choose core and task units. Choose minimum=16, maximum=32, on-demand limit=16, maximum core node=16. You can also enable scaling after the cluster has been launched. Enabling auto-termination helps you save cost by terminating idle clusters. You can leave this enabled since we will not be using this cluster. You can change the EBS root volume. You can increase this value if you are installing many different applications on your cluster. For now, you do not need to change the value. Click on Next.","title":"Step 2: Hardware"},{"location":"day1/fleet/exercise/#step-3-general-cluster-settings","text":"Choose a friendly name for your cluster. Keep logging, debugging and termination protection enabled. Add a type with key named \"type\" and value \"DEV\". You have the option to customize your EC2 AMI and specify the customized image during your cluster launch. This is especially useful for applying security patches or applying CIS/STIG compliance. For now, we can use default EC2 AMI for EMR. You can specify a custom bootstrap action to run a script on all your cluster nodes. For now, we can leave it empty. Click on Next.","title":"Step 3: General Cluster Settings"},{"location":"day1/fleet/exercise/#step-4-security","text":"Under Security Options, choose EC2 key pair \"ee-default-keypair\" which is the key pair we downloaded during event engine setup. You can define custom EMR Service IAM Role which will be used by the EMR control plane and custom EC2 IAM role to be assumed by all the nodes in your cluster. If you leave these values at default, the default IAM roles (EMR_DefaultRole and EMR_EC2_DefaultRole) will be automatically created during cluster creation. Leave it as is. You can create a new Security Configuration in your EMR Web Console and use it in the \"Security Configuration\" section. This is where you define encryption, authentication and authorization for your cluster. We will look into this in detail on Day 3. For now, you can leave it at default. You can provide custom EC2 security groups for your leader and worker node types. You can configure up to 5 security groups per node type. If you do not specify, default EC2 security groups will be automatically created for leader and worker node types. Click on \"Create Cluster\". After about 10 mins, you can observe that the EMR cluster is created and is in \"WAITING\" state. Check all the tabs to see the cluster configurations.","title":"Step 4: Security"},{"location":"day1/mwaa/exercise/","text":"Orchestrating Notebook Pipelines using Amazon EMR Studio and Amazon MWAA \u00b6 In the previous Amazon EMR Studio exercise, you ran a parameterized notebook programmatically using start-notebook-execution API. In this exercise, we are going to orchestrate a pipeline with the same parameterized notebook find_best_sellers.ipynb using Amazon Managed Workflows for Apache Airflow Go to your EMR Studio Workspace and make sure that you have \"find_best_sellers.ipynb\" under workspace-repo/files/notebook. If you uploaded the file manually, go to the path where you uploaded the file and make sure that the first cell in that notebook is tagged as parameters (View -> Show Right Bar -> Advanced Tools). If this cell is not tagged as \"parameters\", re-do the parameterized notebooks section of Day 1 Exercises. Once you have confirmed that, login to the EMR leader node of the cluster \"EMR-Spark-Hive-Presto\" or EC2 JumpHost Session Manager session. Go to EC2 Web Console (Right click -> Open Link in New Tab) -> Click on JumpHost -> Connect -> Session Manager -> Connect. Run the following commands. sudo su ec2 - user cd ~ curl - o test_dag . py https : // raw . githubusercontent . com / vasveena / amazon - emr - ttt - workshop / main / files / dags / test_dag . py instanceProfileRole = $ ( aws iam list - instance - profiles - for - role -- role - name emrEc2InstanceProfileRole | jq . ' InstanceProfiles[].InstanceProfileName ' | sed \" s|\\ \" || g \" ) sed - i \" s|emrEc2InstanceProfileRole|$instanceProfileRole|g \" test_dag . py accountID = $ ( aws sts get - caller - identity -- query \" Account \" -- output text ) aws s3 cp test_dag . py s3 : // airflow - $acc ountID - dayone / dags / Go to the Managed Apache Airflow Web Console (Right click -> Open Link in New Tab). You will be able to see an Managed Airflow environment named \"mwaa\". Click on \"Open Airflow UI\". You will be taken to the Managed Airflow UI. Ignore the DAG Import Errors for now. Create a file called \"variables.json\" like below using a notepad or a vi editor. { \"REGION\": \"us-east-1\", \"SUBNET_ID\": \"subnet-id\", \"EMR_LOG_URI\": \"s3://mrworkshop-youraccountID-dayone/\", \"NOTEBOOK_ID\": \"e-XXXXXXXXXXXXXXXXXXX\", \"NOTEBOOK_FILE_NAME\": \"workshop-repo/files/notebook/find_best_sellers.ipynb\", \"CATEGORIES_CSV\": \"Apparel,Automotive,Baby,Beauty,Books\", \"FROM_DATE\": \"2015-08-25\", \"TO_DATE\": \"2015-08-31\", \"OUTPUT_LOCATION\": \"s3://mrworkshop-youraccountID-dayone/mwaa/\" } Replace youraccountID with your AWS Event Engine account ID. For the subnet ID, choose the subnet of the cluster \"EMR-Spark-Hive-Presto\" ( EMR Web Console (Right click -> Open Link in New Tab) -> EMR-Spark-Hive-Presto -> Summary tab -> Network and hardware section). For the values of NOTEBOOK_ID and NOTEBOOK_FILE_NAME, use the same values you used in the Parameterized Notebooks exercise. i.e., take these values from your workspace URL (or from the API command you ran if you have saved it somewhere). For example: https://e-4ac2fwhw1liin22ezilly60j8.emrnotebooks-prod.us-east-1.amazonaws.com/e-4AC2FWHW1LIIN22EZILLY60J8/lab/tree/workshop-repo/files/notebook/find_best_sellers.ipynb Alternate option: Instead of manually replacing all the values in the API, run the following commands in EC2 JumpHost Session Manager session. sudo su ec2 - user cd ~ cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) leader_dns = $ ( aws emr describe - cluster -- region us - east - 1 -- cluster - id $cluster_id -- query 'Cluster.MasterPublicDnsName' -- output text ) subnet_id = $ ( aws ec2 describe - instances -- region us - east - 1 -- filters \"Name=dns-name,Values=$leader_dns\" -- query 'Reservations[*].Instances[*].SubnetId' -- output text ) studio_id = $ ( / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 list - studios -- region us - east - 1 -- query Studios [ * ] . { Studios : StudioId } -- output text ) studio_s3_location = $ ( / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 describe - studio -- studio - id $studio_id -- query 'Studio.DefaultS3Location' -- output text ) studio_notebook_id = $ ( aws s3 ls $studio_s3_location / e - | sed 's|.*PRE ||g' | sed 's|/||g' | sed 's| ||g' ) accountID = $ ( aws sts get - caller - identity -- query \"Account\" -- output text ) sudo tee . / variables . json >/ dev / null << EOF { \"REGION\" : \"us-east-1\" , \"SUBNET_ID\" : \"$subnet_id\" , \"EMR_LOG_URI\" : \"s3://mrworkshop-$accountID-dayone/\" , \"NOTEBOOK_ID\" : \"$studio_notebook_id\" , \"NOTEBOOK_FILE_NAME\" : \"workshop-repo/files/notebook/find_best_sellers.ipynb\" , \"CATEGORIES_CSV\" : \"Apparel,Automotive,Baby,Beauty,Books\" , \"FROM_DATE\" : \"2015-08-25\" , \"TO_DATE\" : \"2015-08-31\" , \"OUTPUT_LOCATION\" : \"s3://mrworkshop-$accountID-dayone/mwaa/\" } EOF cat . / variables . json You will get a JSON output with all values entered. Copy this JSON and create a file called \"variables.json\" in your local desktop. Paste the contents you copied. An example variables.json file with values entered: { \"REGION\": \"us-east-1\", \"SUBNET_ID\": \"subnet-096759acf495c85df\", \"EMR_LOG_URI\": \"s3://mrworkshop-880847545464-dayone/\", \"NOTEBOOK_ID\": \"e-4AC2FWHW1LIIN22EZILLY60J8\", \"NOTEBOOK_FILE_NAME\": \"workshop-repo/files/notebook/find_best_sellers.ipynb\", \"CATEGORIES_CSV\": \"Apparel,Automotive,Baby,Beauty,Books\", \"FROM_DATE\": \"2015-08-25\", \"TO_DATE\": \"2015-08-31\", \"OUTPUT_LOCATION\": \"s3://mrworkshop-880847545464-dayone/mwaa/\" } Now let's upload this file into the Airflow UI. Go to Airflow UI -> Admin -> Variables. Click on \"Choose File\". Choose the variables.json you created from your local desktop and click on \"Import Variables\" to import the file. Now go to the DAGs on the top left corner and you should be able to see your DAG \"test_dag\". Turn on the DAG by using the Toggle switch. This DAG will execute once at the start of each hour (based on cron schedule: 0 * * ). For now, let us execute the DAG manually. Click on \"Trigger DAG\". Trigger the DAG by clicking on \"Trigger\". Your DAG will start to execute. Click on the DAG runs -> DAG ID -> test_dag. You will see the graph view of this execution. Analyze the steps in this DAG. Also, analyze the DAG code by clicking on \"Code\". Now, if you go to the EMR Web Console (Right click -> Open Link in New Tab), you can see a new cluster called \"Test-Cluster\" being launched. The DAG steps will create a new cluster, submit the notebook API as pipeline and terminate the cluster once the job is finished. From start to finish, this DAG will take about 15-20 mins to complete. Please note that it is not necessary to have your EMR Studio Workspace attached to an EMR cluster to be able to run this notebook pipeline since your notebooks will be persisted in the S3 location of your EMR Studio. After 15 mins, check the DAG execution status. Now, let's check the S3 location you provided as parameter \"OUTPUT_LOCATION\" in your variables.json. This is where your job output is going to be stored. Run the below command on EC2 JumpHost Session Manager session. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) aws s3 ls s3://mrworkshop-$accountID-dayone/mwaa/ Alternatively, you can go to the S3 Web Console (Right click -> Open Link in New Tab) and check this location from the console as well. You should be able to see the output files for the 5 categories we passed as parameter in variables.json: \"Apparel,Automotive,Baby,Beauty,Books\" After the job is finished, the cluster \"Test-cluster\" will be automatically terminated. This DAG will be executed once every hour automatically. You can play around by changing the notebook parameters or pipeline schedule. Once you are done experimenting, you can stop the DAG by using the toggle switch to turn it OFF.","title":"2 - Orchestration Notebook Pipelines using Amazon MWAA"},{"location":"day1/mwaa/exercise/#orchestrating-notebook-pipelines-using-amazon-emr-studio-and-amazon-mwaa","text":"In the previous Amazon EMR Studio exercise, you ran a parameterized notebook programmatically using start-notebook-execution API. In this exercise, we are going to orchestrate a pipeline with the same parameterized notebook find_best_sellers.ipynb using Amazon Managed Workflows for Apache Airflow Go to your EMR Studio Workspace and make sure that you have \"find_best_sellers.ipynb\" under workspace-repo/files/notebook. If you uploaded the file manually, go to the path where you uploaded the file and make sure that the first cell in that notebook is tagged as parameters (View -> Show Right Bar -> Advanced Tools). If this cell is not tagged as \"parameters\", re-do the parameterized notebooks section of Day 1 Exercises. Once you have confirmed that, login to the EMR leader node of the cluster \"EMR-Spark-Hive-Presto\" or EC2 JumpHost Session Manager session. Go to EC2 Web Console (Right click -> Open Link in New Tab) -> Click on JumpHost -> Connect -> Session Manager -> Connect. Run the following commands. sudo su ec2 - user cd ~ curl - o test_dag . py https : // raw . githubusercontent . com / vasveena / amazon - emr - ttt - workshop / main / files / dags / test_dag . py instanceProfileRole = $ ( aws iam list - instance - profiles - for - role -- role - name emrEc2InstanceProfileRole | jq . ' InstanceProfiles[].InstanceProfileName ' | sed \" s|\\ \" || g \" ) sed - i \" s|emrEc2InstanceProfileRole|$instanceProfileRole|g \" test_dag . py accountID = $ ( aws sts get - caller - identity -- query \" Account \" -- output text ) aws s3 cp test_dag . py s3 : // airflow - $acc ountID - dayone / dags / Go to the Managed Apache Airflow Web Console (Right click -> Open Link in New Tab). You will be able to see an Managed Airflow environment named \"mwaa\". Click on \"Open Airflow UI\". You will be taken to the Managed Airflow UI. Ignore the DAG Import Errors for now. Create a file called \"variables.json\" like below using a notepad or a vi editor. { \"REGION\": \"us-east-1\", \"SUBNET_ID\": \"subnet-id\", \"EMR_LOG_URI\": \"s3://mrworkshop-youraccountID-dayone/\", \"NOTEBOOK_ID\": \"e-XXXXXXXXXXXXXXXXXXX\", \"NOTEBOOK_FILE_NAME\": \"workshop-repo/files/notebook/find_best_sellers.ipynb\", \"CATEGORIES_CSV\": \"Apparel,Automotive,Baby,Beauty,Books\", \"FROM_DATE\": \"2015-08-25\", \"TO_DATE\": \"2015-08-31\", \"OUTPUT_LOCATION\": \"s3://mrworkshop-youraccountID-dayone/mwaa/\" } Replace youraccountID with your AWS Event Engine account ID. For the subnet ID, choose the subnet of the cluster \"EMR-Spark-Hive-Presto\" ( EMR Web Console (Right click -> Open Link in New Tab) -> EMR-Spark-Hive-Presto -> Summary tab -> Network and hardware section). For the values of NOTEBOOK_ID and NOTEBOOK_FILE_NAME, use the same values you used in the Parameterized Notebooks exercise. i.e., take these values from your workspace URL (or from the API command you ran if you have saved it somewhere). For example: https://e-4ac2fwhw1liin22ezilly60j8.emrnotebooks-prod.us-east-1.amazonaws.com/e-4AC2FWHW1LIIN22EZILLY60J8/lab/tree/workshop-repo/files/notebook/find_best_sellers.ipynb Alternate option: Instead of manually replacing all the values in the API, run the following commands in EC2 JumpHost Session Manager session. sudo su ec2 - user cd ~ cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) leader_dns = $ ( aws emr describe - cluster -- region us - east - 1 -- cluster - id $cluster_id -- query 'Cluster.MasterPublicDnsName' -- output text ) subnet_id = $ ( aws ec2 describe - instances -- region us - east - 1 -- filters \"Name=dns-name,Values=$leader_dns\" -- query 'Reservations[*].Instances[*].SubnetId' -- output text ) studio_id = $ ( / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 list - studios -- region us - east - 1 -- query Studios [ * ] . { Studios : StudioId } -- output text ) studio_s3_location = $ ( / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 describe - studio -- studio - id $studio_id -- query 'Studio.DefaultS3Location' -- output text ) studio_notebook_id = $ ( aws s3 ls $studio_s3_location / e - | sed 's|.*PRE ||g' | sed 's|/||g' | sed 's| ||g' ) accountID = $ ( aws sts get - caller - identity -- query \"Account\" -- output text ) sudo tee . / variables . json >/ dev / null << EOF { \"REGION\" : \"us-east-1\" , \"SUBNET_ID\" : \"$subnet_id\" , \"EMR_LOG_URI\" : \"s3://mrworkshop-$accountID-dayone/\" , \"NOTEBOOK_ID\" : \"$studio_notebook_id\" , \"NOTEBOOK_FILE_NAME\" : \"workshop-repo/files/notebook/find_best_sellers.ipynb\" , \"CATEGORIES_CSV\" : \"Apparel,Automotive,Baby,Beauty,Books\" , \"FROM_DATE\" : \"2015-08-25\" , \"TO_DATE\" : \"2015-08-31\" , \"OUTPUT_LOCATION\" : \"s3://mrworkshop-$accountID-dayone/mwaa/\" } EOF cat . / variables . json You will get a JSON output with all values entered. Copy this JSON and create a file called \"variables.json\" in your local desktop. Paste the contents you copied. An example variables.json file with values entered: { \"REGION\": \"us-east-1\", \"SUBNET_ID\": \"subnet-096759acf495c85df\", \"EMR_LOG_URI\": \"s3://mrworkshop-880847545464-dayone/\", \"NOTEBOOK_ID\": \"e-4AC2FWHW1LIIN22EZILLY60J8\", \"NOTEBOOK_FILE_NAME\": \"workshop-repo/files/notebook/find_best_sellers.ipynb\", \"CATEGORIES_CSV\": \"Apparel,Automotive,Baby,Beauty,Books\", \"FROM_DATE\": \"2015-08-25\", \"TO_DATE\": \"2015-08-31\", \"OUTPUT_LOCATION\": \"s3://mrworkshop-880847545464-dayone/mwaa/\" } Now let's upload this file into the Airflow UI. Go to Airflow UI -> Admin -> Variables. Click on \"Choose File\". Choose the variables.json you created from your local desktop and click on \"Import Variables\" to import the file. Now go to the DAGs on the top left corner and you should be able to see your DAG \"test_dag\". Turn on the DAG by using the Toggle switch. This DAG will execute once at the start of each hour (based on cron schedule: 0 * * ). For now, let us execute the DAG manually. Click on \"Trigger DAG\". Trigger the DAG by clicking on \"Trigger\". Your DAG will start to execute. Click on the DAG runs -> DAG ID -> test_dag. You will see the graph view of this execution. Analyze the steps in this DAG. Also, analyze the DAG code by clicking on \"Code\". Now, if you go to the EMR Web Console (Right click -> Open Link in New Tab), you can see a new cluster called \"Test-Cluster\" being launched. The DAG steps will create a new cluster, submit the notebook API as pipeline and terminate the cluster once the job is finished. From start to finish, this DAG will take about 15-20 mins to complete. Please note that it is not necessary to have your EMR Studio Workspace attached to an EMR cluster to be able to run this notebook pipeline since your notebooks will be persisted in the S3 location of your EMR Studio. After 15 mins, check the DAG execution status. Now, let's check the S3 location you provided as parameter \"OUTPUT_LOCATION\" in your variables.json. This is where your job output is going to be stored. Run the below command on EC2 JumpHost Session Manager session. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) aws s3 ls s3://mrworkshop-$accountID-dayone/mwaa/ Alternatively, you can go to the S3 Web Console (Right click -> Open Link in New Tab) and check this location from the console as well. You should be able to see the output files for the 5 categories we passed as parameter in variables.json: \"Apparel,Automotive,Baby,Beauty,Books\" After the job is finished, the cluster \"Test-cluster\" will be automatically terminated. This DAG will be executed once every hour automatically. You can play around by changing the notebook parameters or pipeline schedule. Once you are done experimenting, you can stop the DAG by using the toggle switch to turn it OFF.","title":"Orchestrating Notebook Pipelines using Amazon EMR Studio and Amazon MWAA"},{"location":"day1/spark/exercise/","text":"Apache Spark on Amazon EMR \u00b6 Initial Setup \u00b6 SSH to the leader node of the EMR cluster \"EMR-Spark-Hive-Presto\". The key pair you downloaded in the Setup can be used to SSH via terminal or using an SSH client like Putty. To make it easy, ssm-agent has been installed on all EMR nodes via a bootstrap action so that you can use AWS Systems Manager to login to your client EC2 instance and EMR leader node. Go to the EMR Web Console (Right click -> Open Link in New Tab) -> \"EMR-Spark-Hive-Presto\" -> Hardware tab. You will see MASTER and CORE fleets. Click on the MASTER instance fleet id (looks like if-XXXXXXXXXX). Click on the EC2 instance ID (looks like i-xxxxxxxxxxx). It should take you to the EC2 management console. You will be navigated to the EC2 management console. Click on \u201cConnect\u201d and go to the tab \u201cSession Manager\u201d. Click on Connect. You will be navigated to the AWS Session Manager session. In the session, type the following commands to log in as hadoop user (default OS user for EMR). sudo su hadoop cd ~ You will use the same login method to log in to other EC2 instances in this workshop as well. Using spark-submit and spark-shell \u00b6 Once you are logged into the EMR cluster using SSH or SSM agent, type \u201cspark-shell\u201d on your EMR Leader Node. Once spark-shell opens and the Spark session object is created, run the commands below - import org.apache.spark.sql.types. { IntegerType , StringType , StructType , StructField , DoubleType } val schema = StructType ( Array ( StructField ( \"s_suppkey\" , IntegerType , true ), StructField ( \"s_name\" , StringType , true ), StructField ( \"s_address\" , StringType , true ), StructField ( \"s_nationkey\" , StringType , true ), StructField ( \"s_phone\" , StringType , true ), StructField ( \"s_acctbal\" , DoubleType , true ), StructField ( \"s_comment\" , StringType , true ))) val df = spark . read . option ( \"delimiter\" , \"|\" ) . schema ( schema ) . csv ( \"s3://redshift-downloads/TPC-H/3TB/supplier/\" ) df . show ( 5 ) val df2 = df . filter ( $ \"s_acctbal\" > lit ( 0.0 )) . withColumn ( \"randdom\" , lit ( \"insert random column\" )) df2 . show ( 5 ) You will see the results in the spark-shell session. Investigate Spark UI \u00b6 While the Spark session is still active, you can check the Spark UI. You will need to install AWS CLI and Session Manager plugin on your local desktop to do this. You will also need to update your PATH variable if it is not done automatically following this document. Otherwise you may get the error \"SessionManagerPlugin is not found\". Replace --target with your leader node instance ID in the following command. Replace the environmental variables with the values from the Team Dashboard. For Windows, you will need to use \"set\" instead of \"export\". export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > You can run the below commands on the terminal of your local desktop to get the leader node instance ID. cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) leader_dns = $ ( aws emr describe - cluster -- region us - east - 1 -- cluster - id $cluster_id -- query 'Cluster.MasterPublicDnsName' -- output text ) instance_id = $ ( aws ec2 describe - instances -- region us - east - 1 -- filters \"Name=dns-name,Values=$leader_dns\" -- query 'Reservations[*].Instances[*].InstanceId' -- output text ) echo $instance_id Now run the command to start the SSM session. The value of --target is replaced with the instance ID obtained using above commands. Note: If you are not able to run the above commands to obtain the instance ID, replace $instance_id in the below command with the leader node instance ID obtained from the EMR Web Console (Right click -> Open Link in New Tab) -> \"EMR-Spark-Hive-Presto\" -> Hardware tab -> click on Instance Fleet ID of the MASTER Node Type -> Copy the instance ID). aws ssm start - session -- target $instance_id -- document - name AWS - StartPortForwardingSession -- parameters '{ \"portNumber\" : [ \"18080\" ], \"localPortNumber\" : [ \"8158\" ]}' -- region us - east - 1 Following image shows the commands run in macOS terminal. 18080 is the Spark History Server Port and 8157 is the local port. Now open http://localhost:8158 in your browser. Click on \"Show incomplete applications\" -> App ID (for eg: application_1647720368860_0002). Check out all the tabs especially the SQL tab. Click on \"show\" (Spark action) in the SQL tab to see the query plan. Alternative approach - Local SSH tunneling \u00b6 Please note that with this approach, you cannot access YARN Resource Manager UI. You can access it via local port forwarding by running the following command in your local desktop's terminal or using Putty for Windows. You can obtain the leader node public DNS by running the below commands on the EMR leader node Session Manager session. cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) leader_dns = $ ( aws emr describe - cluster -- region us - east - 1 -- cluster - id $cluster_id -- query 'Cluster.MasterPublicDnsName' -- output text ) echo $leader_dns You can also obtain the value from EMR Web Console (Right click -> Open Link in New Tab) -> EMR-Spark-Hive-Presto -> Summary tab -> Master public DNS. Once you obtain leader node DNS using one of the two above methods, replace leaderNodePublicDNS in the below command with your leader node public DNS. ssh - i ~/ ee - default - keypair . pem - N - L 8157 : leaderNodePublicDNS : 8088 hadoop @leaderNodePublicDNS Now enter http://localhost:8157 on your browser to see the Resource Manager UI. You can use this method as well to access Spark UI (replace port 8088 in the above command with port 18080). Optimization exercise (Optional) \u00b6 Converting inefficient join types Run the below code block in spark-shell. Set driver memory to 4G while starting the shell. spark-shell --driver-memory=4G import org.apache.spark.sql.types._ val liSchema = StructType ( Array ( StructField ( \"l_orderkey\" , StringType , true ), StructField ( \"l_partkey\" , IntegerType , true ), StructField ( \"l_suppkey\" , IntegerType , true ), StructField ( \"l_linenumber\" , IntegerType , true ), StructField ( \"l_quantity\" , IntegerType , true ), StructField ( \"l_extendedprice\" , IntegerType , true ), StructField ( \"l_discount\" , DoubleType , true ), StructField ( \"l_tax\" , DoubleType , true ), StructField ( \"l_returnflag\" , StringType , true ), StructField ( \"l_linestatus\" , StringType , true ), StructField ( \"l_shipdate\" , StringType , true ), StructField ( \"l_commitdate\" , StringType , true ), StructField ( \"l_receiptdate\" , StringType , true ), StructField ( \"l_shipinstruct\" , StringType , true ), StructField ( \"l_shipmode\" , StringType , true ), StructField ( \"l_comment\" , StringType , true ) ) ) val orSchema = StructType ( Array ( StructField ( \"o_orderkey\" , StringType , true ), StructField ( \"o_custkey\" , IntegerType , true ), StructField ( \"o_orderstatus\" , StringType , true ), StructField ( \"o_totalprice\" , DoubleType , true ), StructField ( \"o_orderdate\" , StringType , true ), StructField ( \"o_orderpriority\" , StringType , true ), StructField ( \"o_clerk\" , StringType , true ), StructField ( \"o_shippriority\" , IntegerType , true ), StructField ( \"o_comment\" , StringType , true ) ) ) val df1 = spark . read . schema ( liSchema ) . option ( \"delimiter\" , \"|\" ) . csv ( \"s3://redshift-downloads/TPC-H/2.18/3TB/lineitem/\" ) val df2 = spark . read . schema ( orSchema ) . option ( \"delimiter\" , \"|\" ) . csv ( \"s3://redshift-downloads/TPC-H/2.18/3TB/orders/\" ) val lineitem = df1 . limit ( 1000 ) val orders = df2 . limit ( 1000 ) val nestedLoopDF = lineitem . join ( orders , lineitem ( \"l_orderkey\" ) === orders ( \"o_orderkey\" ) || lineitem ( \"l_receiptdate\" ) === orders ( \"o_orderdate\" )) nestedLoopDF . show ( 5 , truncate = false ) Check the Spark UI -> SQL tab -> show (Spark action). You will be able to see that the above code uses BroadcastNestedLoopJoin as the join type. BroadcastNestedLoopJoin is an inefficient join that results from bad coding practice. Convert it into SortMergeJoin or BroadcastJoin by changing the code. val result1 = lineitem . join ( orders , lineitem ( \" l_orderkey \" ) === orders ( \" o_orderkey \" )) val result2 = lineitem . join ( orders , lineitem ( \" l_receiptdate \" ) === orders ( \" o_orderdate \" )) val broadcastDF = result1 . union ( result2 ) broadcastDF . show ( 5 , truncate = false ) Check the Spark UI now. You will be able to see that BroadcastHashJoin is being used instead which is the best join type if at least one of the two tables you are going to join is relatively small (<50MB). Default spark.sql.autoBroadcastJoinThreshold is 10 MB. Submit Spark Work to EMR using AddSteps API \u00b6 Let us submit Spark work to the cluster using EMR\u2019s AddSteps API. Copy the EMR Cluster ID in the Summary Tab of your EMR cluster \"EMR-Spark-Hive-Presto\" from EMR Web Console (Right click -> Open Link in New Tab). It looks like \u2018j-XXXXXXXXXX\u2019. You can submit steps to your EMR from your local desktop after exporting the AWS credentials in your Team Dashboard page. For windows, use set instead of export. export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > Run the below command. Replace cluster-id value with your cluster ID. aws emr add - steps -- cluster - id j - XXXXXXXXXX -- steps Name = \"Spark Pi Job\" , Jar = command - runner . jar , Args = [ spark - submit , -- master , yarn , -- num - executors , 2 , -- class , org . apache . spark . examples . SparkPi , / usr / lib / spark / examples / jars / spark - examples . jar , 10 , - v ] -- region us - east - 1 If you do not want to use AWS CLI in your local desktop, you can submit the below command directly on your Session Manager session created for EMR leader node. You will also find an EC2 instance called \"JumpHost\" in the EC2 Web Console (Right click -> Open Link in New Tab). You can connect to that instance using Session Manager and submit step to EMR cluster from that session. You may find another jump instance starting with the name \"emr-on-eks\". Do not login to that. Once connected, enter following commands to login as ec2-user (default OS user for EC2 instances). sudo su ec2-user cd ~ Now, run the commands below. You do not need to change anything and you do not need to export any credentials since the IAM role attached to this JumpHost has all accesses required. cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) aws emr add - steps -- cluster - id $ { cluster_id } -- steps Name = \"Spark Pi Job\" , Jar = command - runner . jar , Args = [ spark - submit , -- master , yarn , -- num - executors , 2 , -- class , org . apache . spark . examples . SparkPi , / usr / lib / spark / examples / jars / spark - examples . jar , 10 , - v ] -- region us - east - 1 Now, check the EMR step that was submitted to the cluster. You can look into the stdout logs to see the output. Please note in real life scenario, you can submit EMR Steps from anywhere as long as your IAM user or role has IAM access to invoke EMR AddSteps API.","title":"2 - Apache Spark on Amazon EMR"},{"location":"day1/spark/exercise/#apache-spark-on-amazon-emr","text":"","title":"Apache Spark on Amazon EMR"},{"location":"day1/spark/exercise/#initial-setup","text":"SSH to the leader node of the EMR cluster \"EMR-Spark-Hive-Presto\". The key pair you downloaded in the Setup can be used to SSH via terminal or using an SSH client like Putty. To make it easy, ssm-agent has been installed on all EMR nodes via a bootstrap action so that you can use AWS Systems Manager to login to your client EC2 instance and EMR leader node. Go to the EMR Web Console (Right click -> Open Link in New Tab) -> \"EMR-Spark-Hive-Presto\" -> Hardware tab. You will see MASTER and CORE fleets. Click on the MASTER instance fleet id (looks like if-XXXXXXXXXX). Click on the EC2 instance ID (looks like i-xxxxxxxxxxx). It should take you to the EC2 management console. You will be navigated to the EC2 management console. Click on \u201cConnect\u201d and go to the tab \u201cSession Manager\u201d. Click on Connect. You will be navigated to the AWS Session Manager session. In the session, type the following commands to log in as hadoop user (default OS user for EMR). sudo su hadoop cd ~ You will use the same login method to log in to other EC2 instances in this workshop as well.","title":"Initial Setup"},{"location":"day1/spark/exercise/#using-spark-submit-and-spark-shell","text":"Once you are logged into the EMR cluster using SSH or SSM agent, type \u201cspark-shell\u201d on your EMR Leader Node. Once spark-shell opens and the Spark session object is created, run the commands below - import org.apache.spark.sql.types. { IntegerType , StringType , StructType , StructField , DoubleType } val schema = StructType ( Array ( StructField ( \"s_suppkey\" , IntegerType , true ), StructField ( \"s_name\" , StringType , true ), StructField ( \"s_address\" , StringType , true ), StructField ( \"s_nationkey\" , StringType , true ), StructField ( \"s_phone\" , StringType , true ), StructField ( \"s_acctbal\" , DoubleType , true ), StructField ( \"s_comment\" , StringType , true ))) val df = spark . read . option ( \"delimiter\" , \"|\" ) . schema ( schema ) . csv ( \"s3://redshift-downloads/TPC-H/3TB/supplier/\" ) df . show ( 5 ) val df2 = df . filter ( $ \"s_acctbal\" > lit ( 0.0 )) . withColumn ( \"randdom\" , lit ( \"insert random column\" )) df2 . show ( 5 ) You will see the results in the spark-shell session.","title":"Using spark-submit and spark-shell"},{"location":"day1/spark/exercise/#investigate-spark-ui","text":"While the Spark session is still active, you can check the Spark UI. You will need to install AWS CLI and Session Manager plugin on your local desktop to do this. You will also need to update your PATH variable if it is not done automatically following this document. Otherwise you may get the error \"SessionManagerPlugin is not found\". Replace --target with your leader node instance ID in the following command. Replace the environmental variables with the values from the Team Dashboard. For Windows, you will need to use \"set\" instead of \"export\". export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > You can run the below commands on the terminal of your local desktop to get the leader node instance ID. cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) leader_dns = $ ( aws emr describe - cluster -- region us - east - 1 -- cluster - id $cluster_id -- query 'Cluster.MasterPublicDnsName' -- output text ) instance_id = $ ( aws ec2 describe - instances -- region us - east - 1 -- filters \"Name=dns-name,Values=$leader_dns\" -- query 'Reservations[*].Instances[*].InstanceId' -- output text ) echo $instance_id Now run the command to start the SSM session. The value of --target is replaced with the instance ID obtained using above commands. Note: If you are not able to run the above commands to obtain the instance ID, replace $instance_id in the below command with the leader node instance ID obtained from the EMR Web Console (Right click -> Open Link in New Tab) -> \"EMR-Spark-Hive-Presto\" -> Hardware tab -> click on Instance Fleet ID of the MASTER Node Type -> Copy the instance ID). aws ssm start - session -- target $instance_id -- document - name AWS - StartPortForwardingSession -- parameters '{ \"portNumber\" : [ \"18080\" ], \"localPortNumber\" : [ \"8158\" ]}' -- region us - east - 1 Following image shows the commands run in macOS terminal. 18080 is the Spark History Server Port and 8157 is the local port. Now open http://localhost:8158 in your browser. Click on \"Show incomplete applications\" -> App ID (for eg: application_1647720368860_0002). Check out all the tabs especially the SQL tab. Click on \"show\" (Spark action) in the SQL tab to see the query plan.","title":"Investigate Spark UI"},{"location":"day1/spark/exercise/#alternative-approach-local-ssh-tunneling","text":"Please note that with this approach, you cannot access YARN Resource Manager UI. You can access it via local port forwarding by running the following command in your local desktop's terminal or using Putty for Windows. You can obtain the leader node public DNS by running the below commands on the EMR leader node Session Manager session. cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) leader_dns = $ ( aws emr describe - cluster -- region us - east - 1 -- cluster - id $cluster_id -- query 'Cluster.MasterPublicDnsName' -- output text ) echo $leader_dns You can also obtain the value from EMR Web Console (Right click -> Open Link in New Tab) -> EMR-Spark-Hive-Presto -> Summary tab -> Master public DNS. Once you obtain leader node DNS using one of the two above methods, replace leaderNodePublicDNS in the below command with your leader node public DNS. ssh - i ~/ ee - default - keypair . pem - N - L 8157 : leaderNodePublicDNS : 8088 hadoop @leaderNodePublicDNS Now enter http://localhost:8157 on your browser to see the Resource Manager UI. You can use this method as well to access Spark UI (replace port 8088 in the above command with port 18080).","title":"Alternative approach - Local SSH tunneling"},{"location":"day1/spark/exercise/#optimization-exercise-optional","text":"Converting inefficient join types Run the below code block in spark-shell. Set driver memory to 4G while starting the shell. spark-shell --driver-memory=4G import org.apache.spark.sql.types._ val liSchema = StructType ( Array ( StructField ( \"l_orderkey\" , StringType , true ), StructField ( \"l_partkey\" , IntegerType , true ), StructField ( \"l_suppkey\" , IntegerType , true ), StructField ( \"l_linenumber\" , IntegerType , true ), StructField ( \"l_quantity\" , IntegerType , true ), StructField ( \"l_extendedprice\" , IntegerType , true ), StructField ( \"l_discount\" , DoubleType , true ), StructField ( \"l_tax\" , DoubleType , true ), StructField ( \"l_returnflag\" , StringType , true ), StructField ( \"l_linestatus\" , StringType , true ), StructField ( \"l_shipdate\" , StringType , true ), StructField ( \"l_commitdate\" , StringType , true ), StructField ( \"l_receiptdate\" , StringType , true ), StructField ( \"l_shipinstruct\" , StringType , true ), StructField ( \"l_shipmode\" , StringType , true ), StructField ( \"l_comment\" , StringType , true ) ) ) val orSchema = StructType ( Array ( StructField ( \"o_orderkey\" , StringType , true ), StructField ( \"o_custkey\" , IntegerType , true ), StructField ( \"o_orderstatus\" , StringType , true ), StructField ( \"o_totalprice\" , DoubleType , true ), StructField ( \"o_orderdate\" , StringType , true ), StructField ( \"o_orderpriority\" , StringType , true ), StructField ( \"o_clerk\" , StringType , true ), StructField ( \"o_shippriority\" , IntegerType , true ), StructField ( \"o_comment\" , StringType , true ) ) ) val df1 = spark . read . schema ( liSchema ) . option ( \"delimiter\" , \"|\" ) . csv ( \"s3://redshift-downloads/TPC-H/2.18/3TB/lineitem/\" ) val df2 = spark . read . schema ( orSchema ) . option ( \"delimiter\" , \"|\" ) . csv ( \"s3://redshift-downloads/TPC-H/2.18/3TB/orders/\" ) val lineitem = df1 . limit ( 1000 ) val orders = df2 . limit ( 1000 ) val nestedLoopDF = lineitem . join ( orders , lineitem ( \"l_orderkey\" ) === orders ( \"o_orderkey\" ) || lineitem ( \"l_receiptdate\" ) === orders ( \"o_orderdate\" )) nestedLoopDF . show ( 5 , truncate = false ) Check the Spark UI -> SQL tab -> show (Spark action). You will be able to see that the above code uses BroadcastNestedLoopJoin as the join type. BroadcastNestedLoopJoin is an inefficient join that results from bad coding practice. Convert it into SortMergeJoin or BroadcastJoin by changing the code. val result1 = lineitem . join ( orders , lineitem ( \" l_orderkey \" ) === orders ( \" o_orderkey \" )) val result2 = lineitem . join ( orders , lineitem ( \" l_receiptdate \" ) === orders ( \" o_orderdate \" )) val broadcastDF = result1 . union ( result2 ) broadcastDF . show ( 5 , truncate = false ) Check the Spark UI now. You will be able to see that BroadcastHashJoin is being used instead which is the best join type if at least one of the two tables you are going to join is relatively small (<50MB). Default spark.sql.autoBroadcastJoinThreshold is 10 MB.","title":"Optimization exercise (Optional)"},{"location":"day1/spark/exercise/#submit-spark-work-to-emr-using-addsteps-api","text":"Let us submit Spark work to the cluster using EMR\u2019s AddSteps API. Copy the EMR Cluster ID in the Summary Tab of your EMR cluster \"EMR-Spark-Hive-Presto\" from EMR Web Console (Right click -> Open Link in New Tab). It looks like \u2018j-XXXXXXXXXX\u2019. You can submit steps to your EMR from your local desktop after exporting the AWS credentials in your Team Dashboard page. For windows, use set instead of export. export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > Run the below command. Replace cluster-id value with your cluster ID. aws emr add - steps -- cluster - id j - XXXXXXXXXX -- steps Name = \"Spark Pi Job\" , Jar = command - runner . jar , Args = [ spark - submit , -- master , yarn , -- num - executors , 2 , -- class , org . apache . spark . examples . SparkPi , / usr / lib / spark / examples / jars / spark - examples . jar , 10 , - v ] -- region us - east - 1 If you do not want to use AWS CLI in your local desktop, you can submit the below command directly on your Session Manager session created for EMR leader node. You will also find an EC2 instance called \"JumpHost\" in the EC2 Web Console (Right click -> Open Link in New Tab). You can connect to that instance using Session Manager and submit step to EMR cluster from that session. You may find another jump instance starting with the name \"emr-on-eks\". Do not login to that. Once connected, enter following commands to login as ec2-user (default OS user for EC2 instances). sudo su ec2-user cd ~ Now, run the commands below. You do not need to change anything and you do not need to export any credentials since the IAM role attached to this JumpHost has all accesses required. cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) aws emr add - steps -- cluster - id $ { cluster_id } -- steps Name = \"Spark Pi Job\" , Jar = command - runner . jar , Args = [ spark - submit , -- master , yarn , -- num - executors , 2 , -- class , org . apache . spark . examples . SparkPi , / usr / lib / spark / examples / jars / spark - examples . jar , 10 , - v ] -- region us - east - 1 Now, check the EMR step that was submitted to the cluster. You can look into the stdout logs to see the output. Please note in real life scenario, you can submit EMR Steps from anywhere as long as your IAM user or role has IAM access to invoke EMR AddSteps API.","title":"Submit Spark Work to EMR using AddSteps API"},{"location":"day1/step/exercise/","text":"Orchestrating EMR Steps using AWS Step Functions \u00b6 In the previous section, we saw how you can leverage EMR AddSteps API to submit a Spark work to the cluster. You can submit any type of work to the EMR cluster (Hive, Presto, Bash etc.). In this section, we will see how you can orchestrate EMR jobs using EMR Step API and AWS Steps Functions. For a variation, let's orchestrate Hive EMR Steps. But you can choose any EMR step of your choice. Create State Machine \u00b6 Go to AWS Step Functions console (Right click -> Open Link in New Tab). Click on \"Create State Machine\". In Step 1 (Define state machine), choose \"Write your workflow in code\". Let Type be \"Standard\". Copy the entire blurb below and paste into a notepad or any text editor in your local desktop. { \" StartAt \" : \" Should_Create_Cluster \" , \" States \" : { \" Should_Create_Cluster \" : { \" Type \" : \" Choice \" , \" Choices \" : [ { \" Variable \" : \" $.CreateCluster \" , \" BooleanEquals \" : true , \" Next \" : \" Create_A_Cluster \" }, { \" Variable \" : \" $.CreateCluster \" , \" BooleanEquals \" : false , \" Next \" : \" Enable_Termination_Protection \" } ], \" Default \" : \" Create_A_Cluster \" }, \" Create_A_Cluster \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:createCluster.sync \" , \" Parameters \" : { \" Name \" : \" WorkflowCluster \" , \" VisibleToAllUsers \" : true , \" ReleaseLabel \" : \" emr-5.28.0 \" , \" Applications \" : [{ \" Name \" : \" Hive \" }], \" ServiceRole \" : \" emrServiceRole \" , \" JobFlowRole \" : \" emrEc2InstanceProfile \" , \" Instances \" : { \" KeepJobFlowAliveWhenNoSteps \" : true , \" InstanceFleets \" : [ { \" InstanceFleetType \" : \" MASTER \" , \" TargetOnDemandCapacity \" : 1 , \" InstanceTypeConfigs \" : [ { \" InstanceType \" : \" m4.xlarge \" } ] }, { \" InstanceFleetType \" : \" CORE \" , \" TargetOnDemandCapacity \" : 1 , \" InstanceTypeConfigs \" : [ { \" InstanceType \" : \" m4.xlarge \" } ] } ] } }, \" ResultPath \" : \" $.CreateClusterResult \" , \" Next \" : \" Merge_Results \" }, \" Merge_Results \" : { \" Type \" : \" Pass \" , \" Parameters \" : { \" CreateCluster.$ \" : \" $.CreateCluster \" , \" TerminateCluster.$ \" : \" $.TerminateCluster \" , \" ClusterId.$ \" : \" $.CreateClusterResult.ClusterId \" }, \" Next \" : \" Enable_Termination_Protection \" }, \" Enable_Termination_Protection \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:setClusterTerminationProtection \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" TerminationProtected \" : true }, \" ResultPath \" : null , \" Next \" : \" Add_Steps_Parallel \" }, \" Add_Steps_Parallel \" : { \" Type \" : \" Parallel \" , \" Branches \" : [ { \" StartAt \" : \" Step_One \" , \" States \" : { \" Step_One \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:addStep.sync \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" Step \" : { \" Name \" : \" The first step \" , \" ActionOnFailure \" : \" CONTINUE \" , \" HadoopJarStep \" : { \" Jar \" : \" command-runner.jar \" , \" Args \" : [ \" hive-script \" , \" --run-hive-script \" , \" --args \" , \" -f \" , \" s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q \" , \" -d \" , \" INPUT=s3://eu-west-1.elasticmapreduce.samples \" , \" -d \" , \" OUTPUT=s3://mrworkshop-youraccountID-dayone/StepFunc/MyHiveQueryResults/ \" ] } } }, \" End \" : true } } }, { \" StartAt \" : \" Wait_10_Seconds \" , \" States \" : { \" Wait_10_Seconds \" : { \" Type \" : \" Wait \" , \" Seconds \" : 10 , \" Next \" : \" Step_Two (async) \" }, \" Step_Two (async) \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:addStep \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" Step \" : { \" Name \" : \" The second step \" , \" ActionOnFailure \" : \" CONTINUE \" , \" HadoopJarStep \" : { \" Jar \" : \" command-runner.jar \" , \" Args \" : [ \" hive-script \" , \" --run-hive-script \" , \" --args \" , \" -f \" , \" s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q \" , \" -d \" , \" INPUT=s3://eu-west-1.elasticmapreduce.samples \" , \" -d \" , \" OUTPUT=s3://mrworkshop-youraccountID-dayone/StepFunc/MyHiveQueryResults/ \" ] } } }, \" ResultPath \" : \" $.AddStepsResult \" , \" Next \" : \" Wait_Another_10_Seconds \" }, \" Wait_Another_10_Seconds \" : { \" Type \" : \" Wait \" , \" Seconds \" : 10 , \" Next \" : \" Cancel_Step_Two \" }, \" Cancel_Step_Two \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:cancelStep \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" StepId.$ \" : \" $.AddStepsResult.StepId \" }, \" End \" : true } } } ], \" ResultPath \" : null , \" Next \" : \" Step_Three \" }, \" Step_Three \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:addStep.sync \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" Step \" : { \" Name \" : \" The third step \" , \" ActionOnFailure \" : \" CONTINUE \" , \" HadoopJarStep \" : { \" Jar \" : \" command-runner.jar \" , \" Args \" : [ \" hive-script \" , \" --run-hive-script \" , \" --args \" , \" -f \" , \" s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q \" , \" -d \" , \" INPUT=s3://eu-west-1.elasticmapreduce.samples \" , \" -d \" , \" OUTPUT=s3://mrworkshop-youraccountID-dayone/StepFunc/MyHiveQueryResults/ \" ] } } }, \" ResultPath \" : null , \" Next \" : \" Disable_Termination_Protection \" }, \" Disable_Termination_Protection \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:setClusterTerminationProtection \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" TerminationProtected \" : false }, \" ResultPath \" : null , \" Next \" : \" Should_Terminate_Cluster \" }, \" Should_Terminate_Cluster \" : { \" Type \" : \" Choice \" , \" Choices \" : [ { \" Variable \" : \" $.TerminateCluster \" , \" BooleanEquals \" : true , \" Next \" : \" Terminate_Cluster \" }, { \" Variable \" : \" $.TerminateCluster \" , \" BooleanEquals \" : false , \" Next \" : \" Wrapping_Up \" } ], \" Default \" : \" Wrapping_Up \" }, \" Terminate_Cluster \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:terminateCluster.sync \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" }, \" Next \" : \" Wrapping_Up \" }, \" Wrapping_Up \" : { \" Type \" : \" Pass \" , \" End \" : true } } } You can run the below commands on EC2 Jump Host Session Manager session to get the values you have to replace. sudo su ec2 - user cd ~ accountID = ` aws sts get - caller - identity -- query \" Account \" -- output text ` instProfRole = ` aws iam list - instance - profiles - for - role -- role - name emrEc2InstanceProfileRole | jq . ' InstanceProfiles[].InstanceProfileName ' | sed \" s|\\ \" || g \" ` printf \" \\n\\n\\n\\n Replace the following values in your State Machine definition.... \\n\\n\\n\\n \" Get the output account ID and EMR Instance Profile Role. echo $accountID echo $instProfRole Replace the string \"youraccountID\" (in 3 places) with your AWS event engine account ID obtained from the AWS Web Console. Also, replace the EMR Instance Profile Role \"emrEc2InstanceProfileRole\" (in one place). Make sure there are no control characters. Once you have updated, copy the entire content from your notepad or text editor and paste it onto the \"Definition\" section in the AWS Step Functions console. You will see the state machine DAG being generated automatically. Click on \"Next\". Name your State Machine as \"EMR-Steps-State-Machine\". Under \"Permissions\", select \"Choose an existing role\" and from the drop down, choose the IAM role \"emr-stepfunc-role\". Leave everything else as default. Click on \"Create State Machine\". Execute State Machine \u00b6 Once the State Machine is created, click on Start Execution. Enter the below JSON in the \"Input - optional\" section (replace any existing string). Click on \"Start Execution\". { \"CreateCluster\": true, \"TerminateCluster\": true } You will see the State Machine executing. It should start creating an EMR cluster called \"WorkflowCluster\" if you check the EMR Web Console (Right click -> Open Link in New Tab). It will take about 15 minutes for the State Machine to finish executing. Once State Machine is done with the execution, it will look like below. Execution Status should be \"Succeeded\". Go to the EMR cluster \"WorkflowCluster\" from EMR Web Console (Right click -> Open Link in New Tab). Go to the \"Steps\" tab. You can see the three EMR Steps submitted and executed by the AWS Step Functions. The EMR cluster will be automatically terminated once the State Machine completes its execution. Optional: You can also start State Machine execution by entering the below JSON in the \"Input - optional\" section (replace any existing string). i.e., you can specify an existing EMR cluster ID instead of having AWS Step Functions create a new EMR cluster. You will need to replace j-XXXXXXXXXX in the below JSON with an existing EMR cluster ID in your AWS event engine account. { \"CreateCluster\": false, \"TerminateCluster\": false, \"ClusterId\": \"j-XXXXXXXXXX\" } Once modified, you can enter the above JSON in the \"Input - optional\" section (replace any existing string). Click on \"Start Execution\". Please note that if you use an EMR cluster ID of recent release label (EMR 6.x), the EMR Step execution may fail due to syntax changes in Hive major and minor versions. Schedule AWS Step Functions with Amazon Event Bridge \u00b6 We triggered the AWS Step Functions execution on-demand. We can also leverage Amazon Event Bridge to schedule workloads. Go to AWS Step Functions Console (Right click -> Open Link in New Tab). Click on the state machine we created \"EMR-Steps-State-Machine\". On the right hand side, from Actions drop down, choose \"Create Event Bridge (CloudWatch Events) rule\". It will take you to a new tab to create Amazon Event Bridge Rule. Specify rule name \"emr-stepfunc-rule\". Under \"Rule type\", select \"Schedule\". Click Next. In Step 2 (Define schedule), under \"Schedule pattern\", choose \"A schedule that runs at a regular rate, such as every 10 minutes.\". Under \"Rate expression\", choose 60 minutes. Click Next. In Step 3 (Select targets), for \"Target types\", choose AWS service. In \"Select a target\", type and choose \"Step Functions state machine\". Under \"State machine\", choose the State Machine we created \"EMR-Steps-State-Machine\". Under \"Execution role\", choose \"Create a new role for this specific resource\". Expand the \"Additional settings\" section. Under \"Configure target input\", choose \"Constant (JSON Text)\" and enter the following input JSON. Leave everything else as default. { \"CreateCluster\": true, \"TerminateCluster\": true } Click Next. In Step 4 (Configure tags), just click Next. In Step 5 (Review and Create), review the details and click on \"Create rule\". The Event Bridge rule will be created. This rule will execute the AWS Step Functions every 60 mins and will take effect immediately. In your AWS Step Functions, you will see the state machine getting executed. Once you are done experimenting, you can disable the Amazon EventBridge rule to avoid cluttering the EMR Web Console. Go to Amazon Event Bridge Console (Right click -> Open Link in New Tab) -> click on \"emr-stepfunc-rule\" -> Disable.","title":"3 - Orchestrate EMR Steps using AWS Step Functions"},{"location":"day1/step/exercise/#orchestrating-emr-steps-using-aws-step-functions","text":"In the previous section, we saw how you can leverage EMR AddSteps API to submit a Spark work to the cluster. You can submit any type of work to the EMR cluster (Hive, Presto, Bash etc.). In this section, we will see how you can orchestrate EMR jobs using EMR Step API and AWS Steps Functions. For a variation, let's orchestrate Hive EMR Steps. But you can choose any EMR step of your choice.","title":"Orchestrating EMR Steps using AWS Step Functions"},{"location":"day1/step/exercise/#create-state-machine","text":"Go to AWS Step Functions console (Right click -> Open Link in New Tab). Click on \"Create State Machine\". In Step 1 (Define state machine), choose \"Write your workflow in code\". Let Type be \"Standard\". Copy the entire blurb below and paste into a notepad or any text editor in your local desktop. { \" StartAt \" : \" Should_Create_Cluster \" , \" States \" : { \" Should_Create_Cluster \" : { \" Type \" : \" Choice \" , \" Choices \" : [ { \" Variable \" : \" $.CreateCluster \" , \" BooleanEquals \" : true , \" Next \" : \" Create_A_Cluster \" }, { \" Variable \" : \" $.CreateCluster \" , \" BooleanEquals \" : false , \" Next \" : \" Enable_Termination_Protection \" } ], \" Default \" : \" Create_A_Cluster \" }, \" Create_A_Cluster \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:createCluster.sync \" , \" Parameters \" : { \" Name \" : \" WorkflowCluster \" , \" VisibleToAllUsers \" : true , \" ReleaseLabel \" : \" emr-5.28.0 \" , \" Applications \" : [{ \" Name \" : \" Hive \" }], \" ServiceRole \" : \" emrServiceRole \" , \" JobFlowRole \" : \" emrEc2InstanceProfile \" , \" Instances \" : { \" KeepJobFlowAliveWhenNoSteps \" : true , \" InstanceFleets \" : [ { \" InstanceFleetType \" : \" MASTER \" , \" TargetOnDemandCapacity \" : 1 , \" InstanceTypeConfigs \" : [ { \" InstanceType \" : \" m4.xlarge \" } ] }, { \" InstanceFleetType \" : \" CORE \" , \" TargetOnDemandCapacity \" : 1 , \" InstanceTypeConfigs \" : [ { \" InstanceType \" : \" m4.xlarge \" } ] } ] } }, \" ResultPath \" : \" $.CreateClusterResult \" , \" Next \" : \" Merge_Results \" }, \" Merge_Results \" : { \" Type \" : \" Pass \" , \" Parameters \" : { \" CreateCluster.$ \" : \" $.CreateCluster \" , \" TerminateCluster.$ \" : \" $.TerminateCluster \" , \" ClusterId.$ \" : \" $.CreateClusterResult.ClusterId \" }, \" Next \" : \" Enable_Termination_Protection \" }, \" Enable_Termination_Protection \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:setClusterTerminationProtection \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" TerminationProtected \" : true }, \" ResultPath \" : null , \" Next \" : \" Add_Steps_Parallel \" }, \" Add_Steps_Parallel \" : { \" Type \" : \" Parallel \" , \" Branches \" : [ { \" StartAt \" : \" Step_One \" , \" States \" : { \" Step_One \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:addStep.sync \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" Step \" : { \" Name \" : \" The first step \" , \" ActionOnFailure \" : \" CONTINUE \" , \" HadoopJarStep \" : { \" Jar \" : \" command-runner.jar \" , \" Args \" : [ \" hive-script \" , \" --run-hive-script \" , \" --args \" , \" -f \" , \" s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q \" , \" -d \" , \" INPUT=s3://eu-west-1.elasticmapreduce.samples \" , \" -d \" , \" OUTPUT=s3://mrworkshop-youraccountID-dayone/StepFunc/MyHiveQueryResults/ \" ] } } }, \" End \" : true } } }, { \" StartAt \" : \" Wait_10_Seconds \" , \" States \" : { \" Wait_10_Seconds \" : { \" Type \" : \" Wait \" , \" Seconds \" : 10 , \" Next \" : \" Step_Two (async) \" }, \" Step_Two (async) \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:addStep \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" Step \" : { \" Name \" : \" The second step \" , \" ActionOnFailure \" : \" CONTINUE \" , \" HadoopJarStep \" : { \" Jar \" : \" command-runner.jar \" , \" Args \" : [ \" hive-script \" , \" --run-hive-script \" , \" --args \" , \" -f \" , \" s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q \" , \" -d \" , \" INPUT=s3://eu-west-1.elasticmapreduce.samples \" , \" -d \" , \" OUTPUT=s3://mrworkshop-youraccountID-dayone/StepFunc/MyHiveQueryResults/ \" ] } } }, \" ResultPath \" : \" $.AddStepsResult \" , \" Next \" : \" Wait_Another_10_Seconds \" }, \" Wait_Another_10_Seconds \" : { \" Type \" : \" Wait \" , \" Seconds \" : 10 , \" Next \" : \" Cancel_Step_Two \" }, \" Cancel_Step_Two \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:cancelStep \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" StepId.$ \" : \" $.AddStepsResult.StepId \" }, \" End \" : true } } } ], \" ResultPath \" : null , \" Next \" : \" Step_Three \" }, \" Step_Three \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:addStep.sync \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" Step \" : { \" Name \" : \" The third step \" , \" ActionOnFailure \" : \" CONTINUE \" , \" HadoopJarStep \" : { \" Jar \" : \" command-runner.jar \" , \" Args \" : [ \" hive-script \" , \" --run-hive-script \" , \" --args \" , \" -f \" , \" s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q \" , \" -d \" , \" INPUT=s3://eu-west-1.elasticmapreduce.samples \" , \" -d \" , \" OUTPUT=s3://mrworkshop-youraccountID-dayone/StepFunc/MyHiveQueryResults/ \" ] } } }, \" ResultPath \" : null , \" Next \" : \" Disable_Termination_Protection \" }, \" Disable_Termination_Protection \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:setClusterTerminationProtection \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" TerminationProtected \" : false }, \" ResultPath \" : null , \" Next \" : \" Should_Terminate_Cluster \" }, \" Should_Terminate_Cluster \" : { \" Type \" : \" Choice \" , \" Choices \" : [ { \" Variable \" : \" $.TerminateCluster \" , \" BooleanEquals \" : true , \" Next \" : \" Terminate_Cluster \" }, { \" Variable \" : \" $.TerminateCluster \" , \" BooleanEquals \" : false , \" Next \" : \" Wrapping_Up \" } ], \" Default \" : \" Wrapping_Up \" }, \" Terminate_Cluster \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:terminateCluster.sync \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" }, \" Next \" : \" Wrapping_Up \" }, \" Wrapping_Up \" : { \" Type \" : \" Pass \" , \" End \" : true } } } You can run the below commands on EC2 Jump Host Session Manager session to get the values you have to replace. sudo su ec2 - user cd ~ accountID = ` aws sts get - caller - identity -- query \" Account \" -- output text ` instProfRole = ` aws iam list - instance - profiles - for - role -- role - name emrEc2InstanceProfileRole | jq . ' InstanceProfiles[].InstanceProfileName ' | sed \" s|\\ \" || g \" ` printf \" \\n\\n\\n\\n Replace the following values in your State Machine definition.... \\n\\n\\n\\n \" Get the output account ID and EMR Instance Profile Role. echo $accountID echo $instProfRole Replace the string \"youraccountID\" (in 3 places) with your AWS event engine account ID obtained from the AWS Web Console. Also, replace the EMR Instance Profile Role \"emrEc2InstanceProfileRole\" (in one place). Make sure there are no control characters. Once you have updated, copy the entire content from your notepad or text editor and paste it onto the \"Definition\" section in the AWS Step Functions console. You will see the state machine DAG being generated automatically. Click on \"Next\". Name your State Machine as \"EMR-Steps-State-Machine\". Under \"Permissions\", select \"Choose an existing role\" and from the drop down, choose the IAM role \"emr-stepfunc-role\". Leave everything else as default. Click on \"Create State Machine\".","title":"Create State Machine"},{"location":"day1/step/exercise/#execute-state-machine","text":"Once the State Machine is created, click on Start Execution. Enter the below JSON in the \"Input - optional\" section (replace any existing string). Click on \"Start Execution\". { \"CreateCluster\": true, \"TerminateCluster\": true } You will see the State Machine executing. It should start creating an EMR cluster called \"WorkflowCluster\" if you check the EMR Web Console (Right click -> Open Link in New Tab). It will take about 15 minutes for the State Machine to finish executing. Once State Machine is done with the execution, it will look like below. Execution Status should be \"Succeeded\". Go to the EMR cluster \"WorkflowCluster\" from EMR Web Console (Right click -> Open Link in New Tab). Go to the \"Steps\" tab. You can see the three EMR Steps submitted and executed by the AWS Step Functions. The EMR cluster will be automatically terminated once the State Machine completes its execution. Optional: You can also start State Machine execution by entering the below JSON in the \"Input - optional\" section (replace any existing string). i.e., you can specify an existing EMR cluster ID instead of having AWS Step Functions create a new EMR cluster. You will need to replace j-XXXXXXXXXX in the below JSON with an existing EMR cluster ID in your AWS event engine account. { \"CreateCluster\": false, \"TerminateCluster\": false, \"ClusterId\": \"j-XXXXXXXXXX\" } Once modified, you can enter the above JSON in the \"Input - optional\" section (replace any existing string). Click on \"Start Execution\". Please note that if you use an EMR cluster ID of recent release label (EMR 6.x), the EMR Step execution may fail due to syntax changes in Hive major and minor versions.","title":"Execute State Machine"},{"location":"day1/step/exercise/#schedule-aws-step-functions-with-amazon-event-bridge","text":"We triggered the AWS Step Functions execution on-demand. We can also leverage Amazon Event Bridge to schedule workloads. Go to AWS Step Functions Console (Right click -> Open Link in New Tab). Click on the state machine we created \"EMR-Steps-State-Machine\". On the right hand side, from Actions drop down, choose \"Create Event Bridge (CloudWatch Events) rule\". It will take you to a new tab to create Amazon Event Bridge Rule. Specify rule name \"emr-stepfunc-rule\". Under \"Rule type\", select \"Schedule\". Click Next. In Step 2 (Define schedule), under \"Schedule pattern\", choose \"A schedule that runs at a regular rate, such as every 10 minutes.\". Under \"Rate expression\", choose 60 minutes. Click Next. In Step 3 (Select targets), for \"Target types\", choose AWS service. In \"Select a target\", type and choose \"Step Functions state machine\". Under \"State machine\", choose the State Machine we created \"EMR-Steps-State-Machine\". Under \"Execution role\", choose \"Create a new role for this specific resource\". Expand the \"Additional settings\" section. Under \"Configure target input\", choose \"Constant (JSON Text)\" and enter the following input JSON. Leave everything else as default. { \"CreateCluster\": true, \"TerminateCluster\": true } Click Next. In Step 4 (Configure tags), just click Next. In Step 5 (Review and Create), review the details and click on \"Create rule\". The Event Bridge rule will be created. This rule will execute the AWS Step Functions every 60 mins and will take effect immediately. In your AWS Step Functions, you will see the state machine getting executed. Once you are done experimenting, you can disable the Amazon EventBridge rule to avoid cluttering the EMR Web Console. Go to Amazon Event Bridge Console (Right click -> Open Link in New Tab) -> click on \"emr-stepfunc-rule\" -> Disable.","title":"Schedule AWS Step Functions with Amazon Event Bridge"},{"location":"day1/studio/exercise/","text":"Amazon EMR Studio \u00b6 Log in to EMR Studio \u00b6 In this exercise we will run Spark workflows using EMR Studio with managed Jupyter-based notebooks. We will also cover the most standout features of Amazon EMR Studio. Go to the EMR Web Console (Right click -> Open Link in New Tab) and navigate to \"EMR Studio\" on the right hand side. Click on \"Get Started\". You will be able to see an EMR Studio created called \"workshop-studio\". Click on it and in the following page, copy the Studio URL. Open an incognito or a private browser and paste the URL. In the AWS login page, choose \"IAM User\" and enter the account ID retrieved from your event engine's AWS Web console. Click on Next. Under IAM user name, enter \"studiouser\". Under password, enter Test123$. Click on Sign in. You will be logged into the EMR Studio. Users can access this interface without requiring AWS Web Console access. EMR Studio supports both IAM and SSO auth modes. Check EMR clusters from EMR Studio \u00b6 Check the clusters under EMR on EC2. You can filter the clusters. Click on \"EMR-Spark-Hive-Presto\" and go to \"Launch application UI -> Spark History Server\". You will be taken to the EMR Persistent Spark History Server. You can also see the UIs of terminated clusters for up to 60 days after termination. Create a Studio Workspace \u00b6 Go to Workspaces and \"Create Workspace\". Enter a workspace name. For example: \"studio-ws\". Enable \"Allow Workspace Collaboration\". Under \"Advanced Configuration\", select \"Attach Workspace to an EMR cluster\". In the drop down, choose the EMR-Spark-Hive-Presto cluster. Click \"Create Workspace\". It will take about 2 minutes for the Status to change to \"Attached\". Click on the workspace and it will open a managed JupyterLab session. You may need to allow pop-up from this address in your browser to open the JupyterLab. Once opened, you can create a Jupyter notebook with any kernel. Explore EMR Studio Workspace Features \u00b6 Cluster \u00b6 Under cluster tab, check the cluster attachment. Note that you will be able to detach and attach this workspace to a different cluster. For now, you can leave it as is. Git repository \u00b6 Under Git tab, you can add a Git repository by entering the repository name, URL and credentials. You can access public repositories without any credentials. Repository name: workshop-repo Git repository URL: https://github.com/vasveena/amazon-emr-ttt-workshop Branch: main Git credentials: Use a public repository without credentials Once the repository is added, select it from the \"Git repositories\" drop down. You will see that the Git repository will be linked successfully. Once its linked, you can go back to the workspace folder. You will find a folder called \"workshop-repo\". Go to workshop-repo -> files -> notebook to see the notebooks. If you are not able to link repository successfully, create 3 nested folders under your workspace root folder: workshop-repo/files/notebook. Create Folder Icon looks like . Download all the .ipynb files from here to your local desktop. You can download the entire project Zip File . Unzip the zip file and go to amazon-emr-ttt-workshop-main/files/notebook. Upload these .ipynb files from your local desktop to the Jupyter interface under the nested folders created (workshop-repo/files/notebook). Upload icon looks like . Alternate Option: Instead of uploading the files manually, you use the below commands on your JumpHost EC2 instance. Connect to your EC2 instance named \"JumpHost\" using Session Manager and run the below commands. sudo su ec2 - user cd ~ pip3 uninstall awscli - y pip3 install awscli -- upgrade / home / ec2 - user /. local / bin / aws -- version sudo yum install upgrade - y jq studio_id =$ ( / home / ec2 - user /. local / bin / aws emr -- region us - east - 1 list - studios -- region us - east - 1 -- query Studios [ * ] . { Studios : StudioId } -- output text ) studio_s3_location =$ ( / home / ec2 - user /. local / bin / aws emr -- region us - east - 1 describe - studio -- studio - id $ studio_id -- query 'Studio.DefaultS3Location' -- output text ) studio_notebook_id =$ ( aws s3 ls $ studio_s3_location / e - | sed 's|.*PRE ||g' | sed 's|/||g' | sed 's| ||g' ) accountID =$ ( aws sts get - caller - identity -- query \"Account\" -- output text ) clusterArn = ` aws kafka list - clusters -- region us - east - 1 | jq '.ClusterInfoList[0].ClusterArn' ` echo $ clusterArn bs =$ ( echo \"aws kafka get-bootstrap-brokers --cluster-arn ${clusterArn} --region us-east-1\" | bash | jq '.BootstrapBrokerString' | sed 's|\"||g' ) mkdir - p upload cd upload wget https : // github . com / vasveena / amazon - emr - ttt - workshop / archive / refs / heads / main . zip unzip main . zip sed - i \"s|youraccountID|$accountID|g\" amazon - emr - ttt - workshop - main / files / notebook / amazon - emr - spark - streaming - apache - hudi - demo . ipynb sed - i \"s|yourbootstrapbrokers|$bs|g\" amazon - emr - ttt - workshop - main / files / notebook / amazon - emr - spark - streaming - apache - hudi - demo . ipynb sed - i \"s|youraccountID|$accountID|g\" amazon - emr - ttt - workshop - main / files / notebook / apache - hudi - on - amazon - emr - datasource - pyspark - demo . ipynb sed - i \"s|youraccountID|$accountID|g\" amazon - emr - ttt - workshop - main / files / notebook / apache - hudi - on - amazon - emr - dml . ipynb sed - i \"s|youraccountID|$accountID|g\" amazon - emr - ttt - workshop - main / files / notebook / apache - iceberg - on - amazon - emr . ipynb sed - i \"s|youraccountID|$accountID|g\" amazon - emr - ttt - workshop - main / files / notebook / find_best_sellers . ipynb aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / amazon - emr - spark - streaming - apache - hudi - demo . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / apache - hudi - on - amazon - emr - datasource - pyspark - demo . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / apache - hudi - on - amazon - emr - dml . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / apache - iceberg - on - amazon - emr . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / find_best_sellers . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / amazon_reviews . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / smstudio - pyspark - hive - sentiment - analysis . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / Note: If you ran the above commands, you can skip the below steps and go to Restart Studio Workspace section. If you either uploaded the files manually or if your Git repository linking was successful, continue with the following instructions. Once you either link the repository or upload the files successfully, go to workshop-repo -> files -> notebook folder and open find_best_sellers.ipynb file. Notice the OUTPUT_LOCATION S3 path in the first cell: s3://mrworkshop-youraccountID-dayone/studio/best_sellers_output/ . Before you start working with these notebooks, let's replace the string \"youraccountID\" in the S3 locations specified in all the notebooks to make it easy for you while executing the notebook instructions. For this purpose, close the EMR Studio Workshop Jupyter interface in your browser. Connect to your EC2 instance named \"JumpHost\" using Session Manager and run the below commands. sudo su ec2-user cd ~ pip3 uninstall awscli -y pip3 install awscli --upgrade /home/ec2-user/.local/bin/aws --version sudo yum install upgrade -y jq studio_id=$(/home/ec2-user/.local/bin/aws emr --region us-east-1 list-studios --region us-east-1 --query Studios[*].{Studios:StudioId} --output text) studio_s3_location=$(/home/ec2-user/.local/bin/aws emr --region us-east-1 describe-studio --studio-id $studio_id --query 'Studio.DefaultS3Location' --output text) studio_notebook_id=$(aws s3 ls $studio_s3_location /e- | sed 's|.*PRE ||g' | sed 's|/||g' | sed 's| ||g') accountID=$(aws sts get-caller-identity --query \"Account\" --output text) clusterArn=`aws kafka list-clusters --region us-east-1 | jq '.ClusterInfoList[0].ClusterArn'` echo $clusterArn bs=$(echo \"aws kafka get-bootstrap-brokers --cluster-arn ${ clusterArn } --region us-east-1\" | bash | jq '.BootstrapBrokerString' | sed 's|\"||g') mkdir -p studio cd studio aws s3 cp $studio_s3_location / $studio_notebook_id /workshop-repo/files/notebook/ . --recursive sed -i \"s|youraccountID| $accountID |g\" amazon-emr-spark-streaming-apache-hudi-demo.ipynb sed -i \"s|yourbootstrapbrokers| $bs |g\" amazon-emr-spark-streaming-apache-hudi-demo.ipynb sed -i \"s|youraccountID| $accountID |g\" apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb sed -i \"s|youraccountID| $accountID |g\" apache-hudi-on-amazon-emr-dml.ipynb sed -i \"s|youraccountID| $accountID |g\" apache-iceberg-on-amazon-emr.ipynb sed -i \"s|youraccountID| $accountID |g\" find_best_sellers.ipynb aws s3 cp amazon-emr-spark-streaming-apache-hudi-demo.ipynb $studio_s3_location / $studio_notebook_id /workshop-repo/files/notebook/ aws s3 cp apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb $studio_s3_location / $studio_notebook_id /workshop-repo/files/notebook/ aws s3 cp apache-hudi-on-amazon-emr-dml.ipynb $studio_s3_location / $studio_notebook_id /workshop-repo/files/notebook/ aws s3 cp apache-iceberg-on-amazon-emr.ipynb $studio_s3_location / $studio_notebook_id /workshop-repo/files/notebook/ aws s3 cp find_best_sellers.ipynb $studio_s3_location / $studio_notebook_id /workshop-repo/files/notebook/ Restart the Studio Workspace \u00b6 Once the files are uploaded successfully, while logged in as studiouser, from your EMR Studio Console, go to Workspaces. Click on your workspace and under Actions, click on \"Stop\". Once the workspace status becomes \"Idle\" (it will take about 2-3 minutes), while logged in as studiouser, go back to EMR Studio Console -> Workspaces -> Actions again and click on \"Start\". Wait for the status to go from \"Starting\" to \"Attached\". Once the workspace is in \"Attached\" state, go back to your EMR Studio Workspace Jupyter interface. Open the find_best_sellers.ipynb notebook (from workshop-repo -> files -> notebooks) and verify that the the account ID in the OUTPUT_LOCATION of the first cell is changed properly. Notebook-scoped libraries \u00b6 Run all the cells in amazon-reviews.ipynb notebook. Make sure Pyspark kernel is selected. Notice the notebook scoped libraries installed on SparkContext sc. sc.list_packages() sc.install_pypi_package(\"pandas==1.0.1\") #Install pandas version 1.0.5 sc.install_pypi_package(\"numpy==1.20.2\") #Intall numpy version 1.19.5 sc.install_pypi_package(\"matplotlib==3.2.0\",\"https://pypi.org/simple\") #Install matplotlib from given PyPI repository sc.list_packages() You will use these installed dependencies to plot visualizations on top of Amazon Reviews data. You can have two notebooks within the same workspace with different dependencies. You can even reproduce these dependencies and run the same notebook after your cluster is terminated by attaching it to a different active cluster. When you are done, terminate the kernel by clicking on stop icon and restart the kernel by clicking on the restart kernel icon . This will ensure that the Spark session created from this notebook is killed and your EMR cluster's YARN resources will become free. Parameterized notebooks \u00b6 Open the file find_best_sellers.ipynb. Go to View -> Show Right Sidebar. Click on the first cell with comment \"Default parameters\". In the Right Sidebar, click on \"Add tag\" and type \"parameters\" and click \"+\". Now check the \"Advanced Tools\" and make sure that the parameters tag is applied to that cell. Do not create any S3 prefix under the OUTPUT_LOCATION before running the notebook cells. Run all the cells in the notebook. Once all the blocks are executed in the notebook, make sure the outputs for categories \"Apparel\" and \"Baby\" are created under the S3 output location using AWS CLI or S3 Web Console (Right click -> Open Link in New Tab). OR run the following commands in EC2 JumpHost session. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) aws s3 ls s3://mrworkshop-$accountID-dayone/studio/best_sellers_output/ Save the notebook. When you are done, terminate the kernel by clicking on the stop icon and the restart kernel icon . This will ensure that the Spark session created from this notebook is killed and your EMR cluster's YARN resources will become free. Notebooks API \u00b6 Let us run the parameterized notebook \"find_best_sellers.ipynb\" using EMR Notebooks API. Run the below command with your JumpHost EC2 instance (connected with Session Manager). aws s3 ls s3://amazon-reviews-pds/parquet/product_category You can see the list of categories. From EMR Studio, we ran analysis for categories \"Apparel\" and \"Baby\". Now let us run this notebook from API for categories \"Furniture\" and \"PC\". You can select whichever categories you want. Run following commands in your EC2 JumpHost to upgrade your AWS CLI (if not done already). sudo su ec2-user cd ~ pip3 uninstall awscli -y pip3 install awscli --upgrade /home/ec2-user/.local/bin/aws --version Verify that the notebooks APIs are working (if not done already). / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 list - studios Run the following commands to submit a job using EMR Notebooks API. accountID = $ ( aws sts get - caller - identity -- query \"Account\" -- output text ) studio_id = $ ( / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 list - studios -- region us - east - 1 -- query Studios [ * ] . { Studios : StudioId } -- output text ) studio_s3_location = $ ( / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 describe - studio -- studio - id $studio_id -- query 'Studio.DefaultS3Location' -- output text ) studio_notebook_id = $ ( aws s3 ls $studio_s3_location / e - | sed 's|.*PRE ||g' | sed 's|/||g' | sed 's| ||g' ) cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) notebookExecID = $ ( / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 \\ start - notebook - execution \\ -- editor - id $studio_notebook_id \\ -- notebook - params \"{ \\\" CATEGORIES \\\" :[ \\\" Furniture \\\" , \\\" PC \\\" ], \\\" FROM_DATE \\\" : \\\" 2015-08-27 \\\" , \\\" TO_DATE \\\" : \\\" 2015-08-31 \\\" , \\\" OUTPUT_LOCATION \\\" : \\\" s3://mrworkshop-$accountID-dayone/studio/best_sellers_output_fromapi/ \\\" }\" \\ -- relative - path workshop - repo / files / notebook / find_best_sellers . ipynb \\ -- notebook - execution - name demo - execution \\ -- execution - engine \"{ \\\" Id \\\" : \\\" ${cluster_id} \\\" }\" \\ -- service - role emrStudioRole | jq - r . NotebookExecutionId ) echo $notebookExecID You will get a NotebookExecutionId in return. Run the following command to get the status of this notebook execution. No need to replace anything in the command. aws emr -- region us - east - 1 describe - notebook - execution -- notebook - execution - id $notebookExecID After about 2-3 minutes, the Status will be FINISHED. aws emr -- region us - east - 1 describe - notebook - execution -- notebook - execution - id $notebookExecID { \"NotebookExecution\" : { \"Status\" : \"FINISHED\" , \"ExecutionEngine\" : { \"MasterInstanceSecurityGroupId\" : \"sg-066e6805267d1d69c\" , \"Type\" : \"EMR\" , \"Id\" : \"j-142PVKGDZTTXS\" }, \"NotebookParams\" : \"{ \\\" CATEGORIES \\\" :[ \\\" Furniture \\\" , \\\" PC \\\" ], \\\" FROM_DATE \\\" : \\\" 2015-08-27 \\\" , \\\" TO_DATE \\\" : \\\" 2015-08-31 \\\" , \\\" OUTPUT_LOCATION \\\" : \\\" s3://mrworkshop-352365466794-dayone/studio/best_sellers_output_fromapi/ \\\" }\" , \"Tags\" : [], \"OutputNotebookURI\" : \"s3://studio-352365466794-dayone/notebook/e-C9ZY9CMD24CCF4F2B4UZ7D7MA/executions/ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV/find_best_sellers.ipynb\" , \"NotebookExecutionName\" : \"demo-execution\" , \"LastStateChangeReason\" : \"Execution is finished for cluster j-142PVKGDZTTXS.\" , \"StartTime\" : 1647768150.761 , \"NotebookExecutionId\" : \"ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV\" , \"EndTime\" : 1647768220.416 , \"EditorId\" : \"e-C9ZY9CMD24CCF4F2B4UZ7D7MA\" , \"Arn\" : \"arn:aws:elasticmapreduce:us-east-1:352365466794:notebook-execution/ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV\" , \"NotebookInstanceSecurityGroupId\" : \"sg-05e5e70bcaa4a624f\" } } Now let us check the S3 output path. You will now see a new prefix called \"best_sellers_output_fromapi\" with files generated under categories \"Furniture\" and \"PC\". We will see in tomorrow's exercise how to orchestrate a pipeline with this parameterized notebook using Amazon Managed Workflows for Apache Airflow. SQL Explorer \u00b6 Lets check the new SQL explorer feature which helps you run ad-hoc and interactive queries against your tables. Go to the SQL explorer and select \"default\" database. You will be able to see the four tables created in Glue catalog for the 4 categories apparel, baby, furniture and PC from our previous job runs. Click on \"Open Editor\" and query the tables. select * from default.baby limit 10; Collaborators \u00b6 Workspace collaboration is a new feature introduced in EMR Studio. Currently, we are logged in as studiouser. Go to the root folder and choose studio-ws.ipynb which is the default notebook created for this workspace. You can choose any kernel. Let us choose Python3 kernel for this time. Type the following command on the cell. print(\"hello world\") Now go to the Collaborators section and add the IAM user \"collabuser\". Make sure that the user is added to the workspace collaborators. Open another incognito or private window in your browser and paste the Studio access URL (for example: https://es-8QX8R2BETY6B8HA0Y6QM7G6EC.emrstudio-prod.us-east-1.amazonaws.com?locale=en). Click on Logout and logout as studiouser. Once signed out, do not click on \"Log Back In\". Paste the Studio access URL again in the same window and you will be re-directed to login page. Enter your event engine AWS account ID. Under IAM user name, enter collab user. Under password, enter \"Test123$\". Click on sign in. Once logged in, click on the workspace \"studio-ws\" and open JupyterLab console. Now, open the studio-ws.ipynb file. Open the two private browsers side by side with one browser session for IAM user \"studiouser\" and another one for IAM user \"collabuser\". Hover over the hello world code cell from collabuser's browser and see the user name from the studiouser's browser. Similarly, you can hover over the cell from studiouser's browser and see the user name from the collabuser's browser. You can also edit the same cell as collabuser and see the changes getting reflected from the studiouser's browser. This feature is very useful for collaborating with your team members for live troubleshooting and brainstorming.","title":"1 - Amazon EMR Studio"},{"location":"day1/studio/exercise/#amazon-emr-studio","text":"","title":"Amazon EMR Studio"},{"location":"day1/studio/exercise/#log-in-to-emr-studio","text":"In this exercise we will run Spark workflows using EMR Studio with managed Jupyter-based notebooks. We will also cover the most standout features of Amazon EMR Studio. Go to the EMR Web Console (Right click -> Open Link in New Tab) and navigate to \"EMR Studio\" on the right hand side. Click on \"Get Started\". You will be able to see an EMR Studio created called \"workshop-studio\". Click on it and in the following page, copy the Studio URL. Open an incognito or a private browser and paste the URL. In the AWS login page, choose \"IAM User\" and enter the account ID retrieved from your event engine's AWS Web console. Click on Next. Under IAM user name, enter \"studiouser\". Under password, enter Test123$. Click on Sign in. You will be logged into the EMR Studio. Users can access this interface without requiring AWS Web Console access. EMR Studio supports both IAM and SSO auth modes.","title":"Log in to EMR Studio"},{"location":"day1/studio/exercise/#check-emr-clusters-from-emr-studio","text":"Check the clusters under EMR on EC2. You can filter the clusters. Click on \"EMR-Spark-Hive-Presto\" and go to \"Launch application UI -> Spark History Server\". You will be taken to the EMR Persistent Spark History Server. You can also see the UIs of terminated clusters for up to 60 days after termination.","title":"Check EMR clusters from EMR Studio"},{"location":"day1/studio/exercise/#create-a-studio-workspace","text":"Go to Workspaces and \"Create Workspace\". Enter a workspace name. For example: \"studio-ws\". Enable \"Allow Workspace Collaboration\". Under \"Advanced Configuration\", select \"Attach Workspace to an EMR cluster\". In the drop down, choose the EMR-Spark-Hive-Presto cluster. Click \"Create Workspace\". It will take about 2 minutes for the Status to change to \"Attached\". Click on the workspace and it will open a managed JupyterLab session. You may need to allow pop-up from this address in your browser to open the JupyterLab. Once opened, you can create a Jupyter notebook with any kernel.","title":"Create a Studio Workspace"},{"location":"day1/studio/exercise/#explore-emr-studio-workspace-features","text":"","title":"Explore EMR Studio Workspace Features"},{"location":"day1/studio/exercise/#cluster","text":"Under cluster tab, check the cluster attachment. Note that you will be able to detach and attach this workspace to a different cluster. For now, you can leave it as is.","title":"Cluster"},{"location":"day1/studio/exercise/#git-repository","text":"Under Git tab, you can add a Git repository by entering the repository name, URL and credentials. You can access public repositories without any credentials. Repository name: workshop-repo Git repository URL: https://github.com/vasveena/amazon-emr-ttt-workshop Branch: main Git credentials: Use a public repository without credentials Once the repository is added, select it from the \"Git repositories\" drop down. You will see that the Git repository will be linked successfully. Once its linked, you can go back to the workspace folder. You will find a folder called \"workshop-repo\". Go to workshop-repo -> files -> notebook to see the notebooks. If you are not able to link repository successfully, create 3 nested folders under your workspace root folder: workshop-repo/files/notebook. Create Folder Icon looks like . Download all the .ipynb files from here to your local desktop. You can download the entire project Zip File . Unzip the zip file and go to amazon-emr-ttt-workshop-main/files/notebook. Upload these .ipynb files from your local desktop to the Jupyter interface under the nested folders created (workshop-repo/files/notebook). Upload icon looks like . Alternate Option: Instead of uploading the files manually, you use the below commands on your JumpHost EC2 instance. Connect to your EC2 instance named \"JumpHost\" using Session Manager and run the below commands. sudo su ec2 - user cd ~ pip3 uninstall awscli - y pip3 install awscli -- upgrade / home / ec2 - user /. local / bin / aws -- version sudo yum install upgrade - y jq studio_id =$ ( / home / ec2 - user /. local / bin / aws emr -- region us - east - 1 list - studios -- region us - east - 1 -- query Studios [ * ] . { Studios : StudioId } -- output text ) studio_s3_location =$ ( / home / ec2 - user /. local / bin / aws emr -- region us - east - 1 describe - studio -- studio - id $ studio_id -- query 'Studio.DefaultS3Location' -- output text ) studio_notebook_id =$ ( aws s3 ls $ studio_s3_location / e - | sed 's|.*PRE ||g' | sed 's|/||g' | sed 's| ||g' ) accountID =$ ( aws sts get - caller - identity -- query \"Account\" -- output text ) clusterArn = ` aws kafka list - clusters -- region us - east - 1 | jq '.ClusterInfoList[0].ClusterArn' ` echo $ clusterArn bs =$ ( echo \"aws kafka get-bootstrap-brokers --cluster-arn ${clusterArn} --region us-east-1\" | bash | jq '.BootstrapBrokerString' | sed 's|\"||g' ) mkdir - p upload cd upload wget https : // github . com / vasveena / amazon - emr - ttt - workshop / archive / refs / heads / main . zip unzip main . zip sed - i \"s|youraccountID|$accountID|g\" amazon - emr - ttt - workshop - main / files / notebook / amazon - emr - spark - streaming - apache - hudi - demo . ipynb sed - i \"s|yourbootstrapbrokers|$bs|g\" amazon - emr - ttt - workshop - main / files / notebook / amazon - emr - spark - streaming - apache - hudi - demo . ipynb sed - i \"s|youraccountID|$accountID|g\" amazon - emr - ttt - workshop - main / files / notebook / apache - hudi - on - amazon - emr - datasource - pyspark - demo . ipynb sed - i \"s|youraccountID|$accountID|g\" amazon - emr - ttt - workshop - main / files / notebook / apache - hudi - on - amazon - emr - dml . ipynb sed - i \"s|youraccountID|$accountID|g\" amazon - emr - ttt - workshop - main / files / notebook / apache - iceberg - on - amazon - emr . ipynb sed - i \"s|youraccountID|$accountID|g\" amazon - emr - ttt - workshop - main / files / notebook / find_best_sellers . ipynb aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / amazon - emr - spark - streaming - apache - hudi - demo . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / apache - hudi - on - amazon - emr - datasource - pyspark - demo . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / apache - hudi - on - amazon - emr - dml . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / apache - iceberg - on - amazon - emr . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / find_best_sellers . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / amazon_reviews . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / aws s3 cp amazon - emr - ttt - workshop - main / files / notebook / smstudio - pyspark - hive - sentiment - analysis . ipynb $ studio_s3_location /$ studio_notebook_id / workshop - repo / files / notebook / Note: If you ran the above commands, you can skip the below steps and go to Restart Studio Workspace section. If you either uploaded the files manually or if your Git repository linking was successful, continue with the following instructions. Once you either link the repository or upload the files successfully, go to workshop-repo -> files -> notebook folder and open find_best_sellers.ipynb file. Notice the OUTPUT_LOCATION S3 path in the first cell: s3://mrworkshop-youraccountID-dayone/studio/best_sellers_output/ . Before you start working with these notebooks, let's replace the string \"youraccountID\" in the S3 locations specified in all the notebooks to make it easy for you while executing the notebook instructions. For this purpose, close the EMR Studio Workshop Jupyter interface in your browser. Connect to your EC2 instance named \"JumpHost\" using Session Manager and run the below commands. sudo su ec2-user cd ~ pip3 uninstall awscli -y pip3 install awscli --upgrade /home/ec2-user/.local/bin/aws --version sudo yum install upgrade -y jq studio_id=$(/home/ec2-user/.local/bin/aws emr --region us-east-1 list-studios --region us-east-1 --query Studios[*].{Studios:StudioId} --output text) studio_s3_location=$(/home/ec2-user/.local/bin/aws emr --region us-east-1 describe-studio --studio-id $studio_id --query 'Studio.DefaultS3Location' --output text) studio_notebook_id=$(aws s3 ls $studio_s3_location /e- | sed 's|.*PRE ||g' | sed 's|/||g' | sed 's| ||g') accountID=$(aws sts get-caller-identity --query \"Account\" --output text) clusterArn=`aws kafka list-clusters --region us-east-1 | jq '.ClusterInfoList[0].ClusterArn'` echo $clusterArn bs=$(echo \"aws kafka get-bootstrap-brokers --cluster-arn ${ clusterArn } --region us-east-1\" | bash | jq '.BootstrapBrokerString' | sed 's|\"||g') mkdir -p studio cd studio aws s3 cp $studio_s3_location / $studio_notebook_id /workshop-repo/files/notebook/ . --recursive sed -i \"s|youraccountID| $accountID |g\" amazon-emr-spark-streaming-apache-hudi-demo.ipynb sed -i \"s|yourbootstrapbrokers| $bs |g\" amazon-emr-spark-streaming-apache-hudi-demo.ipynb sed -i \"s|youraccountID| $accountID |g\" apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb sed -i \"s|youraccountID| $accountID |g\" apache-hudi-on-amazon-emr-dml.ipynb sed -i \"s|youraccountID| $accountID |g\" apache-iceberg-on-amazon-emr.ipynb sed -i \"s|youraccountID| $accountID |g\" find_best_sellers.ipynb aws s3 cp amazon-emr-spark-streaming-apache-hudi-demo.ipynb $studio_s3_location / $studio_notebook_id /workshop-repo/files/notebook/ aws s3 cp apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb $studio_s3_location / $studio_notebook_id /workshop-repo/files/notebook/ aws s3 cp apache-hudi-on-amazon-emr-dml.ipynb $studio_s3_location / $studio_notebook_id /workshop-repo/files/notebook/ aws s3 cp apache-iceberg-on-amazon-emr.ipynb $studio_s3_location / $studio_notebook_id /workshop-repo/files/notebook/ aws s3 cp find_best_sellers.ipynb $studio_s3_location / $studio_notebook_id /workshop-repo/files/notebook/","title":"Git repository"},{"location":"day1/studio/exercise/#restart-the-studio-workspace","text":"Once the files are uploaded successfully, while logged in as studiouser, from your EMR Studio Console, go to Workspaces. Click on your workspace and under Actions, click on \"Stop\". Once the workspace status becomes \"Idle\" (it will take about 2-3 minutes), while logged in as studiouser, go back to EMR Studio Console -> Workspaces -> Actions again and click on \"Start\". Wait for the status to go from \"Starting\" to \"Attached\". Once the workspace is in \"Attached\" state, go back to your EMR Studio Workspace Jupyter interface. Open the find_best_sellers.ipynb notebook (from workshop-repo -> files -> notebooks) and verify that the the account ID in the OUTPUT_LOCATION of the first cell is changed properly.","title":"Restart the Studio Workspace"},{"location":"day1/studio/exercise/#notebook-scoped-libraries","text":"Run all the cells in amazon-reviews.ipynb notebook. Make sure Pyspark kernel is selected. Notice the notebook scoped libraries installed on SparkContext sc. sc.list_packages() sc.install_pypi_package(\"pandas==1.0.1\") #Install pandas version 1.0.5 sc.install_pypi_package(\"numpy==1.20.2\") #Intall numpy version 1.19.5 sc.install_pypi_package(\"matplotlib==3.2.0\",\"https://pypi.org/simple\") #Install matplotlib from given PyPI repository sc.list_packages() You will use these installed dependencies to plot visualizations on top of Amazon Reviews data. You can have two notebooks within the same workspace with different dependencies. You can even reproduce these dependencies and run the same notebook after your cluster is terminated by attaching it to a different active cluster. When you are done, terminate the kernel by clicking on stop icon and restart the kernel by clicking on the restart kernel icon . This will ensure that the Spark session created from this notebook is killed and your EMR cluster's YARN resources will become free.","title":"Notebook-scoped libraries"},{"location":"day1/studio/exercise/#parameterized-notebooks","text":"Open the file find_best_sellers.ipynb. Go to View -> Show Right Sidebar. Click on the first cell with comment \"Default parameters\". In the Right Sidebar, click on \"Add tag\" and type \"parameters\" and click \"+\". Now check the \"Advanced Tools\" and make sure that the parameters tag is applied to that cell. Do not create any S3 prefix under the OUTPUT_LOCATION before running the notebook cells. Run all the cells in the notebook. Once all the blocks are executed in the notebook, make sure the outputs for categories \"Apparel\" and \"Baby\" are created under the S3 output location using AWS CLI or S3 Web Console (Right click -> Open Link in New Tab). OR run the following commands in EC2 JumpHost session. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) aws s3 ls s3://mrworkshop-$accountID-dayone/studio/best_sellers_output/ Save the notebook. When you are done, terminate the kernel by clicking on the stop icon and the restart kernel icon . This will ensure that the Spark session created from this notebook is killed and your EMR cluster's YARN resources will become free.","title":"Parameterized notebooks"},{"location":"day1/studio/exercise/#notebooks-api","text":"Let us run the parameterized notebook \"find_best_sellers.ipynb\" using EMR Notebooks API. Run the below command with your JumpHost EC2 instance (connected with Session Manager). aws s3 ls s3://amazon-reviews-pds/parquet/product_category You can see the list of categories. From EMR Studio, we ran analysis for categories \"Apparel\" and \"Baby\". Now let us run this notebook from API for categories \"Furniture\" and \"PC\". You can select whichever categories you want. Run following commands in your EC2 JumpHost to upgrade your AWS CLI (if not done already). sudo su ec2-user cd ~ pip3 uninstall awscli -y pip3 install awscli --upgrade /home/ec2-user/.local/bin/aws --version Verify that the notebooks APIs are working (if not done already). / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 list - studios Run the following commands to submit a job using EMR Notebooks API. accountID = $ ( aws sts get - caller - identity -- query \"Account\" -- output text ) studio_id = $ ( / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 list - studios -- region us - east - 1 -- query Studios [ * ] . { Studios : StudioId } -- output text ) studio_s3_location = $ ( / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 describe - studio -- studio - id $studio_id -- query 'Studio.DefaultS3Location' -- output text ) studio_notebook_id = $ ( aws s3 ls $studio_s3_location / e - | sed 's|.*PRE ||g' | sed 's|/||g' | sed 's| ||g' ) cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) notebookExecID = $ ( / home / ec2 - user / . local / bin / aws emr -- region us - east - 1 \\ start - notebook - execution \\ -- editor - id $studio_notebook_id \\ -- notebook - params \"{ \\\" CATEGORIES \\\" :[ \\\" Furniture \\\" , \\\" PC \\\" ], \\\" FROM_DATE \\\" : \\\" 2015-08-27 \\\" , \\\" TO_DATE \\\" : \\\" 2015-08-31 \\\" , \\\" OUTPUT_LOCATION \\\" : \\\" s3://mrworkshop-$accountID-dayone/studio/best_sellers_output_fromapi/ \\\" }\" \\ -- relative - path workshop - repo / files / notebook / find_best_sellers . ipynb \\ -- notebook - execution - name demo - execution \\ -- execution - engine \"{ \\\" Id \\\" : \\\" ${cluster_id} \\\" }\" \\ -- service - role emrStudioRole | jq - r . NotebookExecutionId ) echo $notebookExecID You will get a NotebookExecutionId in return. Run the following command to get the status of this notebook execution. No need to replace anything in the command. aws emr -- region us - east - 1 describe - notebook - execution -- notebook - execution - id $notebookExecID After about 2-3 minutes, the Status will be FINISHED. aws emr -- region us - east - 1 describe - notebook - execution -- notebook - execution - id $notebookExecID { \"NotebookExecution\" : { \"Status\" : \"FINISHED\" , \"ExecutionEngine\" : { \"MasterInstanceSecurityGroupId\" : \"sg-066e6805267d1d69c\" , \"Type\" : \"EMR\" , \"Id\" : \"j-142PVKGDZTTXS\" }, \"NotebookParams\" : \"{ \\\" CATEGORIES \\\" :[ \\\" Furniture \\\" , \\\" PC \\\" ], \\\" FROM_DATE \\\" : \\\" 2015-08-27 \\\" , \\\" TO_DATE \\\" : \\\" 2015-08-31 \\\" , \\\" OUTPUT_LOCATION \\\" : \\\" s3://mrworkshop-352365466794-dayone/studio/best_sellers_output_fromapi/ \\\" }\" , \"Tags\" : [], \"OutputNotebookURI\" : \"s3://studio-352365466794-dayone/notebook/e-C9ZY9CMD24CCF4F2B4UZ7D7MA/executions/ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV/find_best_sellers.ipynb\" , \"NotebookExecutionName\" : \"demo-execution\" , \"LastStateChangeReason\" : \"Execution is finished for cluster j-142PVKGDZTTXS.\" , \"StartTime\" : 1647768150.761 , \"NotebookExecutionId\" : \"ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV\" , \"EndTime\" : 1647768220.416 , \"EditorId\" : \"e-C9ZY9CMD24CCF4F2B4UZ7D7MA\" , \"Arn\" : \"arn:aws:elasticmapreduce:us-east-1:352365466794:notebook-execution/ex-J02QDLG4TWXSNWLO4OGZ9NNX609MV\" , \"NotebookInstanceSecurityGroupId\" : \"sg-05e5e70bcaa4a624f\" } } Now let us check the S3 output path. You will now see a new prefix called \"best_sellers_output_fromapi\" with files generated under categories \"Furniture\" and \"PC\". We will see in tomorrow's exercise how to orchestrate a pipeline with this parameterized notebook using Amazon Managed Workflows for Apache Airflow.","title":"Notebooks API"},{"location":"day1/studio/exercise/#sql-explorer","text":"Lets check the new SQL explorer feature which helps you run ad-hoc and interactive queries against your tables. Go to the SQL explorer and select \"default\" database. You will be able to see the four tables created in Glue catalog for the 4 categories apparel, baby, furniture and PC from our previous job runs. Click on \"Open Editor\" and query the tables. select * from default.baby limit 10;","title":"SQL Explorer"},{"location":"day1/studio/exercise/#collaborators","text":"Workspace collaboration is a new feature introduced in EMR Studio. Currently, we are logged in as studiouser. Go to the root folder and choose studio-ws.ipynb which is the default notebook created for this workspace. You can choose any kernel. Let us choose Python3 kernel for this time. Type the following command on the cell. print(\"hello world\") Now go to the Collaborators section and add the IAM user \"collabuser\". Make sure that the user is added to the workspace collaborators. Open another incognito or private window in your browser and paste the Studio access URL (for example: https://es-8QX8R2BETY6B8HA0Y6QM7G6EC.emrstudio-prod.us-east-1.amazonaws.com?locale=en). Click on Logout and logout as studiouser. Once signed out, do not click on \"Log Back In\". Paste the Studio access URL again in the same window and you will be re-directed to login page. Enter your event engine AWS account ID. Under IAM user name, enter collab user. Under password, enter \"Test123$\". Click on sign in. Once logged in, click on the workspace \"studio-ws\" and open JupyterLab console. Now, open the studio-ws.ipynb file. Open the two private browsers side by side with one browser session for IAM user \"studiouser\" and another one for IAM user \"collabuser\". Hover over the hello world code cell from collabuser's browser and see the user name from the studiouser's browser. Similarly, you can hover over the cell from studiouser's browser and see the user name from the collabuser's browser. You can also edit the same cell as collabuser and see the changes getting reflected from the studiouser's browser. This feature is very useful for collaborating with your team members for live troubleshooting and brainstorming.","title":"Collaborators"},{"location":"day2/hudi/exercise/","text":"Apache Hudi on Amazon EMR \u00b6 In this exercise you will build incremental data lakes on EMR using Apache Hudi. You can build data lakes using Apache Hudi using Spark Datasource APIs, Hudi Deltastreamer utility and SparkSQL. You will also build a real-time live incremental data lake with Spark Structured Streaming + Amazon Managed Streaming for Apache Kafka (MSK) + Apache Hudi. In the previous EMR Studio exercise, we linked the Git repository in the Jupyter interface. We will continue to use the same repository to run these exercises. SSH into the EMR leader node of the cluster \"EMR-Spark-Hive-Presto\" or open a session using AWS Session Manager for the EMR leader node since we will be running a few commands directly on the leader node. Apache Hudi with Spark Datasource APIs \u00b6 Open the file workshop-repo -> files -> notebook -> apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb in the Jupyter. Make sure the Kernel is set to PySpark. All the instructions required to run the notebook are within the notebook itself. Download the file workshop-repo -> schema -> schema.avsc to your local desktop and upload this file into the following S3 location (replace \"youraccountID\" with your event engine AWS account ID): s3://mrworkshop-youraccountID-dayone/schema/schema.avsc Alternatively, you can run the following commands from the leader node of your EMR cluster. We will be using this schema AVRO file to run compaction on Merge-On-Read tables. sudo su hadoop cd ~ accountID=$(aws sts get-caller-identity --query \"Account\" --output text) curl -o schema.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/schema.avsc aws s3 cp schema.avsc s3://mrworkshop-$accountID-dayone/schema/schema.avsc Run the blocks of the notebook \"apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb\". Apache Hudi with SparkSQL DMLs \u00b6 From EMR 6.5.0, you can write Hudi datasets using simple SQL statements. Let's look at an example. From the EMR Studio workspace Jupyterlab session, go to workshop-repo -> files -> notebook -> apache-hudi-on-amazon-emr-dml.ipynb. Run all the blocks of this notebook. Detailed instructions are within the notebook. Apache Hudi with Spark Deltastreamer \u00b6 Hudi provides a utility called Deltastreamer for creating and manipulating Hudi datasets without the need to write any Spark code. For this activity, let us copy a few files to the S3 location. Run the following commands in your EMR leader node session created using Session Manager or SSH. sudo su hadoop cd ~ accountID=$(aws sts get-caller-identity --query \"Account\" --output text) curl -o source-schema-json.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/source-schema-json.avsc curl -o target-schema-json.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/target-schema-json.avsc curl -o json-deltastreamer.properties https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/properties/json-deltastreamer.properties curl -o json-deltastreamer_upsert.properties https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/properties/json-deltastreamer_upsert.properties curl -o apache-hudi-on-amazon-emr-deltastreamer-python-demo.py https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/script/apache-hudi-on-amazon-emr-deltastreamer-python-demo.py Replace event engine AWS account ID in the files json-deltastreamer.properties, json-deltastreamer_upsert.properties and apache-hudi-on-amazon-emr-deltastreamer-python-demo.py. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) sed -i \"s|youraccountID|$accountID|g\" json-deltastreamer.properties sed -i \"s|youraccountID|$accountID|g\" json-deltastreamer_upsert.properties sed -i \"s|youraccountID|$accountID|g\" apache-hudi-on-amazon-emr-deltastreamer-python-demo.py Now, copy the four files to your S3 location. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) aws s3 cp source-schema-json.avsc s3://mrworkshop-$accountID-dayone/hudi-ds/config/ aws s3 cp target-schema-json.avsc s3://mrworkshop-$accountID-dayone/hudi-ds/config/ aws s3 cp json-deltastreamer.properties s3://mrworkshop-$accountID-dayone/hudi-ds/config/ aws s3 cp json-deltastreamer_upsert.properties s3://mrworkshop-$accountID-dayone/hudi-ds/config/ Now let's generate some Fake data for the purpose of this workshop. We will use Faker library for that. Install Faker with the below command. pip3 install Faker pip3 install boto3 Run the Python program to generate Fake data under respective S3 locations. This takes a few minutes to complete. python3 apache-hudi-on-amazon-emr-deltastreamer-python-demo.py Once done, make sure the inputdata and update prefixes are populated with JSON data files. You can copy one file using \u201caws s3 cp\u201d on the EMR leader node session to inspect the data. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) aws s3 ls s3://mrworkshop-$accountID-dayone/hudi-ds/inputdata aws s3 ls s3://mrworkshop-$accountID-dayone/hudi-ds/updates Copy the Hudi utilities bundle to HDFS. hadoop fs -copyFromLocal /usr/lib/hudi/hudi-utilities-bundle.jar hdfs:///user/hadoop/ Let's submit DeltaStreamer step to the EMR cluster. You can submit this step on EC2 JumpHost or leader node of EMR cluster \"EMR-Spark-Hive-Presto\". Since we have the EMR leader node session active, let us use it to run the below scommand. accountID = $ ( aws sts get - caller - identity -- query \"Account\" -- output text ) cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) aws emr add - steps -- cluster - id $cluster_id -- steps Type = Spark , Name = \"Deltastreamer COW - Bulk Insert\" , ActionOnFailure = CONTINUE , Args = [ -- jars , hdfs :/// user / hadoop /*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///user/hadoop/hudi-utilities-bundle.jar,--props,s3://mrworkshop-$accountID-dayone/hudi-ds/config/json-deltastreamer.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://mrworkshop-$accountID-dayone/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,BULK_INSERT] --region us-east-1 You will get an EMR Step ID in return. You will see the corresponding Hudi Deltastreamer step being submitted to your cluster ( EMR Web Console (Right click -> Open Link in New Tab) -> EMR-Spark-Hive-Presto -> Steps). It will take about 2 minutes to complete. Check the S3 location for Hudi files. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) aws s3 ls s3://mrworkshop-$accountID-dayone/hudi-ds-output/person-profile-out1/ Let's go to the hive CLI on EMR leader node by typing \"hive\". Let's run the following command to create a table. Replace \"youraccountID\" with event engine AWS account ID. CREATE EXTERNAL TABLE `profile_cow` ( `_hoodie_commit_time` string , `_hoodie_commit_seqno` string , `_hoodie_record_key` string , `_hoodie_partition_path` string , `_hoodie_file_name` string , `Name` string , `phone` string , `job` string , `company` string , `ssn` string , `street_address` string , `dob` string , `email` string , `ts` string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 's3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1/' ; Select a record from this table and copy the value of hoodie_record_key and street_address to a notepad. select `_hoodie_commit_time` , `_hoodie_record_key` , street_address from profile_cow limit 1 ; Exit from hive. exit ; Now, let's do upsert operation with Hudi Deltastreamer by running the following command. accountID = $ ( aws sts get - caller - identity -- query \"Account\" -- output text ) cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) aws emr add - steps -- cluster - id $cluster_id -- steps Type = Spark , Name = \"Deltastreamer COW - Upsert\" , ActionOnFailure = CONTINUE , Args = [ -- jars , hdfs :/// user / hadoop /*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///user/hadoop/hudi-utilities-bundle.jar,--props,s3://mrworkshop-$accountID-dayone/hudi-ds/config/json-deltastreamer_upsert.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://mrworkshop-$accountID-dayone/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,UPSERT] --region us-east-1 You will get an EMR Step ID in return. You will see the corresponding Hudi Deltastreamer step being submitted to your cluster ( EMR Web Console (Right click -> Open Link in New Tab) -> EMR-Spark-Hive-Presto -> Steps). Wait for the step to complete (~1 minute). Let us check the street_address for the same _hoodie_record_key. Run the following query in hive CLI on the EMR leader node. Replace value of \"_hoodie_record_key\" in the where clause with the one you obtained from previous select query. select `_hoodie_commit_time` , street_address from profile_cow where `_hoodie_record_key` = '00000b94-1500-4f10-bd10-d6393ba24643' ; Notice the change in commit time and street_address. Change Data Capture with Hudi Deltastreamer \u00b6 Go to Amazon RDS Web console (Right click -> Open Link in New Tab) and open the database that was created. Copy the endpoint of this database. Login to your EC2 JumpHost using Session Manager or SSH and run the following command to connect to your DB. sudo su ec2 - user cd ~ dbendpoint = $ ( aws rds describe - db - instances -- region us - east - 1 | jq - r .' DBInstances [] | . Endpoint . Address ') sudo yum install - y mysql mysql - h $dbendpoint - uadmin - pTest123 $ Once you are logged in to your database, run the following commands in the MySQL session to create a DB table. call mysql . rds_set_configuration ( 'binlog retention hours' , 24 ); create table dev . retail_transactions ( tran_id INT , tran_date DATE , store_id INT , store_city varchar ( 50 ), store_state char ( 2 ), item_code varchar ( 50 ), quantity INT , total FLOAT ); Once the table is created, run the below queries to insert data into this table. insert into dev.retail_transactions values(1,'2019-03-17',1,'CHICAGO','IL','XXXXXX',5,106.25); insert into dev.retail_transactions values(2,'2019-03-16',2,'NEW YORK','NY','XXXXXX',6,116.25); insert into dev.retail_transactions values(3,'2019-03-15',3,'SPRINGFIELD','IL','XXXXXX',7,126.25); insert into dev.retail_transactions values(4,'2019-03-17',4,'SAN FRANCISCO','CA','XXXXXX',8,136.25); insert into dev.retail_transactions values(5,'2019-03-11',1,'CHICAGO','IL','XXXXXX',9,146.25); insert into dev.retail_transactions values(6,'2019-03-18',1,'CHICAGO','IL','XXXXXX',10,156.25); insert into dev.retail_transactions values(7,'2019-03-14',2,'NEW YORK','NY','XXXXXX',11,166.25); insert into dev.retail_transactions values(8,'2019-03-11',1,'CHICAGO','IL','XXXXXX',12,176.25); insert into dev.retail_transactions values(9,'2019-03-10',4,'SAN FRANCISCO','CA','XXXXXX',13,186.25); insert into dev.retail_transactions values(10,'2019-03-13',1,'CHICAGO','IL','XXXXXX',14,196.25); insert into dev.retail_transactions values(11,'2019-03-14',5,'CHICAGO','IL','XXXXXX',15,106.25); insert into dev.retail_transactions values(12,'2019-03-15',6,'CHICAGO','IL','XXXXXX',16,116.25); insert into dev.retail_transactions values(13,'2019-03-16',7,'CHICAGO','IL','XXXXXX',17,126.25); insert into dev.retail_transactions values(14,'2019-03-16',7,'CHICAGO','IL','XXXXXX',17,126.25); commit; We will now use AWS DMS to start pushing this data to S3. Go to the Amazon DMS Web Console (Right click -> Open Link in New Tab) -> Endpoints -> hudidmsource. Check if the connection is successful. If not, test the connection again. Start the Database migration task hudiload. Once the task state changes from Running to \"Load complete, replication ongoing\", check the below S3 location for deposited files. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) aws s3 ls s3://mrworkshop-dms-$accountID-dayone/dmsdata/dev/retail_transactions/ Now login to the EMR leader node of the cluster \"EMR-Spark-Hive-Presto\" using Session Manager or SSH and run the following commands. sudo su hadoop cd ~ accountID = $ ( aws sts get - caller - identity -- query \" Account \" -- output text ) aws s3 mv s3 : // mrworkshop - dms - $acc ountID - dayone / dmsdata / dev / retail_transactions / s3 : // mrworkshop - dms - $acc ountID - dayone / dmsdata / data - full / dev / retail_transactions / -- exclude \" * \" -- include \" LOAD*.parquet \" -- recursive With the full table dump available in the data-full S3 folder, we will now use the Hudi Deltastreamer utility on the EMR cluster to populate the Hudi dataset on S3. Run the following command directly on leader node. accountID =$ ( aws sts get - caller - identity -- query \"Account\" -- output text ) spark - submit -- class org . apache . hudi . utilities . deltastreamer . HoodieDeltaStreamer \\ -- jars hdfs : /// user / hadoop /*. jar \\ -- master yarn -- deploy - mode client \\ -- conf spark . serializer = org . apache . spark . serializer . KryoSerializer \\ -- conf spark . sql . hive . convertMetastoreParquet = false \\ / usr / lib / hudi / hudi - utilities - bundle . jar \\ -- table - type COPY_ON_WRITE \\ -- source - ordering - field dms_received_ts \\ -- props s3 : // mrworkshop - dms -$ accountID - dayone / properties / dfs - source - retail - transactions - full . properties \\ -- source - class org . apache . hudi . utilities . sources . ParquetDFSSource \\ -- target - base - path s3 : // mrworkshop - dms -$ accountID - dayone / hudi / retail_transactions -- target - table hudiblogdb . retail_transactions \\ -- transformer - class org . apache . hudi . utilities . transform . SqlQueryBasedTransformer \\ -- payload - class org . apache . hudi . common . model . AWSDmsAvroPayload \\ -- schemaprovider - class org . apache . hudi . utilities . schema . FilebasedSchemaProvider \\ -- enable - hive - sync Once this job completes, check the Hudi table by logging into Spark SQL or Athena console. If using SparkSQL, run the following command on the same EMR leader node session. spark-sql --conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\" --conf \"spark.sql.hive.convertMetastoreParquet=false\" --jars hdfs:///user/hadoop/*.jar To set up Athena, go to the Athena Web Console (Right click -> Open Link in New Tab) -> Explore the query editor. Since this would be your first time using Athena console, you need to go to the Settings -> Manage and add your Query result location like -> s3://mrworkshop-youraccountID-dayone/athena/. OR you can run the following command to set up Athena Query Output location using EC2 JumpHost Session Manager session. sudo su ec2 - user cd ~ accountID = $ ( aws sts get - caller - identity -- query \"Account\" -- output text ) aws athena update - work - group -- work - group primary -- configuration - updates \"{ \\\" ResultConfigurationUpdates \\\" : { \\\" OutputLocation \\\" : \\\" s3://mrworkshop-$accountID-dayone/athena/ \\\" }}\" -- region us - east - 1 Run the query in SparkSQL session or Athena console (Right click -> Open Link in New Tab): select * from hudiblogdb.retail_transactions order by tran_id You should see the same data in the table as the MySQL database with a few columns added by Hudi deltastreamer. Now let's run some DML statements on our MySQL database and take these changes through to the Hudi dataset. Run the following commands in MySQL session from your EC2 JumpHost. insert into dev.retail_transactions values(15,'2022-03-22',7,'CHICAGO','IL','XXXXXX',17,126.25); update dev.retail_transactions set store_city='SPRINGFIELD' where tran_id=12; delete from dev.retail_transactions where tran_id=2; commit; Exit from the MySQL session. In a few minutes, you see a new .parquet file created under s3://mrworkshop-dms-youraccountID-dayone/dmsdata/dev/retail_transactions/ folder in the S3 bucket. CDC data is being captured by our DMS replication task. You can see the changes in the DMS replication task under \"Table statistics\". Now, lets take the incremental changes we made to Hudi. Run the following command on EMR leader node. accountID =$ ( aws sts get - caller - identity -- query \"Account\" -- output text ) spark - submit -- class org . apache . hudi . utilities . deltastreamer . HoodieDeltaStreamer \\ -- jars hdfs : /// user / hadoop /*. jar \\ -- master yarn -- deploy - mode client \\ -- conf spark . serializer = org . apache . spark . serializer . KryoSerializer \\ -- conf spark . sql . hive . convertMetastoreParquet = false \\ / usr / lib / hudi / hudi - utilities - bundle . jar \\ -- table - type COPY_ON_WRITE \\ -- source - ordering - field dms_received_ts \\ -- props s3 : // mrworkshop - dms -$ accountID - dayone / properties / dfs - source - retail - transactions - incremental . properties \\ -- source - class org . apache . hudi . utilities . sources . ParquetDFSSource \\ -- target - base - path s3 : // mrworkshop - dms -$ accountID - dayone / hudi / retail_transactions -- target - table hudiblogdb . retail_transactions \\ -- transformer - class org . apache . hudi . utilities . transform . SqlQueryBasedTransformer \\ -- payload - class org . apache . hudi . common . model . AWSDmsAvroPayload \\ -- schemaprovider - class org . apache . hudi . utilities . schema . FilebasedSchemaProvider \\ -- enable - hive - sync \\ -- checkpoint 0 Once finished, check the Hudi table by logging into Spark SQL or Athena console (Right click -> Open Link in New Tab). If using SparkSQL, run the following command on the same EMR leader node session. spark-sql --conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\" --conf \"spark.sql.hive.convertMetastoreParquet=false\" --jars hdfs:///user/hadoop/*.jar Run the query: select * from hudiblogdb.retail_transactions order by tran_id You should see the changes you made in the MySQL table. Apache Hudi with Spark Structured Streaming \u00b6 This exercise will show how you can write real time Hudi data sets using Spark Structured Streaming. For this exercise, we will use real-time NYC Metro Subway data using MTA API . Keep the EMR Session Manager or SSH session active. In a new browser tab, create a new SSM session for EC2 instance \"JumpHost\" (or SSH into EC2 instance \"JumpHost\"). i.e., under the EC2 Web Console (Right click -> Open Link in New Tab), select the EC2 instance with name \"JumpHost\". Click on \"Connect\" -> Session Manager -> Connect. Switch to EC2 user and go to home directory sudo su ec2-user cd ~ Run the following commands in EC2 instance to get the values of ZookeeperConnectString and BootstrapBrokerString. clusterArn=`aws kafka list-clusters --region us-east-1 | jq '.ClusterInfoList[0].ClusterArn'` echo $clusterArn bs=$(echo \"aws kafka get-bootstrap-brokers --cluster-arn ${ clusterArn } --region us-east-1\" | bash | jq '.BootstrapBrokerString') bs_update=$(echo $bs | sed \"s|,|\\',\\'|g\" | sed \"s|\\\"|'|g\") zs=$(echo \"aws kafka describe-cluster --cluster-arn $clusterArn \" --region us-east-1 | bash | jq '.ClusterInfo.ZookeeperConnectString') Create two Kafka topics. echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-topics.sh --create --zookeeper $zs --replication-factor 3 --partitions 1 --topic trip_update_topic\" | bash echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-topics.sh --create --zookeeper $zs --replication-factor 3 --partitions 1 --topic trip_status_topic\" | bash Install packages required by Kafka client on the JumpHost instance session (via SSH or AWS SSM). pip3 install protobuf pip3 install kafka-python pip3 install --upgrade gtfs-realtime-bindings pip3 install underground pip3 install pathlib pip3 install requests Run the below command to modify the bootstrap servers in the file train_arrival_producer.py on the JumpHost's /home/ec2-user/ directory. Change sudo sed -i \"s|'bootstrapserverstring'|$bs_update|g\" /home/ec2-user/train_arrival_producer.py Export API key on the same session. export MTA_API_KEY = UskS0iAsK06DtSffbgqNi8hlDvApPR833wydQAHG Run the Kafka producer client and terminate the process using Ctrl + C after 10 seconds. python3 train_arrival_producer.py You can verify that the Kafka topics are being written to using the following commands echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-console-consumer.sh --bootstrap-server $bs --topic trip_update_topic --from-beginning\" | bash After a few seconds exit using Ctrl + C. echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-console-consumer.sh --bootstrap-server $bs --topic trip_status_topic --from-beginning\" | bash After a few seconds exit using Ctrl + C. Now let's configure Spark consumer on EMR leader node using Session Manager or SSH. 9). SSH into the leader node of EMR cluster \"EMR-Spark-Hive-Presto\" (or use AWS Session Manager). Download Spark dependencies in EMR leader node session. sudo su hadoop cd ~ cd /usr/lib/spark/jars sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.12/3.0.1/spark-streaming-kafka-0-10_2.12-3.0.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.1/spark-sql-kafka-0-10_2.12-3.0.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.2.1/kafka-clients-2.2.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.12/3.0.1/spark-streaming-kafka-0-10-assembly_2.12-3.0.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar In same session, provide all access to all HDFS folders. You can scope access per user if desired. hdfs dfs -chmod 777 / Switch to the SSH/SSM session of EC2 instance \u201cJumpHost\u201d. Get the bootstrap string from the below command on the EC2 JumpHost session. echo $bs Example output: \"b-3.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092,b-1.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092,b-2.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092\" Copy your bootstrap servers output to a notepad. Run the Kafka producer program again and keep it running. python3 train_arrival_producer.py Now, go to the EMR Studio's JupyterLab workspace and open workshop-repo/files/notebook/amazon-emr-spark-streaming-apache-hudi-demo.ipynb. Make sure that \"Spark\" kernel is selected. Run all the cell blocks. Spark streaming job runs every 30 seconds. You can increase the duration if you want to. Based on the time of the day you run this code, the results may vary. After sometime, query this table using Hive or Athena. You can run the following queries in Athena Web Console (Right click -> Open Link in New Tab) once in every 2 minutes or so to see the live changes. You can also build live dashboards using Amazon Quicksight. select count(*) from hudi_trips_streaming_table; select tripId, numoffuturestops from hudi_trips_streaming_table; After inspecting, stop your Python Kafka producer on the EC2 JumpHost session using Ctrl + C. On EMR Studio workspace session, click on the stop icon to stop the Spark Structured Streaming job.","title":"1 - Apache Hudi on Amazon EMR"},{"location":"day2/hudi/exercise/#apache-hudi-on-amazon-emr","text":"In this exercise you will build incremental data lakes on EMR using Apache Hudi. You can build data lakes using Apache Hudi using Spark Datasource APIs, Hudi Deltastreamer utility and SparkSQL. You will also build a real-time live incremental data lake with Spark Structured Streaming + Amazon Managed Streaming for Apache Kafka (MSK) + Apache Hudi. In the previous EMR Studio exercise, we linked the Git repository in the Jupyter interface. We will continue to use the same repository to run these exercises. SSH into the EMR leader node of the cluster \"EMR-Spark-Hive-Presto\" or open a session using AWS Session Manager for the EMR leader node since we will be running a few commands directly on the leader node.","title":"Apache Hudi on Amazon EMR"},{"location":"day2/hudi/exercise/#apache-hudi-with-spark-datasource-apis","text":"Open the file workshop-repo -> files -> notebook -> apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb in the Jupyter. Make sure the Kernel is set to PySpark. All the instructions required to run the notebook are within the notebook itself. Download the file workshop-repo -> schema -> schema.avsc to your local desktop and upload this file into the following S3 location (replace \"youraccountID\" with your event engine AWS account ID): s3://mrworkshop-youraccountID-dayone/schema/schema.avsc Alternatively, you can run the following commands from the leader node of your EMR cluster. We will be using this schema AVRO file to run compaction on Merge-On-Read tables. sudo su hadoop cd ~ accountID=$(aws sts get-caller-identity --query \"Account\" --output text) curl -o schema.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/schema.avsc aws s3 cp schema.avsc s3://mrworkshop-$accountID-dayone/schema/schema.avsc Run the blocks of the notebook \"apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb\".","title":"Apache Hudi with Spark Datasource APIs"},{"location":"day2/hudi/exercise/#apache-hudi-with-sparksql-dmls","text":"From EMR 6.5.0, you can write Hudi datasets using simple SQL statements. Let's look at an example. From the EMR Studio workspace Jupyterlab session, go to workshop-repo -> files -> notebook -> apache-hudi-on-amazon-emr-dml.ipynb. Run all the blocks of this notebook. Detailed instructions are within the notebook.","title":"Apache Hudi with SparkSQL DMLs"},{"location":"day2/hudi/exercise/#apache-hudi-with-spark-deltastreamer","text":"Hudi provides a utility called Deltastreamer for creating and manipulating Hudi datasets without the need to write any Spark code. For this activity, let us copy a few files to the S3 location. Run the following commands in your EMR leader node session created using Session Manager or SSH. sudo su hadoop cd ~ accountID=$(aws sts get-caller-identity --query \"Account\" --output text) curl -o source-schema-json.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/source-schema-json.avsc curl -o target-schema-json.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/target-schema-json.avsc curl -o json-deltastreamer.properties https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/properties/json-deltastreamer.properties curl -o json-deltastreamer_upsert.properties https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/properties/json-deltastreamer_upsert.properties curl -o apache-hudi-on-amazon-emr-deltastreamer-python-demo.py https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/script/apache-hudi-on-amazon-emr-deltastreamer-python-demo.py Replace event engine AWS account ID in the files json-deltastreamer.properties, json-deltastreamer_upsert.properties and apache-hudi-on-amazon-emr-deltastreamer-python-demo.py. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) sed -i \"s|youraccountID|$accountID|g\" json-deltastreamer.properties sed -i \"s|youraccountID|$accountID|g\" json-deltastreamer_upsert.properties sed -i \"s|youraccountID|$accountID|g\" apache-hudi-on-amazon-emr-deltastreamer-python-demo.py Now, copy the four files to your S3 location. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) aws s3 cp source-schema-json.avsc s3://mrworkshop-$accountID-dayone/hudi-ds/config/ aws s3 cp target-schema-json.avsc s3://mrworkshop-$accountID-dayone/hudi-ds/config/ aws s3 cp json-deltastreamer.properties s3://mrworkshop-$accountID-dayone/hudi-ds/config/ aws s3 cp json-deltastreamer_upsert.properties s3://mrworkshop-$accountID-dayone/hudi-ds/config/ Now let's generate some Fake data for the purpose of this workshop. We will use Faker library for that. Install Faker with the below command. pip3 install Faker pip3 install boto3 Run the Python program to generate Fake data under respective S3 locations. This takes a few minutes to complete. python3 apache-hudi-on-amazon-emr-deltastreamer-python-demo.py Once done, make sure the inputdata and update prefixes are populated with JSON data files. You can copy one file using \u201caws s3 cp\u201d on the EMR leader node session to inspect the data. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) aws s3 ls s3://mrworkshop-$accountID-dayone/hudi-ds/inputdata aws s3 ls s3://mrworkshop-$accountID-dayone/hudi-ds/updates Copy the Hudi utilities bundle to HDFS. hadoop fs -copyFromLocal /usr/lib/hudi/hudi-utilities-bundle.jar hdfs:///user/hadoop/ Let's submit DeltaStreamer step to the EMR cluster. You can submit this step on EC2 JumpHost or leader node of EMR cluster \"EMR-Spark-Hive-Presto\". Since we have the EMR leader node session active, let us use it to run the below scommand. accountID = $ ( aws sts get - caller - identity -- query \"Account\" -- output text ) cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) aws emr add - steps -- cluster - id $cluster_id -- steps Type = Spark , Name = \"Deltastreamer COW - Bulk Insert\" , ActionOnFailure = CONTINUE , Args = [ -- jars , hdfs :/// user / hadoop /*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///user/hadoop/hudi-utilities-bundle.jar,--props,s3://mrworkshop-$accountID-dayone/hudi-ds/config/json-deltastreamer.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://mrworkshop-$accountID-dayone/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,BULK_INSERT] --region us-east-1 You will get an EMR Step ID in return. You will see the corresponding Hudi Deltastreamer step being submitted to your cluster ( EMR Web Console (Right click -> Open Link in New Tab) -> EMR-Spark-Hive-Presto -> Steps). It will take about 2 minutes to complete. Check the S3 location for Hudi files. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) aws s3 ls s3://mrworkshop-$accountID-dayone/hudi-ds-output/person-profile-out1/ Let's go to the hive CLI on EMR leader node by typing \"hive\". Let's run the following command to create a table. Replace \"youraccountID\" with event engine AWS account ID. CREATE EXTERNAL TABLE `profile_cow` ( `_hoodie_commit_time` string , `_hoodie_commit_seqno` string , `_hoodie_record_key` string , `_hoodie_partition_path` string , `_hoodie_file_name` string , `Name` string , `phone` string , `job` string , `company` string , `ssn` string , `street_address` string , `dob` string , `email` string , `ts` string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 's3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1/' ; Select a record from this table and copy the value of hoodie_record_key and street_address to a notepad. select `_hoodie_commit_time` , `_hoodie_record_key` , street_address from profile_cow limit 1 ; Exit from hive. exit ; Now, let's do upsert operation with Hudi Deltastreamer by running the following command. accountID = $ ( aws sts get - caller - identity -- query \"Account\" -- output text ) cluster_id = $ ( aws emr list - clusters -- region us - east - 1 -- query 'Clusters[?Name==`EMR-Spark-Hive-Presto` && Status.State!=`TERMINATED`]' . { Clusters : Id } -- output text ) aws emr add - steps -- cluster - id $cluster_id -- steps Type = Spark , Name = \"Deltastreamer COW - Upsert\" , ActionOnFailure = CONTINUE , Args = [ -- jars , hdfs :/// user / hadoop /*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///user/hadoop/hudi-utilities-bundle.jar,--props,s3://mrworkshop-$accountID-dayone/hudi-ds/config/json-deltastreamer_upsert.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://mrworkshop-$accountID-dayone/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,UPSERT] --region us-east-1 You will get an EMR Step ID in return. You will see the corresponding Hudi Deltastreamer step being submitted to your cluster ( EMR Web Console (Right click -> Open Link in New Tab) -> EMR-Spark-Hive-Presto -> Steps). Wait for the step to complete (~1 minute). Let us check the street_address for the same _hoodie_record_key. Run the following query in hive CLI on the EMR leader node. Replace value of \"_hoodie_record_key\" in the where clause with the one you obtained from previous select query. select `_hoodie_commit_time` , street_address from profile_cow where `_hoodie_record_key` = '00000b94-1500-4f10-bd10-d6393ba24643' ; Notice the change in commit time and street_address.","title":"Apache Hudi with Spark Deltastreamer"},{"location":"day2/hudi/exercise/#change-data-capture-with-hudi-deltastreamer","text":"Go to Amazon RDS Web console (Right click -> Open Link in New Tab) and open the database that was created. Copy the endpoint of this database. Login to your EC2 JumpHost using Session Manager or SSH and run the following command to connect to your DB. sudo su ec2 - user cd ~ dbendpoint = $ ( aws rds describe - db - instances -- region us - east - 1 | jq - r .' DBInstances [] | . Endpoint . Address ') sudo yum install - y mysql mysql - h $dbendpoint - uadmin - pTest123 $ Once you are logged in to your database, run the following commands in the MySQL session to create a DB table. call mysql . rds_set_configuration ( 'binlog retention hours' , 24 ); create table dev . retail_transactions ( tran_id INT , tran_date DATE , store_id INT , store_city varchar ( 50 ), store_state char ( 2 ), item_code varchar ( 50 ), quantity INT , total FLOAT ); Once the table is created, run the below queries to insert data into this table. insert into dev.retail_transactions values(1,'2019-03-17',1,'CHICAGO','IL','XXXXXX',5,106.25); insert into dev.retail_transactions values(2,'2019-03-16',2,'NEW YORK','NY','XXXXXX',6,116.25); insert into dev.retail_transactions values(3,'2019-03-15',3,'SPRINGFIELD','IL','XXXXXX',7,126.25); insert into dev.retail_transactions values(4,'2019-03-17',4,'SAN FRANCISCO','CA','XXXXXX',8,136.25); insert into dev.retail_transactions values(5,'2019-03-11',1,'CHICAGO','IL','XXXXXX',9,146.25); insert into dev.retail_transactions values(6,'2019-03-18',1,'CHICAGO','IL','XXXXXX',10,156.25); insert into dev.retail_transactions values(7,'2019-03-14',2,'NEW YORK','NY','XXXXXX',11,166.25); insert into dev.retail_transactions values(8,'2019-03-11',1,'CHICAGO','IL','XXXXXX',12,176.25); insert into dev.retail_transactions values(9,'2019-03-10',4,'SAN FRANCISCO','CA','XXXXXX',13,186.25); insert into dev.retail_transactions values(10,'2019-03-13',1,'CHICAGO','IL','XXXXXX',14,196.25); insert into dev.retail_transactions values(11,'2019-03-14',5,'CHICAGO','IL','XXXXXX',15,106.25); insert into dev.retail_transactions values(12,'2019-03-15',6,'CHICAGO','IL','XXXXXX',16,116.25); insert into dev.retail_transactions values(13,'2019-03-16',7,'CHICAGO','IL','XXXXXX',17,126.25); insert into dev.retail_transactions values(14,'2019-03-16',7,'CHICAGO','IL','XXXXXX',17,126.25); commit; We will now use AWS DMS to start pushing this data to S3. Go to the Amazon DMS Web Console (Right click -> Open Link in New Tab) -> Endpoints -> hudidmsource. Check if the connection is successful. If not, test the connection again. Start the Database migration task hudiload. Once the task state changes from Running to \"Load complete, replication ongoing\", check the below S3 location for deposited files. accountID=$(aws sts get-caller-identity --query \"Account\" --output text) aws s3 ls s3://mrworkshop-dms-$accountID-dayone/dmsdata/dev/retail_transactions/ Now login to the EMR leader node of the cluster \"EMR-Spark-Hive-Presto\" using Session Manager or SSH and run the following commands. sudo su hadoop cd ~ accountID = $ ( aws sts get - caller - identity -- query \" Account \" -- output text ) aws s3 mv s3 : // mrworkshop - dms - $acc ountID - dayone / dmsdata / dev / retail_transactions / s3 : // mrworkshop - dms - $acc ountID - dayone / dmsdata / data - full / dev / retail_transactions / -- exclude \" * \" -- include \" LOAD*.parquet \" -- recursive With the full table dump available in the data-full S3 folder, we will now use the Hudi Deltastreamer utility on the EMR cluster to populate the Hudi dataset on S3. Run the following command directly on leader node. accountID =$ ( aws sts get - caller - identity -- query \"Account\" -- output text ) spark - submit -- class org . apache . hudi . utilities . deltastreamer . HoodieDeltaStreamer \\ -- jars hdfs : /// user / hadoop /*. jar \\ -- master yarn -- deploy - mode client \\ -- conf spark . serializer = org . apache . spark . serializer . KryoSerializer \\ -- conf spark . sql . hive . convertMetastoreParquet = false \\ / usr / lib / hudi / hudi - utilities - bundle . jar \\ -- table - type COPY_ON_WRITE \\ -- source - ordering - field dms_received_ts \\ -- props s3 : // mrworkshop - dms -$ accountID - dayone / properties / dfs - source - retail - transactions - full . properties \\ -- source - class org . apache . hudi . utilities . sources . ParquetDFSSource \\ -- target - base - path s3 : // mrworkshop - dms -$ accountID - dayone / hudi / retail_transactions -- target - table hudiblogdb . retail_transactions \\ -- transformer - class org . apache . hudi . utilities . transform . SqlQueryBasedTransformer \\ -- payload - class org . apache . hudi . common . model . AWSDmsAvroPayload \\ -- schemaprovider - class org . apache . hudi . utilities . schema . FilebasedSchemaProvider \\ -- enable - hive - sync Once this job completes, check the Hudi table by logging into Spark SQL or Athena console. If using SparkSQL, run the following command on the same EMR leader node session. spark-sql --conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\" --conf \"spark.sql.hive.convertMetastoreParquet=false\" --jars hdfs:///user/hadoop/*.jar To set up Athena, go to the Athena Web Console (Right click -> Open Link in New Tab) -> Explore the query editor. Since this would be your first time using Athena console, you need to go to the Settings -> Manage and add your Query result location like -> s3://mrworkshop-youraccountID-dayone/athena/. OR you can run the following command to set up Athena Query Output location using EC2 JumpHost Session Manager session. sudo su ec2 - user cd ~ accountID = $ ( aws sts get - caller - identity -- query \"Account\" -- output text ) aws athena update - work - group -- work - group primary -- configuration - updates \"{ \\\" ResultConfigurationUpdates \\\" : { \\\" OutputLocation \\\" : \\\" s3://mrworkshop-$accountID-dayone/athena/ \\\" }}\" -- region us - east - 1 Run the query in SparkSQL session or Athena console (Right click -> Open Link in New Tab): select * from hudiblogdb.retail_transactions order by tran_id You should see the same data in the table as the MySQL database with a few columns added by Hudi deltastreamer. Now let's run some DML statements on our MySQL database and take these changes through to the Hudi dataset. Run the following commands in MySQL session from your EC2 JumpHost. insert into dev.retail_transactions values(15,'2022-03-22',7,'CHICAGO','IL','XXXXXX',17,126.25); update dev.retail_transactions set store_city='SPRINGFIELD' where tran_id=12; delete from dev.retail_transactions where tran_id=2; commit; Exit from the MySQL session. In a few minutes, you see a new .parquet file created under s3://mrworkshop-dms-youraccountID-dayone/dmsdata/dev/retail_transactions/ folder in the S3 bucket. CDC data is being captured by our DMS replication task. You can see the changes in the DMS replication task under \"Table statistics\". Now, lets take the incremental changes we made to Hudi. Run the following command on EMR leader node. accountID =$ ( aws sts get - caller - identity -- query \"Account\" -- output text ) spark - submit -- class org . apache . hudi . utilities . deltastreamer . HoodieDeltaStreamer \\ -- jars hdfs : /// user / hadoop /*. jar \\ -- master yarn -- deploy - mode client \\ -- conf spark . serializer = org . apache . spark . serializer . KryoSerializer \\ -- conf spark . sql . hive . convertMetastoreParquet = false \\ / usr / lib / hudi / hudi - utilities - bundle . jar \\ -- table - type COPY_ON_WRITE \\ -- source - ordering - field dms_received_ts \\ -- props s3 : // mrworkshop - dms -$ accountID - dayone / properties / dfs - source - retail - transactions - incremental . properties \\ -- source - class org . apache . hudi . utilities . sources . ParquetDFSSource \\ -- target - base - path s3 : // mrworkshop - dms -$ accountID - dayone / hudi / retail_transactions -- target - table hudiblogdb . retail_transactions \\ -- transformer - class org . apache . hudi . utilities . transform . SqlQueryBasedTransformer \\ -- payload - class org . apache . hudi . common . model . AWSDmsAvroPayload \\ -- schemaprovider - class org . apache . hudi . utilities . schema . FilebasedSchemaProvider \\ -- enable - hive - sync \\ -- checkpoint 0 Once finished, check the Hudi table by logging into Spark SQL or Athena console (Right click -> Open Link in New Tab). If using SparkSQL, run the following command on the same EMR leader node session. spark-sql --conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\" --conf \"spark.sql.hive.convertMetastoreParquet=false\" --jars hdfs:///user/hadoop/*.jar Run the query: select * from hudiblogdb.retail_transactions order by tran_id You should see the changes you made in the MySQL table.","title":"Change Data Capture with Hudi Deltastreamer"},{"location":"day2/hudi/exercise/#apache-hudi-with-spark-structured-streaming","text":"This exercise will show how you can write real time Hudi data sets using Spark Structured Streaming. For this exercise, we will use real-time NYC Metro Subway data using MTA API . Keep the EMR Session Manager or SSH session active. In a new browser tab, create a new SSM session for EC2 instance \"JumpHost\" (or SSH into EC2 instance \"JumpHost\"). i.e., under the EC2 Web Console (Right click -> Open Link in New Tab), select the EC2 instance with name \"JumpHost\". Click on \"Connect\" -> Session Manager -> Connect. Switch to EC2 user and go to home directory sudo su ec2-user cd ~ Run the following commands in EC2 instance to get the values of ZookeeperConnectString and BootstrapBrokerString. clusterArn=`aws kafka list-clusters --region us-east-1 | jq '.ClusterInfoList[0].ClusterArn'` echo $clusterArn bs=$(echo \"aws kafka get-bootstrap-brokers --cluster-arn ${ clusterArn } --region us-east-1\" | bash | jq '.BootstrapBrokerString') bs_update=$(echo $bs | sed \"s|,|\\',\\'|g\" | sed \"s|\\\"|'|g\") zs=$(echo \"aws kafka describe-cluster --cluster-arn $clusterArn \" --region us-east-1 | bash | jq '.ClusterInfo.ZookeeperConnectString') Create two Kafka topics. echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-topics.sh --create --zookeeper $zs --replication-factor 3 --partitions 1 --topic trip_update_topic\" | bash echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-topics.sh --create --zookeeper $zs --replication-factor 3 --partitions 1 --topic trip_status_topic\" | bash Install packages required by Kafka client on the JumpHost instance session (via SSH or AWS SSM). pip3 install protobuf pip3 install kafka-python pip3 install --upgrade gtfs-realtime-bindings pip3 install underground pip3 install pathlib pip3 install requests Run the below command to modify the bootstrap servers in the file train_arrival_producer.py on the JumpHost's /home/ec2-user/ directory. Change sudo sed -i \"s|'bootstrapserverstring'|$bs_update|g\" /home/ec2-user/train_arrival_producer.py Export API key on the same session. export MTA_API_KEY = UskS0iAsK06DtSffbgqNi8hlDvApPR833wydQAHG Run the Kafka producer client and terminate the process using Ctrl + C after 10 seconds. python3 train_arrival_producer.py You can verify that the Kafka topics are being written to using the following commands echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-console-consumer.sh --bootstrap-server $bs --topic trip_update_topic --from-beginning\" | bash After a few seconds exit using Ctrl + C. echo \"/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-console-consumer.sh --bootstrap-server $bs --topic trip_status_topic --from-beginning\" | bash After a few seconds exit using Ctrl + C. Now let's configure Spark consumer on EMR leader node using Session Manager or SSH. 9). SSH into the leader node of EMR cluster \"EMR-Spark-Hive-Presto\" (or use AWS Session Manager). Download Spark dependencies in EMR leader node session. sudo su hadoop cd ~ cd /usr/lib/spark/jars sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.12/3.0.1/spark-streaming-kafka-0-10_2.12-3.0.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.1/spark-sql-kafka-0-10_2.12-3.0.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.2.1/kafka-clients-2.2.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.12/3.0.1/spark-streaming-kafka-0-10-assembly_2.12-3.0.1.jar sudo wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar In same session, provide all access to all HDFS folders. You can scope access per user if desired. hdfs dfs -chmod 777 / Switch to the SSH/SSM session of EC2 instance \u201cJumpHost\u201d. Get the bootstrap string from the below command on the EC2 JumpHost session. echo $bs Example output: \"b-3.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092,b-1.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092,b-2.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092\" Copy your bootstrap servers output to a notepad. Run the Kafka producer program again and keep it running. python3 train_arrival_producer.py Now, go to the EMR Studio's JupyterLab workspace and open workshop-repo/files/notebook/amazon-emr-spark-streaming-apache-hudi-demo.ipynb. Make sure that \"Spark\" kernel is selected. Run all the cell blocks. Spark streaming job runs every 30 seconds. You can increase the duration if you want to. Based on the time of the day you run this code, the results may vary. After sometime, query this table using Hive or Athena. You can run the following queries in Athena Web Console (Right click -> Open Link in New Tab) once in every 2 minutes or so to see the live changes. You can also build live dashboards using Amazon Quicksight. select count(*) from hudi_trips_streaming_table; select tripId, numoffuturestops from hudi_trips_streaming_table; After inspecting, stop your Python Kafka producer on the EC2 JumpHost session using Ctrl + C. On EMR Studio workspace session, click on the stop icon to stop the Spark Structured Streaming job.","title":"Apache Hudi with Spark Structured Streaming"},{"location":"day2/icebrg/exercise/","text":"Apache Iceberg on Amazon EMR \u00b6 In this exercise you will build incremental data lakes on EMR using Apache Iceberg. You will learn about the most important features of Iceberg like schema evolution, time traveling and also S3 traffic scaling using Object Store File Layout . Apache Iceberg Features \u00b6 From the EMR Studio workspace Jupyterlab session, go to workshop-repo -> files -> notebook -> apache-iceberg-on-amazon-emr.ipynb. Run all the blocks of this notebook. Detailed instructions are within the notebook. Stop the session and restart kernel once you are done.","title":"2 - Apache Iceberg on Amazon EMR"},{"location":"day2/icebrg/exercise/#apache-iceberg-on-amazon-emr","text":"In this exercise you will build incremental data lakes on EMR using Apache Iceberg. You will learn about the most important features of Iceberg like schema evolution, time traveling and also S3 traffic scaling using Object Store File Layout .","title":"Apache Iceberg on Amazon EMR"},{"location":"day2/icebrg/exercise/#apache-iceberg-features","text":"From the EMR Studio workspace Jupyterlab session, go to workshop-repo -> files -> notebook -> apache-iceberg-on-amazon-emr.ipynb. Run all the blocks of this notebook. Detailed instructions are within the notebook. Stop the session and restart kernel once you are done.","title":"Apache Iceberg Features"},{"location":"day2/smstudio/exercise/","text":"Sagemaker Studio Integration with Amazon EMR \u00b6 Amazon Sagemaker provides native integration with Amazon EMR from Sagemaker Studio so that you can run data preparation tasks for your machine learning workloads using EMR from Sagemaker. Login to the Sagemaker Studio \u00b6 Go to the Amazon Sagemaker Web Console -> Get started -> Sagemaker Studio You should see the Sagemaker domain ready status. Launch the Sagemaker Studio from Launch app -> Studio. It will take about 2 minutes to initialize after which you will be taken to the Sagemaker Studio interface. Once you are in, carry on with rest of the steps. Create EMR Cluster from Sagemaker Studio \u00b6 Click on the icon and choose Clusters from the Sagemaker resources drop down. You will be able to see the EMR clusters. You can filter the EMR clusters. Now, create a new EMR cluster with a cluster template created from AWS Service Catalog. Go to Clusters -> Create cluster. You will be able to see a template. When you click on the template and click \"Select Template\", it will show you the blueprint for your EMR cluster creation. Enter a cluster name (for eg: \"Sagemaker Studio Cluster\"). You can keep everything else as default. Click on \"Create Cluster\". You can go to the AWS CloudFormation Web Console (Right click -> Open Link in New Tab). You will be able to see the stack launched by AWS Service Catalog from our Create Cluster action. Wait for the stack to complete and for the cluster \"Sagemaker Studio Cluster\" to get created. It will take about 15 mins. Once its complete, you will see the below message in your Sagemaker Studio. You will also see that an EMR cluster called \"Sagemaker Studio Cluster\" is created in the EMR Web Console (Right click -> Open Link in New Tab) Please note we are not using one of the existing EMR clusters because this Studio was created in a different VPC and we have not established peering between the two VPCs. Connect to EMR cluster from Sagemaker Studio and run data processing jobs \u00b6 Go to Git repository section and click on Clone the repository. Specify the repository to clone: https://github.com/vasveena/amazon-emr-ttt-workshop.git Make sure that the repository is cloned. Go to Files section (folder icon on the left hand side pane). Go to the directory files -> notebook -> smstudio-pyspark-hive-sentiment-analysis.ipynb. PySpark Kernel should be chosen automatically for this notebook. If promoted, choose the SparkMagic Kernel and click \"Select\". It will take a few minutes for the kernel to initialize. Once the kernel starts, go to Cluster on the top right corner and choose the EMR cluster \"Sagemaker Studio Cluster\". When it prompts for credential type, choose \"No credential\" and Connect. Once you connect, a Spark application will be created automatically. After the Spark application is created successfully and you have the YARN application ID for the Spark session like in the above screenshot, you can run the remaining code blocks of the notebook which will perform data transformations and explorations using the EMR cluster and create and host an ML model using Sagemaker.","title":"3 - Sagemaker Studio Integration"},{"location":"day2/smstudio/exercise/#sagemaker-studio-integration-with-amazon-emr","text":"Amazon Sagemaker provides native integration with Amazon EMR from Sagemaker Studio so that you can run data preparation tasks for your machine learning workloads using EMR from Sagemaker.","title":"Sagemaker Studio Integration with Amazon EMR"},{"location":"day2/smstudio/exercise/#login-to-the-sagemaker-studio","text":"Go to the Amazon Sagemaker Web Console -> Get started -> Sagemaker Studio You should see the Sagemaker domain ready status. Launch the Sagemaker Studio from Launch app -> Studio. It will take about 2 minutes to initialize after which you will be taken to the Sagemaker Studio interface. Once you are in, carry on with rest of the steps.","title":"Login to the Sagemaker Studio"},{"location":"day2/smstudio/exercise/#create-emr-cluster-from-sagemaker-studio","text":"Click on the icon and choose Clusters from the Sagemaker resources drop down. You will be able to see the EMR clusters. You can filter the EMR clusters. Now, create a new EMR cluster with a cluster template created from AWS Service Catalog. Go to Clusters -> Create cluster. You will be able to see a template. When you click on the template and click \"Select Template\", it will show you the blueprint for your EMR cluster creation. Enter a cluster name (for eg: \"Sagemaker Studio Cluster\"). You can keep everything else as default. Click on \"Create Cluster\". You can go to the AWS CloudFormation Web Console (Right click -> Open Link in New Tab). You will be able to see the stack launched by AWS Service Catalog from our Create Cluster action. Wait for the stack to complete and for the cluster \"Sagemaker Studio Cluster\" to get created. It will take about 15 mins. Once its complete, you will see the below message in your Sagemaker Studio. You will also see that an EMR cluster called \"Sagemaker Studio Cluster\" is created in the EMR Web Console (Right click -> Open Link in New Tab) Please note we are not using one of the existing EMR clusters because this Studio was created in a different VPC and we have not established peering between the two VPCs.","title":"Create EMR Cluster from Sagemaker Studio"},{"location":"day2/smstudio/exercise/#connect-to-emr-cluster-from-sagemaker-studio-and-run-data-processing-jobs","text":"Go to Git repository section and click on Clone the repository. Specify the repository to clone: https://github.com/vasveena/amazon-emr-ttt-workshop.git Make sure that the repository is cloned. Go to Files section (folder icon on the left hand side pane). Go to the directory files -> notebook -> smstudio-pyspark-hive-sentiment-analysis.ipynb. PySpark Kernel should be chosen automatically for this notebook. If promoted, choose the SparkMagic Kernel and click \"Select\". It will take a few minutes for the kernel to initialize. Once the kernel starts, go to Cluster on the top right corner and choose the EMR cluster \"Sagemaker Studio Cluster\". When it prompts for credential type, choose \"No credential\" and Connect. Once you connect, a Spark application will be created automatically. After the Spark application is created successfully and you have the YARN application ID for the Spark session like in the above screenshot, you can run the remaining code blocks of the notebook which will perform data transformations and explorations using the EMR cluster and create and host an ML model using Sagemaker.","title":"Connect to EMR cluster from Sagemaker Studio and run data processing jobs"},{"location":"day3/","text":"Amazon EMR Deployment Options \u00b6 Amazon EMR offers four deployment options to run your applications. Amazon EMR on EC2 Amazon EMR on EKS Amazon EMR on Outposts Amazon EMR Serverless (Preview) So far we leveraged EMR on EC2 deployment. In the following exercises, we will run the same workloads using EMR on EKS and EMR Serverless deployment options. Note that all the deployment options offer the same optimized runtime Spark engine version for every EMR release label. This means you can build your application code only once and then use any of these deployment modes interchangeably to run your applications.","title":"Introduction"},{"location":"day3/#amazon-emr-deployment-options","text":"Amazon EMR offers four deployment options to run your applications. Amazon EMR on EC2 Amazon EMR on EKS Amazon EMR on Outposts Amazon EMR Serverless (Preview) So far we leveraged EMR on EC2 deployment. In the following exercises, we will run the same workloads using EMR on EKS and EMR Serverless deployment options. Note that all the deployment options offer the same optimized runtime Spark engine version for every EMR release label. This means you can build your application code only once and then use any of these deployment modes interchangeably to run your applications.","title":"Amazon EMR Deployment Options"},{"location":"day3/emroneks/exercise/","text":"Amazon EMR on EKS \u00b6 In this exercise, you will run Spark applications using EMR on EKS. For this exercise, we are going to use an EKS cluster created using the CloudFormation template. If you are interested in building this environment end-to-end, you can do that following the first 10 steps (until line 131) in this note . Create the EMR on EKS virtual clusters \u00b6 Go to the CloudFormation Web Console (Right click -> Open Link in New Tab) and see if you have the CloudFormation stack named \"emr-on-eks\" deployed in your AWS event engine accounts. Now go to the EC2 and search for \"jumphost\". Choose the instance starting with the name \"emr-on-eks-PrepStack\". Connect to this instance using Session Manager. Once you are inside the session manager, do NOT switch to the ec2-user. Export the AWS credentials of your AWS event engine accounts (follow the step 6 of Setup ). export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > Next, run the following command to configure the Kubernetes client (kubectl) and download the certificate from the Amazon EKS control plane for authentication. sudo sed -i 's|<REGION>|us-east-1|g' /tmp/aws-logging-cloudwatch-configmap.yaml sudo sed -i 's|auto_create_group On|auto_create_group true|g' /tmp/aws-logging-cloudwatch-configmap.yaml sh /tmp/kubeconfig.sh It will take about 2 mins to complete. Once done, you will see the 3 nodes of the deployed EKS cluster in ready status. Go to the EKS Web console (Right click -> Open Link in New Tab) and check the cluster myEKS. Click on the cluster and look at the 3 EC2 nodes in the Overview section. These are the EC2 instances of the EKS cluster. Go to Configuration -> Compute and scroll down to see the Fargate profile attached to the EKS cluster. Run the following command to register the EKS namespace backed by EC2 instances with an EMR virtual cluster. sh /tmp/emroneks.sh ec2-ns ec2-vc Run the following command to register the EKS namespace backed by Fargate profile with another EMR virtual cluster. sh /tmp/emroneks-fargate.sh fargate-vc You can now go to the EMR Web Console (Right click -> Open Link in New Tab). On the bottom left side pane, you will be able to see the \"EMR on EKS\" section. Click on Virtual clusters. You can see the two EMR on EKS virtual clusters created for EC2 and Fargate namespaces respectively. Virtual clusters do not run any resources. Build the Cloud9 workspace \u00b6 We will use Cloud9 to observe Kubernetes dashboards. Go to the Cloud9 Web Console (Right click -> Open Link in New Tab) -> Create environment. In Step 1, name your environment like \"emr-on-eks-platform\". In Step 2, choose t3.small for instance type. Leave the values for Environment type, Instance type, Platform defaulted. Choose Network settings (advanced) and choose VPC. Select the VPC starting with the name \"emr-on-eks-EksStack\". Go to Next Step and create environment. This will take a few minutes. Once the environment is created, click on the grey circle button in top right corner and select Manage EC2 Instance. You will be taken to the cloud9 instance in the EC2 console. Select the instance, then choose Actions / Security / Modify IAM Role. Select the IAM role that looks like \"emr-on-eks-PrepStack-XXXX-rJumpHostInstanceProfile\" and Save. Now return to your Cloud9 workspace and click the gear icon in top right corner. Select AWS SETTINGS. Turn off AWS managed temporary credentials. Close the Preferences tab. Install Tools \u00b6 Let's install eksctl and kubectl in our Cloud9 environment. Kubernetes uses a command line utility called kubectl for communicating with the cluster API server. eksctl is a simple CLI tool for creating and managing EKS clusters. Run the following commands in your Cloud9 IDE. You can expand the bash screen for better visibility. Upgrade your AWS CLI. pip3 install awscli --upgrade --user Install eksctl with following commands. curl -- silent -- location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz - C / tmp sudo mv / tmp / eksctl / usr / local / bin eksctl version Install kubectl with following commands and associate it with the EKS cluster. curl - o kubectl https : // amazon - eks . s3 . us - west - 2. amazonaws . com / 1.19 . 6 / 2021 - 01 - 05 / bin / linux / amd64 / kubectl chmod + x ./ kubectl mkdir - p $ HOME / bin && cp ./ kubectl $ HOME / bin / kubectl && export PATH =$ PATH : $ HOME / bin echo 'export PATH=$PATH:$HOME/bin' >> ~/. bashrc kubectl version -- short -- client aws eks update - kubeconfig -- region us - east - 1 -- name myEKS kubectl get nodes - o wide The last command should display the EC2 nodes of your EKS cluster. Similarly, running the eksctl get cluster command should display the myEKS cluster. eksctl get cluster -- region us - east - 1 Run the following command and see if the EMR on EKS clusters we created are getting listed. aws emr - containers list - virtual - clusters -- region us - east - 1 Submit jobs using EMR on EKS \u00b6 Now, lets submit jobs to these clusters. For that, we are going to use EMR Containers API. To make it easy, lets assign variables to be used in these APIs. sudo yum install - y jq ec2_vc = $ ( aws emr - containers list - virtual - clusters -- region us - east - 1 | jq - r .' virtualClusters [] | select (. name == \"ec2-vc\" ) | . id ') fargate_vc = $ ( aws emr - containers list - virtual - clusters -- region us - east - 1 | jq - r .' virtualClusters [] | select (. name == \"fargate-vc\" ) | . id ') emrOnEksExecRoleArn = $ ( aws iam list - roles | jq - r .' Roles [] | select (. RoleName | endswith ( \"-EMRExectionRole\" )) | . Arn ') accountID = $ ( aws sts get - caller - identity -- query Account -- output text ) Submit jobs to EC2 namespace \u00b6 Let us submit a Spark job to the EMR on EKS cluster with namespace attached to the EC2 node group. No need to change anything since we have used variables. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-pi-6.5 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-6.5.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 You should get the below response. { \"virtualClusterId\": \"vaqteerxju21c161kkr7awe00\", \"arn\": \"arn:aws:emr-containers:us-east-1:156321623241:/virtualclusters/vaqteerxju21c161kkr7awe00/jobruns/00000002vtm1d42gmtd\", \"id\": \"00000002vtm1d42gmtd\", \"name\": \"spark-pi-6.5\" } Right after this, execute the below command to see what happens on the EC2 namespace in your EKS cluster. kubectl get pods -n ec2-ns --watch Almost immediately you will see EMR virtual cluster will create driver and executor pods on the EC2 namespace of your EKS cluster. Check the job in the EMR Web Console (Right click -> Open Link in New Tab) -> ec2-vc. You will see that the job is completed. Click on \"View Logs\" to open the persistent Spark history server. You can also check the Spark job logs by going to CloudWatch Web Console (Right click -> Open Link in New Tab) and clicking on \"Log groups\". You will see there is a log group called \"emroneks\". You can see the driver and executor logs. Pass Advanced Configs as JSON to the API \u00b6 Let's run a Spark ETL job with Glue metastore integration and configure it to send driver and executor logs to S3 and Cloudwatch log group. This is the ETL job (spark-etl-glue.py) we are going to use which will read data, do some basic transformations and write the output to a table on AWS Glue data catalog. This code is just for your reference. import sys from datetime import datetime from pyspark.sql import SparkSession from pyspark.sql import SQLContext from pyspark.sql.functions import * if __name__ == \"__main__\" : print ( len ( sys . argv )) if ( len ( sys . argv ) != 4 ): print ( \"Usage: spark-etl-glue [input-folder] [output-folder] [dbName]\" ) sys . exit ( 0 ) spark = SparkSession \\ . builder \\ . appName ( \"Python Spark SQL Glue integration example\" ) \\ . enableHiveSupport () \\ . getOrCreate () nyTaxi = spark . read . option ( \"inferSchema\" , \"true\" ) . option ( \"header\" , \"true\" ) . csv ( sys . argv [ 1 ]) updatedNYTaxi = nyTaxi . withColumn ( \"current_date\" , lit ( datetime . now ())) updatedNYTaxi . printSchema () print ( updatedNYTaxi . show ()) print ( \"Total number of records: \" + str ( updatedNYTaxi . count ())) updatedNYTaxi . write . parquet ( sys . argv [ 2 ]) updatedNYTaxi . registerTempTable ( \"ny_taxi_table\" ) dbName = sys . argv [ 3 ] spark . sql ( \"CREATE database if not exists \" + dbName ) spark . sql ( \"USE \" + dbName ) spark . sql ( \"CREATE table if not exists ny_taxi_parquet USING PARQUET LOCATION '\" + sys . argv [ 2 ] + \"' AS SELECT * from ny_taxi_table \" ) We will create a JSON with all the configurations we need using the below command. sudo tee ./emroneks-config.json >/dev/null < <EOF { \"name\": \"spark-glue-integration-and-s3-log\", \"virtualClusterId\": \"${ec2_vc}\", \"executionRoleArn\": \"${emrOnEksExecRoleArn}\", \"releaseLabel\": \"emr-6.5.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/spark-etl-glue.py\", \"entryPointArguments\": [ \"s3://aws-data-analytics-workshops/shared_datasets/tripdata/\",\"s3://mrworkshop-$accountID-dayone/taxi-data-glue/\",\"tripdata\" ], \"sparkSubmitParameters\": \"--conf spark.driver. cores= 1 --conf spark.executor. memory= 2G --conf spark.driver. memory= 2G --conf spark.executor. cores= 2\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.hadoop.hive.metastore.client.factory.class\":\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" } }, { \"classification\": \"spark-log4j\", \"properties\": { \"log4j.rootCategory\": \"DEBUG, console\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"emroneks\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://mrworkshop-$accountID-dayone/emroneks/logs/\" } } } } EOF Now you can pass the config file we created in your API. aws emr - containers start - job - run -- cli - input - json file: //emroneks-config.json --region us-east-1 You can find the Spark driver and executor pod logs in S3 for the above job. You can also see the table \"ny_taxi_parquet\" in database \"tripdata\" created in AWS Glue data log. Go to Athena Web Console and see the table under database \"tripdata\". Since this is the first time we are using Athena, in order to run queries, you need to go to the Settings -> Manage add an S3 location in your account to save your query results. For example: s3://mrworkshop- -dayone/athena/. You can query the table now. select * from tripdata.ny_taxi_parquet limit 10; Change EMR Release Label \u00b6 One of the important features with EMR on EKS is the ability to change major and minor versions per release label. So far we submitted jobs using EMR 6.5.0 label. Let's now submit the jobs to other release labels. Let's submit a job to EMR 5.34.0 label. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-pi-5.34 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Similarly, you can change the minor versions also. Let's submit the same job to EMR 6.2.0. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-pi-6.2 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-6.2.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Notice the jobs in EMR on EKS console (Right click -> Open Link in New Tab). We are consolidating jobs from different versions in the same infrastructure on EKS. Submit serverless Spark jobs using Fargate \u00b6 So far we were using EC2 namespace to submit jobs. We have an EMR on EKS virtual cluster created for Fargate namespace as well. Let's submit serverless Spark jobs using Fargate. Run the below command. Notice that for --virtual-cluster-id we are passing the EMR on EKS cluster mapped to Fargate namespace. aws emr-containers start-job-run --virtual-cluster-id ${ fargate_vc } \\ --name spark-pi-5.34 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 You will see the job being submitted to Fargate virtual cluster. Now go to the EKS Web console (Right click -> Open Link in New Tab) -> Overview. In about a minute, you will see that the fargate resource has come up to run this job. Fargate automatically scales up and down based on your processing requirements. Deploy Kubernetes Dashboard \u00b6 Kubernetes Dashboard is a web-based user interface. You can use Dashboard to get an overview of applications running on your cluster. In this lab we will deploy the official Kubernetes Dashboard. Check the documentation here . To deploy the dashboard, run the following command: export DASHBOARD_VERSION = \"v2.0.0\" kubectl apply - f https : // raw . githubusercontent . com / kubernetes / dashboard /$ { DASHBOARD_VERSION } / aio / deploy / recommended . yaml You can access Dashboard using the kubectl command-line tool by running the following command in your Cloud9 terminal. This will start the proxy on port 8080. kubectl proxy --port=8080 --address=0.0.0.0 --disable-filter=true & In your Cloud9 workspace, click Tools / Preview / Preview Running Application. Scroll to the end of the browser address URL and append the following: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ You will see the Kubernetes dashboard asking for token. Once you have the login screen in the Cloud9 preview browser tab, press the Pop Out button to open the login screen in a regular browser tab, like below: To get the token, you need to create an EKS admin account. Let's do that by running following commands on Cloud9. sudo tee ./eks-admin-service-account.yaml >/dev/null <<EOF apiVersion: v1 kind: ServiceAccount metadata: name: eks-admin namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: eks-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: eks-admin namespace: kube-system EOF kubectl apply -f eks-admin-service-account.yaml Now, run the below command in Cloud9 and paste the \"token\" of the output on to your browser where it prompts you to enter token. kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}') You will now be able to see the dashboard. Submit a job again. aws emr-containers start-job-run --virtual-cluster-id ${ fargate_vc } \\ --name spark-pi-5.34 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Explore the dashboard. Orchestrate jobs on EMR on EKS using Amazon MWAA \u00b6 Let's submit (or orchestrate) jobs to EMR on EKS through Amazon MWAA. Go to the MWAA Web Console (Right click -> Open Link in New Tab). You will find a new environment starting with name \"emr-on-eks-MWAAStack\". Open the airflow UI. You will see two DAGs. One for EC2 namespace and one for fargate namespace. Toggle the DAGs ON and trigger the DAGs manually. This job copies, unzips and transforms files from S3 and then runs some analytics on top of this transformed data. This DAG takes about 10-12 mins end-to-end. You can check the status in EMR on EKS console (Right click -> Open Link in New Tab). Single AZ placement \u00b6 Our EMR on EKS cluster uses two AZs: us-east-1a and us-east-1b. Go to the Nodes section in Kubernetes dashboard to see which EC2 nodes belong to which AZ. This provides resiliency when compared to EMR on EC2 which runs on a single AZ. However, cross-AZ communication may impact performance and cost. You can define topology for your jobs and instruct all pods to launch in a single AZ. Let's see an example. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-single-az-us-east-1a \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.kubernetes.node.selector.topology.kubernetes.io/zone='us-east-1a' --conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Check the Pods section of Kubernetes dashboard. You will see that the pods are launched only on nodes running in us-east-1a. You can test the same command by substituting us-east-1a with us-east-1b and see how the pods are getting placed in your Kubernetes dashboard. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-single-az-us-east-1b \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.kubernetes.node.selector.topology.kubernetes.io/zone='us-east-1b' --conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1","title":"1 - Amazon EMR on EKS"},{"location":"day3/emroneks/exercise/#amazon-emr-on-eks","text":"In this exercise, you will run Spark applications using EMR on EKS. For this exercise, we are going to use an EKS cluster created using the CloudFormation template. If you are interested in building this environment end-to-end, you can do that following the first 10 steps (until line 131) in this note .","title":"Amazon EMR on EKS"},{"location":"day3/emroneks/exercise/#create-the-emr-on-eks-virtual-clusters","text":"Go to the CloudFormation Web Console (Right click -> Open Link in New Tab) and see if you have the CloudFormation stack named \"emr-on-eks\" deployed in your AWS event engine accounts. Now go to the EC2 and search for \"jumphost\". Choose the instance starting with the name \"emr-on-eks-PrepStack\". Connect to this instance using Session Manager. Once you are inside the session manager, do NOT switch to the ec2-user. Export the AWS credentials of your AWS event engine accounts (follow the step 6 of Setup ). export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > Next, run the following command to configure the Kubernetes client (kubectl) and download the certificate from the Amazon EKS control plane for authentication. sudo sed -i 's|<REGION>|us-east-1|g' /tmp/aws-logging-cloudwatch-configmap.yaml sudo sed -i 's|auto_create_group On|auto_create_group true|g' /tmp/aws-logging-cloudwatch-configmap.yaml sh /tmp/kubeconfig.sh It will take about 2 mins to complete. Once done, you will see the 3 nodes of the deployed EKS cluster in ready status. Go to the EKS Web console (Right click -> Open Link in New Tab) and check the cluster myEKS. Click on the cluster and look at the 3 EC2 nodes in the Overview section. These are the EC2 instances of the EKS cluster. Go to Configuration -> Compute and scroll down to see the Fargate profile attached to the EKS cluster. Run the following command to register the EKS namespace backed by EC2 instances with an EMR virtual cluster. sh /tmp/emroneks.sh ec2-ns ec2-vc Run the following command to register the EKS namespace backed by Fargate profile with another EMR virtual cluster. sh /tmp/emroneks-fargate.sh fargate-vc You can now go to the EMR Web Console (Right click -> Open Link in New Tab). On the bottom left side pane, you will be able to see the \"EMR on EKS\" section. Click on Virtual clusters. You can see the two EMR on EKS virtual clusters created for EC2 and Fargate namespaces respectively. Virtual clusters do not run any resources.","title":"Create the EMR on EKS virtual clusters"},{"location":"day3/emroneks/exercise/#build-the-cloud9-workspace","text":"We will use Cloud9 to observe Kubernetes dashboards. Go to the Cloud9 Web Console (Right click -> Open Link in New Tab) -> Create environment. In Step 1, name your environment like \"emr-on-eks-platform\". In Step 2, choose t3.small for instance type. Leave the values for Environment type, Instance type, Platform defaulted. Choose Network settings (advanced) and choose VPC. Select the VPC starting with the name \"emr-on-eks-EksStack\". Go to Next Step and create environment. This will take a few minutes. Once the environment is created, click on the grey circle button in top right corner and select Manage EC2 Instance. You will be taken to the cloud9 instance in the EC2 console. Select the instance, then choose Actions / Security / Modify IAM Role. Select the IAM role that looks like \"emr-on-eks-PrepStack-XXXX-rJumpHostInstanceProfile\" and Save. Now return to your Cloud9 workspace and click the gear icon in top right corner. Select AWS SETTINGS. Turn off AWS managed temporary credentials. Close the Preferences tab.","title":"Build the Cloud9 workspace"},{"location":"day3/emroneks/exercise/#install-tools","text":"Let's install eksctl and kubectl in our Cloud9 environment. Kubernetes uses a command line utility called kubectl for communicating with the cluster API server. eksctl is a simple CLI tool for creating and managing EKS clusters. Run the following commands in your Cloud9 IDE. You can expand the bash screen for better visibility. Upgrade your AWS CLI. pip3 install awscli --upgrade --user Install eksctl with following commands. curl -- silent -- location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz - C / tmp sudo mv / tmp / eksctl / usr / local / bin eksctl version Install kubectl with following commands and associate it with the EKS cluster. curl - o kubectl https : // amazon - eks . s3 . us - west - 2. amazonaws . com / 1.19 . 6 / 2021 - 01 - 05 / bin / linux / amd64 / kubectl chmod + x ./ kubectl mkdir - p $ HOME / bin && cp ./ kubectl $ HOME / bin / kubectl && export PATH =$ PATH : $ HOME / bin echo 'export PATH=$PATH:$HOME/bin' >> ~/. bashrc kubectl version -- short -- client aws eks update - kubeconfig -- region us - east - 1 -- name myEKS kubectl get nodes - o wide The last command should display the EC2 nodes of your EKS cluster. Similarly, running the eksctl get cluster command should display the myEKS cluster. eksctl get cluster -- region us - east - 1 Run the following command and see if the EMR on EKS clusters we created are getting listed. aws emr - containers list - virtual - clusters -- region us - east - 1","title":"Install Tools"},{"location":"day3/emroneks/exercise/#submit-jobs-using-emr-on-eks","text":"Now, lets submit jobs to these clusters. For that, we are going to use EMR Containers API. To make it easy, lets assign variables to be used in these APIs. sudo yum install - y jq ec2_vc = $ ( aws emr - containers list - virtual - clusters -- region us - east - 1 | jq - r .' virtualClusters [] | select (. name == \"ec2-vc\" ) | . id ') fargate_vc = $ ( aws emr - containers list - virtual - clusters -- region us - east - 1 | jq - r .' virtualClusters [] | select (. name == \"fargate-vc\" ) | . id ') emrOnEksExecRoleArn = $ ( aws iam list - roles | jq - r .' Roles [] | select (. RoleName | endswith ( \"-EMRExectionRole\" )) | . Arn ') accountID = $ ( aws sts get - caller - identity -- query Account -- output text )","title":"Submit jobs using EMR on EKS"},{"location":"day3/emroneks/exercise/#submit-jobs-to-ec2-namespace","text":"Let us submit a Spark job to the EMR on EKS cluster with namespace attached to the EC2 node group. No need to change anything since we have used variables. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-pi-6.5 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-6.5.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 You should get the below response. { \"virtualClusterId\": \"vaqteerxju21c161kkr7awe00\", \"arn\": \"arn:aws:emr-containers:us-east-1:156321623241:/virtualclusters/vaqteerxju21c161kkr7awe00/jobruns/00000002vtm1d42gmtd\", \"id\": \"00000002vtm1d42gmtd\", \"name\": \"spark-pi-6.5\" } Right after this, execute the below command to see what happens on the EC2 namespace in your EKS cluster. kubectl get pods -n ec2-ns --watch Almost immediately you will see EMR virtual cluster will create driver and executor pods on the EC2 namespace of your EKS cluster. Check the job in the EMR Web Console (Right click -> Open Link in New Tab) -> ec2-vc. You will see that the job is completed. Click on \"View Logs\" to open the persistent Spark history server. You can also check the Spark job logs by going to CloudWatch Web Console (Right click -> Open Link in New Tab) and clicking on \"Log groups\". You will see there is a log group called \"emroneks\". You can see the driver and executor logs.","title":"Submit jobs to EC2 namespace"},{"location":"day3/emroneks/exercise/#pass-advanced-configs-as-json-to-the-api","text":"Let's run a Spark ETL job with Glue metastore integration and configure it to send driver and executor logs to S3 and Cloudwatch log group. This is the ETL job (spark-etl-glue.py) we are going to use which will read data, do some basic transformations and write the output to a table on AWS Glue data catalog. This code is just for your reference. import sys from datetime import datetime from pyspark.sql import SparkSession from pyspark.sql import SQLContext from pyspark.sql.functions import * if __name__ == \"__main__\" : print ( len ( sys . argv )) if ( len ( sys . argv ) != 4 ): print ( \"Usage: spark-etl-glue [input-folder] [output-folder] [dbName]\" ) sys . exit ( 0 ) spark = SparkSession \\ . builder \\ . appName ( \"Python Spark SQL Glue integration example\" ) \\ . enableHiveSupport () \\ . getOrCreate () nyTaxi = spark . read . option ( \"inferSchema\" , \"true\" ) . option ( \"header\" , \"true\" ) . csv ( sys . argv [ 1 ]) updatedNYTaxi = nyTaxi . withColumn ( \"current_date\" , lit ( datetime . now ())) updatedNYTaxi . printSchema () print ( updatedNYTaxi . show ()) print ( \"Total number of records: \" + str ( updatedNYTaxi . count ())) updatedNYTaxi . write . parquet ( sys . argv [ 2 ]) updatedNYTaxi . registerTempTable ( \"ny_taxi_table\" ) dbName = sys . argv [ 3 ] spark . sql ( \"CREATE database if not exists \" + dbName ) spark . sql ( \"USE \" + dbName ) spark . sql ( \"CREATE table if not exists ny_taxi_parquet USING PARQUET LOCATION '\" + sys . argv [ 2 ] + \"' AS SELECT * from ny_taxi_table \" ) We will create a JSON with all the configurations we need using the below command. sudo tee ./emroneks-config.json >/dev/null < <EOF { \"name\": \"spark-glue-integration-and-s3-log\", \"virtualClusterId\": \"${ec2_vc}\", \"executionRoleArn\": \"${emrOnEksExecRoleArn}\", \"releaseLabel\": \"emr-6.5.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/spark-etl-glue.py\", \"entryPointArguments\": [ \"s3://aws-data-analytics-workshops/shared_datasets/tripdata/\",\"s3://mrworkshop-$accountID-dayone/taxi-data-glue/\",\"tripdata\" ], \"sparkSubmitParameters\": \"--conf spark.driver. cores= 1 --conf spark.executor. memory= 2G --conf spark.driver. memory= 2G --conf spark.executor. cores= 2\" } }, \"configurationOverrides\": { \"applicationConfiguration\": [{ \"classification\": \"spark-defaults\", \"properties\": { \"spark.dynamicAllocation.enabled\": \"false\", \"spark.hadoop.hive.metastore.client.factory.class\":\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" } }, { \"classification\": \"spark-log4j\", \"properties\": { \"log4j.rootCategory\": \"DEBUG, console\" } } ], \"monitoringConfiguration\": { \"cloudWatchMonitoringConfiguration\": { \"logGroupName\": \"emroneks\" }, \"s3MonitoringConfiguration\": { \"logUri\": \"s3://mrworkshop-$accountID-dayone/emroneks/logs/\" } } } } EOF Now you can pass the config file we created in your API. aws emr - containers start - job - run -- cli - input - json file: //emroneks-config.json --region us-east-1 You can find the Spark driver and executor pod logs in S3 for the above job. You can also see the table \"ny_taxi_parquet\" in database \"tripdata\" created in AWS Glue data log. Go to Athena Web Console and see the table under database \"tripdata\". Since this is the first time we are using Athena, in order to run queries, you need to go to the Settings -> Manage add an S3 location in your account to save your query results. For example: s3://mrworkshop- -dayone/athena/. You can query the table now. select * from tripdata.ny_taxi_parquet limit 10;","title":"Pass Advanced Configs as JSON to the API"},{"location":"day3/emroneks/exercise/#change-emr-release-label","text":"One of the important features with EMR on EKS is the ability to change major and minor versions per release label. So far we submitted jobs using EMR 6.5.0 label. Let's now submit the jobs to other release labels. Let's submit a job to EMR 5.34.0 label. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-pi-5.34 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Similarly, you can change the minor versions also. Let's submit the same job to EMR 6.2.0. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-pi-6.2 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-6.2.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Notice the jobs in EMR on EKS console (Right click -> Open Link in New Tab). We are consolidating jobs from different versions in the same infrastructure on EKS.","title":"Change EMR Release Label"},{"location":"day3/emroneks/exercise/#submit-serverless-spark-jobs-using-fargate","text":"So far we were using EC2 namespace to submit jobs. We have an EMR on EKS virtual cluster created for Fargate namespace as well. Let's submit serverless Spark jobs using Fargate. Run the below command. Notice that for --virtual-cluster-id we are passing the EMR on EKS cluster mapped to Fargate namespace. aws emr-containers start-job-run --virtual-cluster-id ${ fargate_vc } \\ --name spark-pi-5.34 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 You will see the job being submitted to Fargate virtual cluster. Now go to the EKS Web console (Right click -> Open Link in New Tab) -> Overview. In about a minute, you will see that the fargate resource has come up to run this job. Fargate automatically scales up and down based on your processing requirements.","title":"Submit serverless Spark jobs using Fargate"},{"location":"day3/emroneks/exercise/#deploy-kubernetes-dashboard","text":"Kubernetes Dashboard is a web-based user interface. You can use Dashboard to get an overview of applications running on your cluster. In this lab we will deploy the official Kubernetes Dashboard. Check the documentation here . To deploy the dashboard, run the following command: export DASHBOARD_VERSION = \"v2.0.0\" kubectl apply - f https : // raw . githubusercontent . com / kubernetes / dashboard /$ { DASHBOARD_VERSION } / aio / deploy / recommended . yaml You can access Dashboard using the kubectl command-line tool by running the following command in your Cloud9 terminal. This will start the proxy on port 8080. kubectl proxy --port=8080 --address=0.0.0.0 --disable-filter=true & In your Cloud9 workspace, click Tools / Preview / Preview Running Application. Scroll to the end of the browser address URL and append the following: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ You will see the Kubernetes dashboard asking for token. Once you have the login screen in the Cloud9 preview browser tab, press the Pop Out button to open the login screen in a regular browser tab, like below: To get the token, you need to create an EKS admin account. Let's do that by running following commands on Cloud9. sudo tee ./eks-admin-service-account.yaml >/dev/null <<EOF apiVersion: v1 kind: ServiceAccount metadata: name: eks-admin namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: eks-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: eks-admin namespace: kube-system EOF kubectl apply -f eks-admin-service-account.yaml Now, run the below command in Cloud9 and paste the \"token\" of the output on to your browser where it prompts you to enter token. kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}') You will now be able to see the dashboard. Submit a job again. aws emr-containers start-job-run --virtual-cluster-id ${ fargate_vc } \\ --name spark-pi-5.34 \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Explore the dashboard.","title":"Deploy Kubernetes Dashboard"},{"location":"day3/emroneks/exercise/#orchestrate-jobs-on-emr-on-eks-using-amazon-mwaa","text":"Let's submit (or orchestrate) jobs to EMR on EKS through Amazon MWAA. Go to the MWAA Web Console (Right click -> Open Link in New Tab). You will find a new environment starting with name \"emr-on-eks-MWAAStack\". Open the airflow UI. You will see two DAGs. One for EC2 namespace and one for fargate namespace. Toggle the DAGs ON and trigger the DAGs manually. This job copies, unzips and transforms files from S3 and then runs some analytics on top of this transformed data. This DAG takes about 10-12 mins end-to-end. You can check the status in EMR on EKS console (Right click -> Open Link in New Tab).","title":"Orchestrate jobs on EMR on EKS using Amazon MWAA"},{"location":"day3/emroneks/exercise/#single-az-placement","text":"Our EMR on EKS cluster uses two AZs: us-east-1a and us-east-1b. Go to the Nodes section in Kubernetes dashboard to see which EC2 nodes belong to which AZ. This provides resiliency when compared to EMR on EC2 which runs on a single AZ. However, cross-AZ communication may impact performance and cost. You can define topology for your jobs and instruct all pods to launch in a single AZ. Let's see an example. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-single-az-us-east-1a \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.kubernetes.node.selector.topology.kubernetes.io/zone='us-east-1a' --conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1 Check the Pods section of Kubernetes dashboard. You will see that the pods are launched only on nodes running in us-east-1a. You can test the same command by substituting us-east-1a with us-east-1b and see how the pods are getting placed in your Kubernetes dashboard. aws emr-containers start-job-run --virtual-cluster-id ${ ec2_vc } \\ --name spark-single-az-us-east-1b \\ --execution-role-arn ${ emrOnEksExecRoleArn } \\ --release-label emr-5.34.0-latest \\ --job-driver '{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/pi.py\", \"sparkSubmitParameters\": \"--conf spark.kubernetes.node.selector.topology.kubernetes.io/zone='us-east-1b' --conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=2 --conf spark.driver.cores=1\" } }' \\ --configuration-overrides '{\"monitoringConfiguration\": {\"cloudWatchMonitoringConfiguration\": {\"logGroupName\": \"emroneks\"}}}' \\ --region us-east-1","title":"Single AZ placement"},{"location":"day3/hbase/exercise/","text":"Apache HBase on S3 on Amazon EMR \u00b6","title":"**Apache HBase on S3 on Amazon EMR**"},{"location":"day3/hbase/exercise/#apache-hbase-on-s3-on-amazon-emr","text":"","title":"Apache HBase on S3 on Amazon EMR"},{"location":"day3/hive/exercise/","text":"Apache Hive and Presto on EMR \u00b6 In this section we are going to use Hive and Presto to run batch ETL jobs and adhoc queries. Hive on EMR \u00b6 Running Hive jobs on Hue \u00b6 In AWS Web Console, Go to EMR Console -> EMR-Hive-HBaseOnS3 Login to the leader node of this cluster using Session Manager or SSH (Go to Hardware tab -> master fleet -> Click on the instance ID -> Go to EC2 console -> Connect with Session Manager). Run the following command: cd ~ sudo su hadoop sudo -su hdfs hdfs dfs -chmod -R 777 / Let's connect to Hue in this cluster to submit Hive queries. For this you will need to install AWS CLI and Session Manager plugin on your local desktop to do this. Replace the environmental variables with the values from the Team Dashboard. Run the below commands in your local desktop. For Windows, you will need to use \"set\" instead of \"export\". export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > Run the below command on your local desktop. Replace --target with your leader node instance ID of the EMR cluster in the following command. aws ssm start - session -- target i - 054 c79edd2456227b -- document - name AWS - StartPortForwardingSession -- parameters '{ \"portNumber\" : [ \"8888\" ], \"localPortNumber\" : [ \"8158\" ]}' -- region us - east - 1 It should start a session on Hue port (8888). Go to the your browser and type http://localhost:8158 in the address bar. You should now be taken to the Hue console. Login to Hue by. You can choose any user name and password you like. Click \"Create account\". Once you are logged in, you will see Hue editor. By default, it goes to MySQL editor. Click on this icon on the top left corner and choose Hive editor. Now we can run some Hive queries. Copy the below contents into Hue editor. Run these queries on Hue one at a time. REPLACE your account ID in S3 location wherever instructed. CREATE TABLE ` lineitem ` ( ` l_orderkey ` string , ` l_partkey ` bigint , ` l_suppkey ` string , ` l_linenumber ` bigint , ` l_quantity ` bigint , ` l_extendedprice ` double , ` l_discount ` double , ` l_tax ` double , ` l_returnflag ` string , ` l_linestatus ` string , ` l_shipdate ` string , ` l_commitdate ` string , ` l_receiptdate ` string , ` l_shipinstruct ` string , ` l_shipmode ` string , ` l_comment ` string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://redshift-downloads/TPC-H/10GB/lineitem/' ; CREATE EXTERNAL TABLE ` orders ` ( ` o_orderkey ` bigint , ` o_custkey ` bigint , ` o_orderstatus ` string , ` o_totalprice ` double , ` o_orderdate ` string , ` o_orderpriority ` string , ` o_clerk ` string , ` o_shippriority ` bigint , ` o_comment ` string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://redshift-downloads/TPC-H/10GB/orders/' ; -- REPLACE your account ID in S3 location CREATE EXTERNAL TABLE ` lineitemorders ` ( ` orderkey ` string , ` linenumber ` bigint , ` quantity ` bigint , ` totalprice ` double , ` extendedprice ` double , ` tax ` double , ` discount ` double , ` orderpriority ` string , ` shippriority ` bigint ) PARTITIONED BY ( linestatus string ) CLUSTERED BY ( orderkey ) SORTED BY ( orderkey ASC ) INTO 10 BUCKETS STORED AS PARQUET LOCATION 's3://mrworkshop-youraccountID-dayone/hive/lineitemorders/' ; set hive . exec . dynamic . partition = true ; set hive . exec . dynamic . partition . mode = nonstrict ; insert overwrite table lineitemorders partition ( linestatus = 'O' ) select o_orderkey , l_linenumber l_quantity , o_totalprice , l_extendedprice , l_tax , l_discount , o_orderpriority , o_shippriority , o_orderdate as orderdate from lineitem join orders on l_orderkey = o_orderkey where l_linestatus = 'O' ; select * from lineitemorders limit 5 ; select quantity * ( totalprice + extendedprice ) as finalprice , orderpriority from lineitemorders where linestatus = 'O' group by orderpriority , quantity , totalprice , extendedprice , tax , discount order by 1 desc limit 5 ; Hive ACID tables \u00b6 Hive supports ACID tables. A single statement can write to multiple partitions or multiple tables. If the operation fails, partial writes or inserts are not visible to users. Operations remain performant even if data changes often, such as one percent per hour. Does not overwrite the entire partition to perform update or delete operations. Run the below commands in Hue editor one by one. Replace youraccountID with your AWS event engine account ID. set hive . support . concurrency = true ; set hive . exec . dynamic . partition . mode = nonstrict ; set hive . txn . manager = org . apache . hadoop . hive . ql . lockmgr . DbTxnManager ; --REPLACE your account in the S3 location CREATE TABLE acid_tbl ( key INT , value STRING , action STRING ) CLUSTERED BY ( key ) INTO 3 BUCKETS STORED AS ORC LOCATION 's3://mrworkshop-youraccountID-dayone/hive/acid_tbl' TBLPROPERTIES ( 'transactional' = 'true' ); INSERT INTO acid_tbl VALUES ( 1 , 'val1' , 'insert' ), ( 2 , 'val2' , 'insert' ), ( 3 , 'val3' , 'insert' ), ( 4 , 'val4' , 'insert' ), ( 5 , 'val5' , 'insert' ); SELECT * FROM acid_tbl ; UPDATE acid_tbl SET value = 'val5_1' , action = 'update' WHERE key = 5 ; SELECT * FROM acid_tbl ; DELETE FROM acid_tbl WHERE key = 4 ; SELECT * FROM acid_tbl ; DROP TABLE IF EXISTS acid_merge ; CREATE TABLE acid_merge ( key INT , new_value STRING ) STORED AS ORC ; INSERT INTO acid_merge VALUES ( 1 , 'val1_1' ), ( 3 , NULL ), ( 6 , 'val6' ); MERGE INTO acid_tbl AS T USING acid_merge AS M ON T . key = M . key WHEN MATCHED AND M . new_value IS NOT NULL THEN UPDATE SET value = M . new_value , action = 'merge_update' WHEN MATCHED AND M . new_value IS NULL THEN DELETE WHEN NOT MATCHED THEN INSERT VALUES ( M . key , M . new_value , 'merge_insert' ); SELECT * FROM acid_tbl ; ALTER TABLE acid_tbl COMPACT 'minor' ; SHOW COMPACTIONS ; SET hive . compactor . check . interval ; ALTER TABLE acid_tbl COMPACT 'major' ; show compactions ; select row__id , key , value , action from acid_tbl ; Orchestrate Hive jobs with AWS Step Functions \u00b6 Simba provides JDBC drivers for Hive and Presto to connect from BI tools like Tableau or SQL Workbench. Please note you can use AWS Step Functions to orchestrate any kind of EMR steps. But we are just using Hive steps here as an example. Go to AWS Management Console -> AWS Step Functions -> Create State Machine. Choose middle option \u201cWrite your own workflow in code\u201d. Under \u201cDefinition\u201d enter the following code block. Edit InstanceProfile Role and account ID in below code (marked as CHANGEME). Use the instance profile being used by your \"EMR-Spark-Hive-Presto\" cluster. It will look like \"dayone-emrEc2InstanceProfile-XXXXXXX\". This information can be retrieved from the Summary tab of your EMR cluster. (EMR Web Console -> EMR-Spark-Hive-Presto -> EC2 instance profile under Security Access). Change your account ID in the S3 bucket names. { \" StartAt \" : \" Should_Create_Cluster \" , \" States \" : { \" Should_Create_Cluster \" : { \" Type \" : \" Choice \" , \" Choices \" : [ { \" Variable \" : \" $.CreateCluster \" , \" BooleanEquals \" : true , \" Next \" : \" Create_A_Cluster \" }, { \" Variable \" : \" $.CreateCluster \" , \" BooleanEquals \" : false , \" Next \" : \" Enable_Termination_Protection \" } ], \" Default \" : \" Create_A_Cluster \" }, \" Create_A_Cluster \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:createCluster.sync \" , \" Parameters \" : { \" Name \" : \" WorkflowCluster \" , \" VisibleToAllUsers \" : true , \" ReleaseLabel \" : \" emr-5.28.0 \" , \" Applications \" : [{ \" Name \" : \" Hive \" }], \" ServiceRole \" : \" emrServiceRole \" , \" JobFlowRole \" : \" CHANGEME \" , \" Instances \" : { \" KeepJobFlowAliveWhenNoSteps \" : true , \" InstanceFleets \" : [ { \" InstanceFleetType \" : \" MASTER \" , \" TargetOnDemandCapacity \" : 1 , \" InstanceTypeConfigs \" : [ { \" InstanceType \" : \" m4.xlarge \" } ] }, { \" InstanceFleetType \" : \" CORE \" , \" TargetOnDemandCapacity \" : 1 , \" InstanceTypeConfigs \" : [ { \" InstanceType \" : \" m4.xlarge \" } ] } ] } }, \" ResultPath \" : \" $.CreateClusterResult \" , \" Next \" : \" Merge_Results \" }, \" Merge_Results \" : { \" Type \" : \" Pass \" , \" Parameters \" : { \" CreateCluster.$ \" : \" $.CreateCluster \" , \" TerminateCluster.$ \" : \" $.TerminateCluster \" , \" ClusterId.$ \" : \" $.CreateClusterResult.ClusterId \" }, \" Next \" : \" Enable_Termination_Protection \" }, \" Enable_Termination_Protection \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:setClusterTerminationProtection \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" TerminationProtected \" : true }, \" ResultPath \" : null , \" Next \" : \" Add_Steps_Parallel \" }, \" Add_Steps_Parallel \" : { \" Type \" : \" Parallel \" , \" Branches \" : [ { \" StartAt \" : \" Step_One \" , \" States \" : { \" Step_One \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:addStep.sync \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" Step \" : { \" Name \" : \" The first step \" , \" ActionOnFailure \" : \" CONTINUE \" , \" HadoopJarStep \" : { \" Jar \" : \" command-runner.jar \" , \" Args \" : [ \" hive-script \" , \" --run-hive-script \" , \" --args \" , \" -f \" , \" s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q \" , \" -d \" , \" INPUT=s3://eu-west-1.elasticmapreduce.samples \" , \" -d \" , \" OUTPUT=s3://mrworkshop-CHANGEME-dayone/MyHiveQueryResults/ \" ] } } }, \" End \" : true } } }, { \" StartAt \" : \" Wait_10_Seconds \" , \" States \" : { \" Wait_10_Seconds \" : { \" Type \" : \" Wait \" , \" Seconds \" : 10 , \" Next \" : \" Step_Two (async) \" }, \" Step_Two (async) \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:addStep \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" Step \" : { \" Name \" : \" The second step \" , \" ActionOnFailure \" : \" CONTINUE \" , \" HadoopJarStep \" : { \" Jar \" : \" command-runner.jar \" , \" Args \" : [ \" hive-script \" , \" --run-hive-script \" , \" --args \" , \" -f \" , \" s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q \" , \" -d \" , \" INPUT=s3://eu-west-1.elasticmapreduce.samples \" , \" -d \" , \" OUTPUT=s3://mrworkshop-CHANGEME-dayone/MyHiveQueryResults/ \" ] } } }, \" ResultPath \" : \" $.AddStepsResult \" , \" Next \" : \" Wait_Another_10_Seconds \" }, \" Wait_Another_10_Seconds \" : { \" Type \" : \" Wait \" , \" Seconds \" : 10 , \" Next \" : \" Cancel_Step_Two \" }, \" Cancel_Step_Two \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:cancelStep \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" StepId.$ \" : \" $.AddStepsResult.StepId \" }, \" End \" : true } } } ], \" ResultPath \" : null , \" Next \" : \" Step_Three \" }, \" Step_Three \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:addStep.sync \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" Step \" : { \" Name \" : \" The third step \" , \" ActionOnFailure \" : \" CONTINUE \" , \" HadoopJarStep \" : { \" Jar \" : \" command-runner.jar \" , \" Args \" : [ \" hive-script \" , \" --run-hive-script \" , \" --args \" , \" -f \" , \" s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q \" , \" -d \" , \" INPUT=s3://eu-west-1.elasticmapreduce.samples \" , \" -d \" , \" OUTPUT=s3://mrworkshop-CHANGEME-dayone/MyHiveQueryResults/ \" ] } } }, \" ResultPath \" : null , \" Next \" : \" Disable_Termination_Protection \" }, \" Disable_Termination_Protection \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:setClusterTerminationProtection \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" TerminationProtected \" : false }, \" ResultPath \" : null , \" Next \" : \" Should_Terminate_Cluster \" }, \" Should_Terminate_Cluster \" : { \" Type \" : \" Choice \" , \" Choices \" : [ { \" Variable \" : \" $.TerminateCluster \" , \" BooleanEquals \" : true , \" Next \" : \" Terminate_Cluster \" }, { \" Variable \" : \" $.TerminateCluster \" , \" BooleanEquals \" : false , \" Next \" : \" Wrapping_Up \" } ], \" Default \" : \" Wrapping_Up \" }, \" Terminate_Cluster \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:terminateCluster.sync \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" }, \" Next \" : \" Wrapping_Up \" }, \" Wrapping_Up \" : { \" Type \" : \" Pass \" , \" End \" : true } } } Then, you can keep rest as default and create state machine. After the state machine is created, click on \u201cStart Execution\u201d and enter the below JSON input. { \"CreateCluster\": true, \"TerminateCluster\": true } Click on \u201cStart Execution\u201d and observe the workflow. You can see the \u201cWorkflow cluster\u201d being created. It will run the EMR Hive steps as chained workflows. You can also use AWS Event Bridge to schedule jobs using AWS Step Functions. S3DistCp Utility \u00b6 Hive is typically used for batch ETL transformations. Presto is used for adhoc/interactive querying. For flat copy (copying files without applying any transformations), it is always better to use S3-Dist-Cp than using Hive insert overwrite queries. Before we use Hive and Presto for querying, lets use S3-Dist-Cp utility to migrate CSV data and convert it into Parquet format. Run the below two commands on EMR leader node. sudo su hadoop cd ~ s3 - dist - cp -- src s3 : // redshift - downloads / TPC - H / 3 TB / lineitem / -- dest / user / hadoop / lineitem s3 - dist - cp -- src s3 : // redshift - downloads / TPC - H / 3 TB / orders / -- dest / user / hadoop / orders Now, lets","title":"**Apache Hive and Presto on EMR**"},{"location":"day3/hive/exercise/#apache-hive-and-presto-on-emr","text":"In this section we are going to use Hive and Presto to run batch ETL jobs and adhoc queries.","title":"Apache Hive and Presto on EMR"},{"location":"day3/hive/exercise/#hive-on-emr","text":"","title":"Hive on EMR"},{"location":"day3/hive/exercise/#running-hive-jobs-on-hue","text":"In AWS Web Console, Go to EMR Console -> EMR-Hive-HBaseOnS3 Login to the leader node of this cluster using Session Manager or SSH (Go to Hardware tab -> master fleet -> Click on the instance ID -> Go to EC2 console -> Connect with Session Manager). Run the following command: cd ~ sudo su hadoop sudo -su hdfs hdfs dfs -chmod -R 777 / Let's connect to Hue in this cluster to submit Hive queries. For this you will need to install AWS CLI and Session Manager plugin on your local desktop to do this. Replace the environmental variables with the values from the Team Dashboard. Run the below commands in your local desktop. For Windows, you will need to use \"set\" instead of \"export\". export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > Run the below command on your local desktop. Replace --target with your leader node instance ID of the EMR cluster in the following command. aws ssm start - session -- target i - 054 c79edd2456227b -- document - name AWS - StartPortForwardingSession -- parameters '{ \"portNumber\" : [ \"8888\" ], \"localPortNumber\" : [ \"8158\" ]}' -- region us - east - 1 It should start a session on Hue port (8888). Go to the your browser and type http://localhost:8158 in the address bar. You should now be taken to the Hue console. Login to Hue by. You can choose any user name and password you like. Click \"Create account\". Once you are logged in, you will see Hue editor. By default, it goes to MySQL editor. Click on this icon on the top left corner and choose Hive editor. Now we can run some Hive queries. Copy the below contents into Hue editor. Run these queries on Hue one at a time. REPLACE your account ID in S3 location wherever instructed. CREATE TABLE ` lineitem ` ( ` l_orderkey ` string , ` l_partkey ` bigint , ` l_suppkey ` string , ` l_linenumber ` bigint , ` l_quantity ` bigint , ` l_extendedprice ` double , ` l_discount ` double , ` l_tax ` double , ` l_returnflag ` string , ` l_linestatus ` string , ` l_shipdate ` string , ` l_commitdate ` string , ` l_receiptdate ` string , ` l_shipinstruct ` string , ` l_shipmode ` string , ` l_comment ` string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://redshift-downloads/TPC-H/10GB/lineitem/' ; CREATE EXTERNAL TABLE ` orders ` ( ` o_orderkey ` bigint , ` o_custkey ` bigint , ` o_orderstatus ` string , ` o_totalprice ` double , ` o_orderdate ` string , ` o_orderpriority ` string , ` o_clerk ` string , ` o_shippriority ` bigint , ` o_comment ` string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://redshift-downloads/TPC-H/10GB/orders/' ; -- REPLACE your account ID in S3 location CREATE EXTERNAL TABLE ` lineitemorders ` ( ` orderkey ` string , ` linenumber ` bigint , ` quantity ` bigint , ` totalprice ` double , ` extendedprice ` double , ` tax ` double , ` discount ` double , ` orderpriority ` string , ` shippriority ` bigint ) PARTITIONED BY ( linestatus string ) CLUSTERED BY ( orderkey ) SORTED BY ( orderkey ASC ) INTO 10 BUCKETS STORED AS PARQUET LOCATION 's3://mrworkshop-youraccountID-dayone/hive/lineitemorders/' ; set hive . exec . dynamic . partition = true ; set hive . exec . dynamic . partition . mode = nonstrict ; insert overwrite table lineitemorders partition ( linestatus = 'O' ) select o_orderkey , l_linenumber l_quantity , o_totalprice , l_extendedprice , l_tax , l_discount , o_orderpriority , o_shippriority , o_orderdate as orderdate from lineitem join orders on l_orderkey = o_orderkey where l_linestatus = 'O' ; select * from lineitemorders limit 5 ; select quantity * ( totalprice + extendedprice ) as finalprice , orderpriority from lineitemorders where linestatus = 'O' group by orderpriority , quantity , totalprice , extendedprice , tax , discount order by 1 desc limit 5 ;","title":"Running Hive jobs on Hue"},{"location":"day3/hive/exercise/#hive-acid-tables","text":"Hive supports ACID tables. A single statement can write to multiple partitions or multiple tables. If the operation fails, partial writes or inserts are not visible to users. Operations remain performant even if data changes often, such as one percent per hour. Does not overwrite the entire partition to perform update or delete operations. Run the below commands in Hue editor one by one. Replace youraccountID with your AWS event engine account ID. set hive . support . concurrency = true ; set hive . exec . dynamic . partition . mode = nonstrict ; set hive . txn . manager = org . apache . hadoop . hive . ql . lockmgr . DbTxnManager ; --REPLACE your account in the S3 location CREATE TABLE acid_tbl ( key INT , value STRING , action STRING ) CLUSTERED BY ( key ) INTO 3 BUCKETS STORED AS ORC LOCATION 's3://mrworkshop-youraccountID-dayone/hive/acid_tbl' TBLPROPERTIES ( 'transactional' = 'true' ); INSERT INTO acid_tbl VALUES ( 1 , 'val1' , 'insert' ), ( 2 , 'val2' , 'insert' ), ( 3 , 'val3' , 'insert' ), ( 4 , 'val4' , 'insert' ), ( 5 , 'val5' , 'insert' ); SELECT * FROM acid_tbl ; UPDATE acid_tbl SET value = 'val5_1' , action = 'update' WHERE key = 5 ; SELECT * FROM acid_tbl ; DELETE FROM acid_tbl WHERE key = 4 ; SELECT * FROM acid_tbl ; DROP TABLE IF EXISTS acid_merge ; CREATE TABLE acid_merge ( key INT , new_value STRING ) STORED AS ORC ; INSERT INTO acid_merge VALUES ( 1 , 'val1_1' ), ( 3 , NULL ), ( 6 , 'val6' ); MERGE INTO acid_tbl AS T USING acid_merge AS M ON T . key = M . key WHEN MATCHED AND M . new_value IS NOT NULL THEN UPDATE SET value = M . new_value , action = 'merge_update' WHEN MATCHED AND M . new_value IS NULL THEN DELETE WHEN NOT MATCHED THEN INSERT VALUES ( M . key , M . new_value , 'merge_insert' ); SELECT * FROM acid_tbl ; ALTER TABLE acid_tbl COMPACT 'minor' ; SHOW COMPACTIONS ; SET hive . compactor . check . interval ; ALTER TABLE acid_tbl COMPACT 'major' ; show compactions ; select row__id , key , value , action from acid_tbl ;","title":"Hive ACID tables"},{"location":"day3/hive/exercise/#orchestrate-hive-jobs-with-aws-step-functions","text":"Simba provides JDBC drivers for Hive and Presto to connect from BI tools like Tableau or SQL Workbench. Please note you can use AWS Step Functions to orchestrate any kind of EMR steps. But we are just using Hive steps here as an example. Go to AWS Management Console -> AWS Step Functions -> Create State Machine. Choose middle option \u201cWrite your own workflow in code\u201d. Under \u201cDefinition\u201d enter the following code block. Edit InstanceProfile Role and account ID in below code (marked as CHANGEME). Use the instance profile being used by your \"EMR-Spark-Hive-Presto\" cluster. It will look like \"dayone-emrEc2InstanceProfile-XXXXXXX\". This information can be retrieved from the Summary tab of your EMR cluster. (EMR Web Console -> EMR-Spark-Hive-Presto -> EC2 instance profile under Security Access). Change your account ID in the S3 bucket names. { \" StartAt \" : \" Should_Create_Cluster \" , \" States \" : { \" Should_Create_Cluster \" : { \" Type \" : \" Choice \" , \" Choices \" : [ { \" Variable \" : \" $.CreateCluster \" , \" BooleanEquals \" : true , \" Next \" : \" Create_A_Cluster \" }, { \" Variable \" : \" $.CreateCluster \" , \" BooleanEquals \" : false , \" Next \" : \" Enable_Termination_Protection \" } ], \" Default \" : \" Create_A_Cluster \" }, \" Create_A_Cluster \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:createCluster.sync \" , \" Parameters \" : { \" Name \" : \" WorkflowCluster \" , \" VisibleToAllUsers \" : true , \" ReleaseLabel \" : \" emr-5.28.0 \" , \" Applications \" : [{ \" Name \" : \" Hive \" }], \" ServiceRole \" : \" emrServiceRole \" , \" JobFlowRole \" : \" CHANGEME \" , \" Instances \" : { \" KeepJobFlowAliveWhenNoSteps \" : true , \" InstanceFleets \" : [ { \" InstanceFleetType \" : \" MASTER \" , \" TargetOnDemandCapacity \" : 1 , \" InstanceTypeConfigs \" : [ { \" InstanceType \" : \" m4.xlarge \" } ] }, { \" InstanceFleetType \" : \" CORE \" , \" TargetOnDemandCapacity \" : 1 , \" InstanceTypeConfigs \" : [ { \" InstanceType \" : \" m4.xlarge \" } ] } ] } }, \" ResultPath \" : \" $.CreateClusterResult \" , \" Next \" : \" Merge_Results \" }, \" Merge_Results \" : { \" Type \" : \" Pass \" , \" Parameters \" : { \" CreateCluster.$ \" : \" $.CreateCluster \" , \" TerminateCluster.$ \" : \" $.TerminateCluster \" , \" ClusterId.$ \" : \" $.CreateClusterResult.ClusterId \" }, \" Next \" : \" Enable_Termination_Protection \" }, \" Enable_Termination_Protection \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:setClusterTerminationProtection \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" TerminationProtected \" : true }, \" ResultPath \" : null , \" Next \" : \" Add_Steps_Parallel \" }, \" Add_Steps_Parallel \" : { \" Type \" : \" Parallel \" , \" Branches \" : [ { \" StartAt \" : \" Step_One \" , \" States \" : { \" Step_One \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:addStep.sync \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" Step \" : { \" Name \" : \" The first step \" , \" ActionOnFailure \" : \" CONTINUE \" , \" HadoopJarStep \" : { \" Jar \" : \" command-runner.jar \" , \" Args \" : [ \" hive-script \" , \" --run-hive-script \" , \" --args \" , \" -f \" , \" s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q \" , \" -d \" , \" INPUT=s3://eu-west-1.elasticmapreduce.samples \" , \" -d \" , \" OUTPUT=s3://mrworkshop-CHANGEME-dayone/MyHiveQueryResults/ \" ] } } }, \" End \" : true } } }, { \" StartAt \" : \" Wait_10_Seconds \" , \" States \" : { \" Wait_10_Seconds \" : { \" Type \" : \" Wait \" , \" Seconds \" : 10 , \" Next \" : \" Step_Two (async) \" }, \" Step_Two (async) \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:addStep \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" Step \" : { \" Name \" : \" The second step \" , \" ActionOnFailure \" : \" CONTINUE \" , \" HadoopJarStep \" : { \" Jar \" : \" command-runner.jar \" , \" Args \" : [ \" hive-script \" , \" --run-hive-script \" , \" --args \" , \" -f \" , \" s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q \" , \" -d \" , \" INPUT=s3://eu-west-1.elasticmapreduce.samples \" , \" -d \" , \" OUTPUT=s3://mrworkshop-CHANGEME-dayone/MyHiveQueryResults/ \" ] } } }, \" ResultPath \" : \" $.AddStepsResult \" , \" Next \" : \" Wait_Another_10_Seconds \" }, \" Wait_Another_10_Seconds \" : { \" Type \" : \" Wait \" , \" Seconds \" : 10 , \" Next \" : \" Cancel_Step_Two \" }, \" Cancel_Step_Two \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:cancelStep \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" StepId.$ \" : \" $.AddStepsResult.StepId \" }, \" End \" : true } } } ], \" ResultPath \" : null , \" Next \" : \" Step_Three \" }, \" Step_Three \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:addStep.sync \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" Step \" : { \" Name \" : \" The third step \" , \" ActionOnFailure \" : \" CONTINUE \" , \" HadoopJarStep \" : { \" Jar \" : \" command-runner.jar \" , \" Args \" : [ \" hive-script \" , \" --run-hive-script \" , \" --args \" , \" -f \" , \" s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q \" , \" -d \" , \" INPUT=s3://eu-west-1.elasticmapreduce.samples \" , \" -d \" , \" OUTPUT=s3://mrworkshop-CHANGEME-dayone/MyHiveQueryResults/ \" ] } } }, \" ResultPath \" : null , \" Next \" : \" Disable_Termination_Protection \" }, \" Disable_Termination_Protection \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:setClusterTerminationProtection \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" , \" TerminationProtected \" : false }, \" ResultPath \" : null , \" Next \" : \" Should_Terminate_Cluster \" }, \" Should_Terminate_Cluster \" : { \" Type \" : \" Choice \" , \" Choices \" : [ { \" Variable \" : \" $.TerminateCluster \" , \" BooleanEquals \" : true , \" Next \" : \" Terminate_Cluster \" }, { \" Variable \" : \" $.TerminateCluster \" , \" BooleanEquals \" : false , \" Next \" : \" Wrapping_Up \" } ], \" Default \" : \" Wrapping_Up \" }, \" Terminate_Cluster \" : { \" Type \" : \" Task \" , \" Resource \" : \" arn:aws:states:::elasticmapreduce:terminateCluster.sync \" , \" Parameters \" : { \" ClusterId.$ \" : \" $.ClusterId \" }, \" Next \" : \" Wrapping_Up \" }, \" Wrapping_Up \" : { \" Type \" : \" Pass \" , \" End \" : true } } } Then, you can keep rest as default and create state machine. After the state machine is created, click on \u201cStart Execution\u201d and enter the below JSON input. { \"CreateCluster\": true, \"TerminateCluster\": true } Click on \u201cStart Execution\u201d and observe the workflow. You can see the \u201cWorkflow cluster\u201d being created. It will run the EMR Hive steps as chained workflows. You can also use AWS Event Bridge to schedule jobs using AWS Step Functions.","title":"Orchestrate Hive jobs with AWS Step Functions"},{"location":"day3/hive/exercise/#s3distcp-utility","text":"Hive is typically used for batch ETL transformations. Presto is used for adhoc/interactive querying. For flat copy (copying files without applying any transformations), it is always better to use S3-Dist-Cp than using Hive insert overwrite queries. Before we use Hive and Presto for querying, lets use S3-Dist-Cp utility to migrate CSV data and convert it into Parquet format. Run the below two commands on EMR leader node. sudo su hadoop cd ~ s3 - dist - cp -- src s3 : // redshift - downloads / TPC - H / 3 TB / lineitem / -- dest / user / hadoop / lineitem s3 - dist - cp -- src s3 : // redshift - downloads / TPC - H / 3 TB / orders / -- dest / user / hadoop / orders Now, lets","title":"S3DistCp Utility"},{"location":"day3/lf/exercise/","text":"Fine-grained Authorization with LakeFormation Integration \u00b6 In this exercise you will integrate LakeFormation with EMR for fine-grained access control. Setup Okta IdP \u00b6 For this lab, you will create an Okta account with your business email. It is free to create. This is the sign up page . Once you create the account and login, this is how the page will look like. From the left hand side pane, go to Directory -> People -> Add person. Add the following users: Analyst One \u2013 analyst1@dsar.com Analyst Two \u2013 analyst2@dsar.com For Password, choose Set by admin and enter a valid password. Deselect the option User must change password on first login. Choose Save. Once user is created, click on the user name and go to Profile tab. Edit the profile attributes for the newly created users to specify the Display name as analyst1 and analyst2 respectively. Do not specify @dsar.com in the Display name field . Again from the left hand side pane, go to Applications -> Applications. Choose Create App Integration and choose SAML 2.0. Click Next. In App name, provide a friendly name for your application. For eg: lfemrapp. Go to next. Enter following values in the next section. Replace youraccountID with your AWS event engine account ID in the two IAM ARNs in the Value column of 3rd row. Be careful to not add any extra trailing whitespaces. # Section Name Value 1 General Single Sign-On URL https://public-dns:8442/gateway/knoxsso/api/v1/websso?pac4jCallback=true&client_name=SAML2Client 2 General Audience URI (SP Entity ID) urn:amazon:webservices 3 Attribute Statements https://aws.amazon.com/SAML/Attributes/Role arn:aws:iam::youraccountID:role/LF-SAML-Role-emr-lf-saml,arn:aws:iam::youraccountID:saml-provider/oktaSAMLProvider 4 Attribute Statements https://aws.amazon.com/SAML/Attributes/RoleSessionName user.displayName 5 Attribute Statements https://glue.amazon.com/SAML/Attributes/UserName user.displayName Filled out form with above values looks like below. Go to Next. Choose \"I'm an Okta customer adding an internal app\". Click Finish. Your application will now be created and will be in active state. Now go the Assignments tab (last tab in the above screenshot) to assign analyst1 and analyst2 to this newly created application. Do not specify @dsar.com in the User Name field when assigning people to the application. . Click on Assign -> Assign to People. Click on Assign next to analyst 1 and analyst 2 users. Remove the @dsar.com for both users. Do not specify @dsar.com in the User Name field when assigning people to the application. . Save and go back. Once both users are in Assigned state, click Done. Now go to Sign On tab. Under Sign On Methods, choose the link location for the IdP metadata file (right-click) and choose Copy Link Address. It should look like: https://dev- .okta.com/app/ /sso/saml/metadata Also, save this metadata to your local desktop. (Right click -> Save File). Type .xml as extension. Set up Lake Formation \u00b6 In your AWS Web Console (event engine), go to the AWS Lake Formation Console. Once you go to the console, the screen will look like below. Click Get Started. In left side pane, under Data catalog, choose Settings. For Data catalog settings, deselect the two permissions check-boxes. Choose Save. In left side pane, go to Permissions -> Administrative roles and tasks. In Database creators section, click on Revoke. Under IAM users and roles, type IAMAllowedPrincipals and select it from drop down. Select \"Create Database\" and click \"Revoke\". Now, choose Data catalog -> Databases from the left navigation pane. Choose Create database. Database name: lfoktasamlblogdb For Location, enter S3 path with your account ID replaced -> s3://mrworkshop-youraccountID-dayone/lf/ Click Create Database. Launch Cloudformation Stack \u00b6 Now, launch the CloudFormation Stack (Right click -> Open Link in New Tab). Provide following parameters. Replace youraccountID with your AWS event engine account ID. Parameter Value StackName emr-lf-saml userBucketName s3://mrworkshop-youraccountID-dayone/ OktaAppMetadataURL metadata URL we copied from the Okta console SAMLProviderName oktaSAMLProvider Realm EC2.INTERNAL KdcAdminPassword Test123$ ReleaseLabel emr-5.31.0 InstanceType c4.xlarge VPCSubnet Choose MMPublicSubnetOne myIPCidr 0.0.0.0/0 oktaUser1 analyst1 oktaUser2 analyst2 EC2KeyPair ee-default-keypair Screenshot of filled values: Click on Next two times. Then check \"I acknowledge that AWS CloudFormation might create IAM resources with custom names.\" and click \"Create Stack\". The stack will take about 15 minutes to get created. Once the stack is created, in the Outputs tab of the Cloud Formation console, make a note of the primary DNS, SAML role ARN, and IdP ARN. Go back to your Okta page. Go to General tab under application. Update the attributes. Validate the solution \u00b6 Launch a web browser in incognito mode and open a Zeppelin notebook using the URL https:// :8442/gateway/default/zeppelin/ (replace with the master node public DNS of the EMR cluster obtained from Cloudformation Output). The Single Sign-On login page appears. Okta validates the login credentials with the system of record, like Active Directory, and returns a SAML, which is parsed and the next page is displayed based on the redirect URL parameter. Log in to Zeppelin as analyst1. After login, choose Create a note and run this SQL statement: spark . sql ( \" select * from lfoktasamlblogdb.taxi_data limit 10 \" ) . show () The following screenshot shows that analyst1 can only see the two columns that you specified in Lake Formation. Open another web browser in incognito mode and log in to Zeppelin as analyst2. The same select query shows all the columns, as shown in the following screenshot.","title":"**Fine-grained Authorization with LakeFormation Integration**"},{"location":"day3/lf/exercise/#fine-grained-authorization-with-lakeformation-integration","text":"In this exercise you will integrate LakeFormation with EMR for fine-grained access control.","title":"Fine-grained Authorization with LakeFormation Integration"},{"location":"day3/lf/exercise/#setup-okta-idp","text":"For this lab, you will create an Okta account with your business email. It is free to create. This is the sign up page . Once you create the account and login, this is how the page will look like. From the left hand side pane, go to Directory -> People -> Add person. Add the following users: Analyst One \u2013 analyst1@dsar.com Analyst Two \u2013 analyst2@dsar.com For Password, choose Set by admin and enter a valid password. Deselect the option User must change password on first login. Choose Save. Once user is created, click on the user name and go to Profile tab. Edit the profile attributes for the newly created users to specify the Display name as analyst1 and analyst2 respectively. Do not specify @dsar.com in the Display name field . Again from the left hand side pane, go to Applications -> Applications. Choose Create App Integration and choose SAML 2.0. Click Next. In App name, provide a friendly name for your application. For eg: lfemrapp. Go to next. Enter following values in the next section. Replace youraccountID with your AWS event engine account ID in the two IAM ARNs in the Value column of 3rd row. Be careful to not add any extra trailing whitespaces. # Section Name Value 1 General Single Sign-On URL https://public-dns:8442/gateway/knoxsso/api/v1/websso?pac4jCallback=true&client_name=SAML2Client 2 General Audience URI (SP Entity ID) urn:amazon:webservices 3 Attribute Statements https://aws.amazon.com/SAML/Attributes/Role arn:aws:iam::youraccountID:role/LF-SAML-Role-emr-lf-saml,arn:aws:iam::youraccountID:saml-provider/oktaSAMLProvider 4 Attribute Statements https://aws.amazon.com/SAML/Attributes/RoleSessionName user.displayName 5 Attribute Statements https://glue.amazon.com/SAML/Attributes/UserName user.displayName Filled out form with above values looks like below. Go to Next. Choose \"I'm an Okta customer adding an internal app\". Click Finish. Your application will now be created and will be in active state. Now go the Assignments tab (last tab in the above screenshot) to assign analyst1 and analyst2 to this newly created application. Do not specify @dsar.com in the User Name field when assigning people to the application. . Click on Assign -> Assign to People. Click on Assign next to analyst 1 and analyst 2 users. Remove the @dsar.com for both users. Do not specify @dsar.com in the User Name field when assigning people to the application. . Save and go back. Once both users are in Assigned state, click Done. Now go to Sign On tab. Under Sign On Methods, choose the link location for the IdP metadata file (right-click) and choose Copy Link Address. It should look like: https://dev- .okta.com/app/ /sso/saml/metadata Also, save this metadata to your local desktop. (Right click -> Save File). Type .xml as extension.","title":"Setup Okta IdP"},{"location":"day3/lf/exercise/#set-up-lake-formation","text":"In your AWS Web Console (event engine), go to the AWS Lake Formation Console. Once you go to the console, the screen will look like below. Click Get Started. In left side pane, under Data catalog, choose Settings. For Data catalog settings, deselect the two permissions check-boxes. Choose Save. In left side pane, go to Permissions -> Administrative roles and tasks. In Database creators section, click on Revoke. Under IAM users and roles, type IAMAllowedPrincipals and select it from drop down. Select \"Create Database\" and click \"Revoke\". Now, choose Data catalog -> Databases from the left navigation pane. Choose Create database. Database name: lfoktasamlblogdb For Location, enter S3 path with your account ID replaced -> s3://mrworkshop-youraccountID-dayone/lf/ Click Create Database.","title":"Set up Lake Formation"},{"location":"day3/lf/exercise/#launch-cloudformation-stack","text":"Now, launch the CloudFormation Stack (Right click -> Open Link in New Tab). Provide following parameters. Replace youraccountID with your AWS event engine account ID. Parameter Value StackName emr-lf-saml userBucketName s3://mrworkshop-youraccountID-dayone/ OktaAppMetadataURL metadata URL we copied from the Okta console SAMLProviderName oktaSAMLProvider Realm EC2.INTERNAL KdcAdminPassword Test123$ ReleaseLabel emr-5.31.0 InstanceType c4.xlarge VPCSubnet Choose MMPublicSubnetOne myIPCidr 0.0.0.0/0 oktaUser1 analyst1 oktaUser2 analyst2 EC2KeyPair ee-default-keypair Screenshot of filled values: Click on Next two times. Then check \"I acknowledge that AWS CloudFormation might create IAM resources with custom names.\" and click \"Create Stack\". The stack will take about 15 minutes to get created. Once the stack is created, in the Outputs tab of the Cloud Formation console, make a note of the primary DNS, SAML role ARN, and IdP ARN. Go back to your Okta page. Go to General tab under application. Update the attributes.","title":"Launch Cloudformation Stack"},{"location":"day3/lf/exercise/#validate-the-solution","text":"Launch a web browser in incognito mode and open a Zeppelin notebook using the URL https:// :8442/gateway/default/zeppelin/ (replace with the master node public DNS of the EMR cluster obtained from Cloudformation Output). The Single Sign-On login page appears. Okta validates the login credentials with the system of record, like Active Directory, and returns a SAML, which is parsed and the next page is displayed based on the redirect URL parameter. Log in to Zeppelin as analyst1. After login, choose Create a note and run this SQL statement: spark . sql ( \" select * from lfoktasamlblogdb.taxi_data limit 10 \" ) . show () The following screenshot shows that analyst1 can only see the two columns that you specified in Lake Formation. Open another web browser in incognito mode and log in to Zeppelin as analyst2. The same select query shows all the columns, as shown in the following screenshot.","title":"Validate the solution"},{"location":"day3/ranger/exercise/","text":"Apache Ranger Integration for FGAC \u00b6","title":"**Apache Ranger Integration for FGAC**"},{"location":"day3/ranger/exercise/#apache-ranger-integration-for-fgac","text":"","title":"Apache Ranger Integration for FGAC"},{"location":"day3/serverless/exercise/","text":"Amazon EMR Serverless \u00b6 In this exercise, you will run Hive and Spark applications using EMR Serverless. To run the exercises in this section, you need to use your own AWS account. Since the feature is still in preview (as of writing this), you will not be charged. However, your AWS account should have been whitelisted for using EMR Serverless feature to be able to run these exercises. If your AWS account is not whitelisted yet, you can sign up for preview of this feature using this sign up form. Once your AWS account is whitelisted, you can come back to this exercise and run them. Since you are using your own AWS accounts, you can run this exercise even after the workshop. You will need latest version of AWS CLI and jq installed to run these exercises. Run all the commands in the terminal of your local desktop (or export your AWS credentials and run them in an EC2 instance). Let's first create an IAM role that we are going to use for job submission. Go to IAM Web Console (Right click -> Open Link in New Tab) -> Create Role. Under \"Select Trusted Entity\", choose \"Custom Trust Policy\" and paste the following: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"EMRServerlessTrustPolicy\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"emr-serverless.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] } In the Permissions Policy section, choose a policy that will be used to access resources from your jobs. For time being, you can attach AdministratorAccess managed policy. But it is not recommended to do so other than for testing purpose. Always make sure to give the least privileges possible. In next step, name your role. You can name this role \"sampleJobExecutionRole\". Create the role. Spark on EMR Serverless \u00b6 List the applications. aws emr - serverless list - applications -- region us - east - 1 Create a new Spark application with min and and max limits for vCPU and memory. result = $ ( aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 6.5.0 - preview \\ -- type ' SPARK ' \\ -- initial - capacity '{ \"DRIVER\" : { \"workerCount\" : 5 , \"resourceConfiguration\" : { \"cpu\" : \"2vCPU\" , \"memory\" : \"4GB\" } }, \"EXECUTOR\" : { \"workerCount\" : 50 , \"resourceConfiguration\" : { \"cpu\" : \"4vCPU\" , \"memory\" : \"8GB\" } } }' \\ -- maximum - capacity '{ \"cpu\" : \"400vCPU\" , \"memory\" : \"1024GB\" }' \\ -- name spark - 6.5.0 - demo - application ) echo $result appID = $ ( echo $result | jq - r . applicationId ) Now let's start this application. aws --region us-east-1 emr-serverless start-application \\ --application-id ${ appID } You can get the status of the application with below command. aws --region us-east-1 emr-serverless get-application \\ --application-id ${ appID } The state will be STARTING. It will take about 1-2 minutes for the status to become STARTED. Once the status becomes STARTED, lets submit a job to this application. Get the ARN for the execution role you created. Also, create or use an existing S3 bucket in your own account in the same region where you are running these commands (us-east-1). serverlessArn=$(aws iam get-role --role-name sampleJobExecutionRole | jq -r .'Role | .Arn') s3bucket = 'yours3bucketname' Let's use the following Spark code (same job we ran for EMR on EKS). You don't have to copy this script anywhere. This is just for your reference. import sys from datetime import datetime from pyspark.sql import SparkSession from pyspark.sql import SQLContext from pyspark.sql.functions import * if __name__ == \"__main__\" : print ( len ( sys . argv )) if ( len ( sys . argv ) != 4 ): print ( \"Usage: spark-etl-glue [input-folder] [output-folder] [dbName]\" ) sys . exit ( 0 ) spark = SparkSession \\ . builder \\ . appName ( \"Python Spark SQL Glue integration example\" ) \\ . enableHiveSupport () \\ . getOrCreate () nyTaxi = spark . read . option ( \"inferSchema\" , \"true\" ) . option ( \"header\" , \"true\" ) . csv ( sys . argv [ 1 ]) updatedNYTaxi = nyTaxi . withColumn ( \"current_date\" , lit ( datetime . now ())) updatedNYTaxi . printSchema () print ( updatedNYTaxi . show ()) print ( \"Total number of records: \" + str ( updatedNYTaxi . count ())) updatedNYTaxi . write . parquet ( sys . argv [ 2 ]) updatedNYTaxi . registerTempTable ( \"ny_taxi_table\" ) dbName = sys . argv [ 3 ] spark . sql ( \"CREATE database if not exists \" + dbName ) spark . sql ( \"USE \" + dbName ) spark . sql ( \"CREATE table if not exists ny_taxi_parquet USING PARQUET LOCATION '\" + sys . argv [ 2 ] + \"' AS SELECT * from ny_taxi_table \" ) Submit the job using following command. result=$(echo \"aws --region us-east-1 emr-serverless start-job-run \\ --application-id ${ appID } \\ --execution-role-arn ${ serverlessArn } \\ --job-driver '{ \\\"sparkSubmit\\\": { \\\"entryPoint\\\": \\\"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/spark-etl-glue.py\\\", \\\"entryPointArguments\\\": [ \\\"s3://aws-data-analytics-workshops/shared_datasets/tripdata/\\\",\\\"s3:// $s3bucket /emrserverless/taxi-data-glue/\\\",\\\"tripdata\\\" ], \\\"sparkSubmitParameters\\\": \\\"--conf spark.executor.cores=1 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1\\\" } }' \\ --configuration-overrides '{ \\\"applicationConfiguration\\\": [{ \\\"classification\\\": \\\"spark-defaults\\\", \\\"properties\\\": { \\\"spark.dynamicAllocation.enabled\\\": \\\"false\\\", \\\"spark.hadoop.hive.metastore.client.factory.class\\\": \\\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\\\" } }], \\\"monitoringConfiguration\\\": { \\\"s3MonitoringConfiguration\\\": { \\\"logUri\\\": \\\"s3:// $s3bucket /emrserverless/logs\\\" } } }'\" | bash ) Get the job run ID. jobID=$(echo $result | jq -r .'jobRunId') You can get the status of our job using the following command. aws -- region us - east - 1 emr - serverless get - job - run \\ -- application - id $appID \\ -- job - run - id $jobID You will see the job being scheduled. Wait for the status to go from SCHEDULED to RUNNING to SUCCESS. Now check the S3 location for logs and output. Download the stderr and stdout logs from Spark driver and inspect them. aws s3 cp s3:// $s3bucket /emrserverless/logs/applications/ ${ appID } /jobs/ ${ jobID } /SPARK_DRIVER/stderr.gz . aws s3 cp s3:// $s3bucket /emrserverless/logs/applications/ ${ appID } /jobs/ ${ jobID } /SPARK_DRIVER/stdout.gz . You will see the job output in stdout and driver execution logs in stderr. Similarly, you can check out executor logs as well. Hive on EMR Serverless \u00b6 EMR Serverless supports Hive applications also. Let's start by creating a Hive application. result = $ ( aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 6.5.0 - preview \\ -- initial - capacity '{ \"DRIVER\" : { \"workerCount\" : 5 , \"resourceConfiguration\" : { \"cpu\" : \"2vCPU\" , \"memory\" : \"4GB\" } }, \"TEZ_TASK\" : { \"workerCount\" : 50 , \"resourceConfiguration\" : { \"cpu\" : \"4vCPU\" , \"memory\" : \"8GB\" } } }' \\ -- maximum - capacity '{ \"cpu\" : \"400vCPU\" , \"memory\" : \"1024GB\" }' \\ -- type ' HIVE ' \\ -- name hive - 6.5.0 - demo - application ) echo $result appID = $ ( echo $result | jq - r . applicationId ) Start the application. aws --region us-east-1 emr-serverless start-application \\ --application-id ${ appID } Get application status. aws --region us-east-1 emr-serverless get-application \\ --application-id ${ appID } Once the application status becomes STARTED, submit a Hive job. For this, create a file called \"hive-query.ql\" with following contents: create database if not exists emrserverless ; use emrserverless ; create table if not exists test_table ( id int ) ; drop table if exists Values__Tmp__Table__1 ; insert into test_table values ( 1 ) , ( 2 ) , ( 2 ) , ( 3 ) , ( 3 ) , ( 3 ) ; select id , count ( id ) from test_table group by id order by id desc ; Upload this file to an S3 location. You can use the same S3 bucket with a different prefix. aws s3 cp hive-query.ql s3://$s3bucket/emrserverless/scripts/hive/ Now let's submit a Hive job to this application. result=$(echo \"aws --region us-east-1 emr-serverless start-job-run \\ --application-id ${ appID } \\ --execution-role-arn ${ serverlessArn } \\ --job-driver '{ \\\"hive\\\": { \\\"query\\\": \\\"s3:// $s3bucket /emr-serverless-hive/query/hive-query.ql\\\", \\\"parameters\\\": \\\"--hiveconf hive.root.logger=DEBUG,DRFA\\\" } }' \\ --configuration-overrides '{ \\\"applicationConfiguration\\\": [{ \\\"classification\\\": \\\"hive-site\\\", \\\"properties\\\": { \\\"hive.exec.scratchdir\\\": \\\"s3:// $s3bucket /emr-serverless-hive/hive/scratch\\\", \\\"hive.metastore.warehouse.dir\\\": \\\"s3:// $s3bucket /emr-serverless-hive/hive/warehouse\\\", \\\"hive.driver.cores\\\": \\\"2\\\", \\\"hive.driver.memory\\\": \\\"4g\\\", \\\"hive.tez.container.size\\\": \\\"4096\\\", \\\"hive.tez.cpu.vcores\\\": \\\"1\\\" } } ], \\\"monitoringConfiguration\\\": { \\\"s3MonitoringConfiguration\\\": { \\\"logUri\\\": \\\"s3:// $s3bucket /emrserverless-hive/logs\\\" } } }'\" | bash ) Get the job run ID. jobID=$(echo $result | jq -r .'jobRunId') You can get the status of our job using the following command. aws -- region us - east - 1 emr - serverless get - job - run \\ -- application - id $appID \\ -- job - run - id $jobID Look at the S3 stdout for results. Check the table in Glue catalog. Changing minor and major versions \u00b6 Let's use a different Hive version for our application. Create a new application with release label 5.34.0-preview. result = $ ( aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 5.34.0 - preview \\ -- initial - capacity '{ \"DRIVER\" : { \"workerCount\" : 5 , \"resourceConfiguration\" : { \"cpu\" : \"2vCPU\" , \"memory\" : \"4GB\" } }, \"TEZ_TASK\" : { \"workerCount\" : 50 , \"resourceConfiguration\" : { \"cpu\" : \"4vCPU\" , \"memory\" : \"8GB\" } } }' \\ -- maximum - capacity '{ \"cpu\" : \"400vCPU\" , \"memory\" : \"1024GB\" }' \\ -- type ' HIVE ' \\ -- name hive - 6.5.0 - demo - application ) echo $result appID = $ ( echo $result | jq - r . applicationId ) Now let's submit the same Hive job to this application. result=$(echo \"aws --region us-east-1 emr-serverless start-job-run \\ --application-id ${ appID } \\ --execution-role-arn ${ serverlessArn } \\ --job-driver '{ \\\"hive\\\": { \\\"query\\\": \\\"s3:// $s3bucket /emr-serverless-hive/query/hive-query.ql\\\", \\\"parameters\\\": \\\"--hiveconf hive.root.logger=DEBUG,DRFA\\\" } }' \\ --configuration-overrides '{ \\\"applicationConfiguration\\\": [{ \\\"classification\\\": \\\"hive-site\\\", \\\"properties\\\": { \\\"hive.exec.scratchdir\\\": \\\"s3:// $s3bucket /emr-serverless-hive/hive/scratch\\\", \\\"hive.metastore.warehouse.dir\\\": \\\"s3:// $s3bucket /emr-serverless-hive/hive/warehouse\\\", \\\"hive.driver.cores\\\": \\\"2\\\", \\\"hive.driver.memory\\\": \\\"4g\\\", \\\"hive.tez.container.size\\\": \\\"4096\\\", \\\"hive.tez.cpu.vcores\\\": \\\"1\\\" } } ], \\\"monitoringConfiguration\\\": { \\\"s3MonitoringConfiguration\\\": { \\\"logUri\\\": \\\"s3:// $s3bucket /emrserverless-hive/logs\\\" } } }'\" | bash ) Get the job run ID. jobID=$(echo $result | jq -r .'jobRunId') You can get the status of our job using the following command. aws -- region us - east - 1 emr - serverless get - job - run \\ -- application - id $appID \\ -- job - run - id $jobID Once it finishes, check the stdout log in S3 for results and check the Glue catalog.","title":"2 - Amazon EMR Serverless"},{"location":"day3/serverless/exercise/#amazon-emr-serverless","text":"In this exercise, you will run Hive and Spark applications using EMR Serverless. To run the exercises in this section, you need to use your own AWS account. Since the feature is still in preview (as of writing this), you will not be charged. However, your AWS account should have been whitelisted for using EMR Serverless feature to be able to run these exercises. If your AWS account is not whitelisted yet, you can sign up for preview of this feature using this sign up form. Once your AWS account is whitelisted, you can come back to this exercise and run them. Since you are using your own AWS accounts, you can run this exercise even after the workshop. You will need latest version of AWS CLI and jq installed to run these exercises. Run all the commands in the terminal of your local desktop (or export your AWS credentials and run them in an EC2 instance). Let's first create an IAM role that we are going to use for job submission. Go to IAM Web Console (Right click -> Open Link in New Tab) -> Create Role. Under \"Select Trusted Entity\", choose \"Custom Trust Policy\" and paste the following: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"EMRServerlessTrustPolicy\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"emr-serverless.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] } In the Permissions Policy section, choose a policy that will be used to access resources from your jobs. For time being, you can attach AdministratorAccess managed policy. But it is not recommended to do so other than for testing purpose. Always make sure to give the least privileges possible. In next step, name your role. You can name this role \"sampleJobExecutionRole\". Create the role.","title":"Amazon EMR Serverless"},{"location":"day3/serverless/exercise/#spark-on-emr-serverless","text":"List the applications. aws emr - serverless list - applications -- region us - east - 1 Create a new Spark application with min and and max limits for vCPU and memory. result = $ ( aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 6.5.0 - preview \\ -- type ' SPARK ' \\ -- initial - capacity '{ \"DRIVER\" : { \"workerCount\" : 5 , \"resourceConfiguration\" : { \"cpu\" : \"2vCPU\" , \"memory\" : \"4GB\" } }, \"EXECUTOR\" : { \"workerCount\" : 50 , \"resourceConfiguration\" : { \"cpu\" : \"4vCPU\" , \"memory\" : \"8GB\" } } }' \\ -- maximum - capacity '{ \"cpu\" : \"400vCPU\" , \"memory\" : \"1024GB\" }' \\ -- name spark - 6.5.0 - demo - application ) echo $result appID = $ ( echo $result | jq - r . applicationId ) Now let's start this application. aws --region us-east-1 emr-serverless start-application \\ --application-id ${ appID } You can get the status of the application with below command. aws --region us-east-1 emr-serverless get-application \\ --application-id ${ appID } The state will be STARTING. It will take about 1-2 minutes for the status to become STARTED. Once the status becomes STARTED, lets submit a job to this application. Get the ARN for the execution role you created. Also, create or use an existing S3 bucket in your own account in the same region where you are running these commands (us-east-1). serverlessArn=$(aws iam get-role --role-name sampleJobExecutionRole | jq -r .'Role | .Arn') s3bucket = 'yours3bucketname' Let's use the following Spark code (same job we ran for EMR on EKS). You don't have to copy this script anywhere. This is just for your reference. import sys from datetime import datetime from pyspark.sql import SparkSession from pyspark.sql import SQLContext from pyspark.sql.functions import * if __name__ == \"__main__\" : print ( len ( sys . argv )) if ( len ( sys . argv ) != 4 ): print ( \"Usage: spark-etl-glue [input-folder] [output-folder] [dbName]\" ) sys . exit ( 0 ) spark = SparkSession \\ . builder \\ . appName ( \"Python Spark SQL Glue integration example\" ) \\ . enableHiveSupport () \\ . getOrCreate () nyTaxi = spark . read . option ( \"inferSchema\" , \"true\" ) . option ( \"header\" , \"true\" ) . csv ( sys . argv [ 1 ]) updatedNYTaxi = nyTaxi . withColumn ( \"current_date\" , lit ( datetime . now ())) updatedNYTaxi . printSchema () print ( updatedNYTaxi . show ()) print ( \"Total number of records: \" + str ( updatedNYTaxi . count ())) updatedNYTaxi . write . parquet ( sys . argv [ 2 ]) updatedNYTaxi . registerTempTable ( \"ny_taxi_table\" ) dbName = sys . argv [ 3 ] spark . sql ( \"CREATE database if not exists \" + dbName ) spark . sql ( \"USE \" + dbName ) spark . sql ( \"CREATE table if not exists ny_taxi_parquet USING PARQUET LOCATION '\" + sys . argv [ 2 ] + \"' AS SELECT * from ny_taxi_table \" ) Submit the job using following command. result=$(echo \"aws --region us-east-1 emr-serverless start-job-run \\ --application-id ${ appID } \\ --execution-role-arn ${ serverlessArn } \\ --job-driver '{ \\\"sparkSubmit\\\": { \\\"entryPoint\\\": \\\"s3://aws-data-analytics-workshops/emr-eks-workshop/scripts/spark-etl-glue.py\\\", \\\"entryPointArguments\\\": [ \\\"s3://aws-data-analytics-workshops/shared_datasets/tripdata/\\\",\\\"s3:// $s3bucket /emrserverless/taxi-data-glue/\\\",\\\"tripdata\\\" ], \\\"sparkSubmitParameters\\\": \\\"--conf spark.executor.cores=1 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1\\\" } }' \\ --configuration-overrides '{ \\\"applicationConfiguration\\\": [{ \\\"classification\\\": \\\"spark-defaults\\\", \\\"properties\\\": { \\\"spark.dynamicAllocation.enabled\\\": \\\"false\\\", \\\"spark.hadoop.hive.metastore.client.factory.class\\\": \\\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\\\" } }], \\\"monitoringConfiguration\\\": { \\\"s3MonitoringConfiguration\\\": { \\\"logUri\\\": \\\"s3:// $s3bucket /emrserverless/logs\\\" } } }'\" | bash ) Get the job run ID. jobID=$(echo $result | jq -r .'jobRunId') You can get the status of our job using the following command. aws -- region us - east - 1 emr - serverless get - job - run \\ -- application - id $appID \\ -- job - run - id $jobID You will see the job being scheduled. Wait for the status to go from SCHEDULED to RUNNING to SUCCESS. Now check the S3 location for logs and output. Download the stderr and stdout logs from Spark driver and inspect them. aws s3 cp s3:// $s3bucket /emrserverless/logs/applications/ ${ appID } /jobs/ ${ jobID } /SPARK_DRIVER/stderr.gz . aws s3 cp s3:// $s3bucket /emrserverless/logs/applications/ ${ appID } /jobs/ ${ jobID } /SPARK_DRIVER/stdout.gz . You will see the job output in stdout and driver execution logs in stderr. Similarly, you can check out executor logs as well.","title":"Spark on EMR Serverless"},{"location":"day3/serverless/exercise/#hive-on-emr-serverless","text":"EMR Serverless supports Hive applications also. Let's start by creating a Hive application. result = $ ( aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 6.5.0 - preview \\ -- initial - capacity '{ \"DRIVER\" : { \"workerCount\" : 5 , \"resourceConfiguration\" : { \"cpu\" : \"2vCPU\" , \"memory\" : \"4GB\" } }, \"TEZ_TASK\" : { \"workerCount\" : 50 , \"resourceConfiguration\" : { \"cpu\" : \"4vCPU\" , \"memory\" : \"8GB\" } } }' \\ -- maximum - capacity '{ \"cpu\" : \"400vCPU\" , \"memory\" : \"1024GB\" }' \\ -- type ' HIVE ' \\ -- name hive - 6.5.0 - demo - application ) echo $result appID = $ ( echo $result | jq - r . applicationId ) Start the application. aws --region us-east-1 emr-serverless start-application \\ --application-id ${ appID } Get application status. aws --region us-east-1 emr-serverless get-application \\ --application-id ${ appID } Once the application status becomes STARTED, submit a Hive job. For this, create a file called \"hive-query.ql\" with following contents: create database if not exists emrserverless ; use emrserverless ; create table if not exists test_table ( id int ) ; drop table if exists Values__Tmp__Table__1 ; insert into test_table values ( 1 ) , ( 2 ) , ( 2 ) , ( 3 ) , ( 3 ) , ( 3 ) ; select id , count ( id ) from test_table group by id order by id desc ; Upload this file to an S3 location. You can use the same S3 bucket with a different prefix. aws s3 cp hive-query.ql s3://$s3bucket/emrserverless/scripts/hive/ Now let's submit a Hive job to this application. result=$(echo \"aws --region us-east-1 emr-serverless start-job-run \\ --application-id ${ appID } \\ --execution-role-arn ${ serverlessArn } \\ --job-driver '{ \\\"hive\\\": { \\\"query\\\": \\\"s3:// $s3bucket /emr-serverless-hive/query/hive-query.ql\\\", \\\"parameters\\\": \\\"--hiveconf hive.root.logger=DEBUG,DRFA\\\" } }' \\ --configuration-overrides '{ \\\"applicationConfiguration\\\": [{ \\\"classification\\\": \\\"hive-site\\\", \\\"properties\\\": { \\\"hive.exec.scratchdir\\\": \\\"s3:// $s3bucket /emr-serverless-hive/hive/scratch\\\", \\\"hive.metastore.warehouse.dir\\\": \\\"s3:// $s3bucket /emr-serverless-hive/hive/warehouse\\\", \\\"hive.driver.cores\\\": \\\"2\\\", \\\"hive.driver.memory\\\": \\\"4g\\\", \\\"hive.tez.container.size\\\": \\\"4096\\\", \\\"hive.tez.cpu.vcores\\\": \\\"1\\\" } } ], \\\"monitoringConfiguration\\\": { \\\"s3MonitoringConfiguration\\\": { \\\"logUri\\\": \\\"s3:// $s3bucket /emrserverless-hive/logs\\\" } } }'\" | bash ) Get the job run ID. jobID=$(echo $result | jq -r .'jobRunId') You can get the status of our job using the following command. aws -- region us - east - 1 emr - serverless get - job - run \\ -- application - id $appID \\ -- job - run - id $jobID Look at the S3 stdout for results. Check the table in Glue catalog.","title":"Hive on EMR Serverless"},{"location":"day3/serverless/exercise/#changing-minor-and-major-versions","text":"Let's use a different Hive version for our application. Create a new application with release label 5.34.0-preview. result = $ ( aws -- region us - east - 1 emr - serverless create - application \\ -- release - label emr - 5.34.0 - preview \\ -- initial - capacity '{ \"DRIVER\" : { \"workerCount\" : 5 , \"resourceConfiguration\" : { \"cpu\" : \"2vCPU\" , \"memory\" : \"4GB\" } }, \"TEZ_TASK\" : { \"workerCount\" : 50 , \"resourceConfiguration\" : { \"cpu\" : \"4vCPU\" , \"memory\" : \"8GB\" } } }' \\ -- maximum - capacity '{ \"cpu\" : \"400vCPU\" , \"memory\" : \"1024GB\" }' \\ -- type ' HIVE ' \\ -- name hive - 6.5.0 - demo - application ) echo $result appID = $ ( echo $result | jq - r . applicationId ) Now let's submit the same Hive job to this application. result=$(echo \"aws --region us-east-1 emr-serverless start-job-run \\ --application-id ${ appID } \\ --execution-role-arn ${ serverlessArn } \\ --job-driver '{ \\\"hive\\\": { \\\"query\\\": \\\"s3:// $s3bucket /emr-serverless-hive/query/hive-query.ql\\\", \\\"parameters\\\": \\\"--hiveconf hive.root.logger=DEBUG,DRFA\\\" } }' \\ --configuration-overrides '{ \\\"applicationConfiguration\\\": [{ \\\"classification\\\": \\\"hive-site\\\", \\\"properties\\\": { \\\"hive.exec.scratchdir\\\": \\\"s3:// $s3bucket /emr-serverless-hive/hive/scratch\\\", \\\"hive.metastore.warehouse.dir\\\": \\\"s3:// $s3bucket /emr-serverless-hive/hive/warehouse\\\", \\\"hive.driver.cores\\\": \\\"2\\\", \\\"hive.driver.memory\\\": \\\"4g\\\", \\\"hive.tez.container.size\\\": \\\"4096\\\", \\\"hive.tez.cpu.vcores\\\": \\\"1\\\" } } ], \\\"monitoringConfiguration\\\": { \\\"s3MonitoringConfiguration\\\": { \\\"logUri\\\": \\\"s3:// $s3bucket /emrserverless-hive/logs\\\" } } }'\" | bash ) Get the job run ID. jobID=$(echo $result | jq -r .'jobRunId') You can get the status of our job using the following command. aws -- region us - east - 1 emr - serverless get - job - run \\ -- application - id $appID \\ -- job - run - id $jobID Once it finishes, check the stdout log in S3 for results and check the Glue catalog.","title":"Changing minor and major versions"}]}