
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.1.11">
    
    
      
        <title>1 - Apache Hudi on Amazon EMR - Amazon EMR Train-The-Trainer Workshop</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.50e68009.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#apache-hudi-on-amazon-emr" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Amazon EMR Train-The-Trainer Workshop" class="md-header__button md-logo" aria-label="Amazon EMR Train-The-Trainer Workshop" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Amazon EMR Train-The-Trainer Workshop
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              1 - Apache Hudi on Amazon EMR
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/vasveena/amazon-emr-ttt-workshop/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    vasveena/amazon-emr-ttt-workshop
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Amazon EMR Train-The-Trainer Workshop" class="md-nav__button md-logo" aria-label="Amazon EMR Train-The-Trainer Workshop" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Amazon EMR Train-The-Trainer Workshop
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/vasveena/amazon-emr-ttt-workshop/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    vasveena/amazon-emr-ttt-workshop
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Introduction
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../setup/" class="md-nav__link">
        Setup
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Part 1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Part 1" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Part 1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../day1/fleet/exercise/" class="md-nav__link">
        1 - Launching EMR cluster with Instance Fleets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../day1/spark/exercise/" class="md-nav__link">
        2 - Apache Spark on Amazon EMR
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../day1/step/exercise/" class="md-nav__link">
        3 - Orchestrate EMR Steps using AWS Step Functions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Part 2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Part 2" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Part 2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../day1/studio/exercise/" class="md-nav__link">
        1 - Amazon EMR Studio
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../day1/mwaa/exercise/" class="md-nav__link">
        2 - Orchestration Notebook Pipelines using Amazon MWAA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../smstudio/exercise/" class="md-nav__link">
        3 - Sagemaker Studio Integration
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Part 3
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Part 3" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Part 3
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_1" type="checkbox" id="__nav_5_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_5_1">
          Build Incremental Data Lakes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Build Incremental Data Lakes" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_1">
          <span class="md-nav__icon md-icon"></span>
          Build Incremental Data Lakes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          1 - Apache Hudi on Amazon EMR
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        1 - Apache Hudi on Amazon EMR
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#apache-hudi-with-spark-datasource-apis" class="md-nav__link">
    Apache Hudi with Spark Datasource APIs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#apache-hudi-with-sparksql-dmls" class="md-nav__link">
    Apache Hudi with SparkSQL DMLs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#apache-hudi-with-spark-deltastreamer" class="md-nav__link">
    Apache Hudi with Spark Deltastreamer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#change-data-capture-with-hudi-deltastreamer" class="md-nav__link">
    Change Data Capture with Hudi Deltastreamer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#apache-hudi-with-spark-structured-streaming" class="md-nav__link">
    Apache Hudi with Spark Structured Streaming
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../icebrg/exercise/" class="md-nav__link">
        2 - Apache Iceberg on Amazon EMR
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          Part 4
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Part 4" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Part 4
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_1" type="checkbox" id="__nav_6_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_1">
          Deployment Options
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Deployment Options" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_1">
          <span class="md-nav__icon md-icon"></span>
          Deployment Options
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../day3/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../day3/emroneks/exercise/" class="md-nav__link">
        1 - Amazon EMR on EKS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../day3/serverless/exercise/" class="md-nav__link">
        2 - Amazon EMR Serverless
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#apache-hudi-with-spark-datasource-apis" class="md-nav__link">
    Apache Hudi with Spark Datasource APIs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#apache-hudi-with-sparksql-dmls" class="md-nav__link">
    Apache Hudi with SparkSQL DMLs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#apache-hudi-with-spark-deltastreamer" class="md-nav__link">
    Apache Hudi with Spark Deltastreamer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#change-data-capture-with-hudi-deltastreamer" class="md-nav__link">
    Change Data Capture with Hudi Deltastreamer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#apache-hudi-with-spark-structured-streaming" class="md-nav__link">
    Apache Hudi with Spark Structured Streaming
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
<a href="https://github.com/vasveena/amazon-emr-ttt-workshop/edit/master/docs/day2/hudi/exercise.md" title="Edit this page" class="md-content__button md-icon">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
</a>


<h1 id="apache-hudi-on-amazon-emr"><strong>Apache Hudi on Amazon EMR</strong><a class="headerlink" href="#apache-hudi-on-amazon-emr" title="Permanent link">&para;</a></h1>
<p>In this exercise you will build incremental data lakes on EMR using Apache Hudi. You can build data lakes using Apache Hudi using Spark Datasource APIs, Hudi Deltastreamer utility and SparkSQL. You will also build a real-time live incremental data lake with Spark Structured Streaming + Amazon Managed Streaming for Apache Kafka (MSK) + Apache Hudi.</p>
<p>In the previous EMR Studio exercise, we linked the Git repository in the Jupyter interface. We will continue to use the same repository to run these exercises.</p>
<p>SSH into the EMR leader node of the cluster "EMR-Spark-Hive-Presto" or open a session using AWS Session Manager for the EMR leader node since we will be running a few commands directly on the leader node.</p>
<h3 id="apache-hudi-with-spark-datasource-apis">Apache Hudi with Spark Datasource APIs<a class="headerlink" href="#apache-hudi-with-spark-datasource-apis" title="Permanent link">&para;</a></h3>
<p>Open the file workshop-repo -&gt; files -&gt; notebook -&gt; apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb in the Jupyter. Make sure the Kernel is set to PySpark.</p>
<p><img alt="Hudi - 1" src="../images/hudi-1.png" /></p>
<p>All the instructions required to run the notebook are within the notebook itself.</p>
<p>Download the file workshop-repo -&gt; schema -&gt; schema.avsc to your local desktop and upload this file into the following S3 location (replace "youraccountID" with your event engine AWS account ID): <em>s3://mrworkshop-youraccountID-dayone/schema/schema.avsc</em></p>
<p><img alt="Hudi - 2" src="../images/hudi-2.png" /></p>
<p>Alternatively, you can run the following commands from the leader node of your EMR cluster. Replace "youraccountID" with your event engine AWS account ID. We will be using this schema AVRO file to run compaction on Merge-On-Read tables.</p>
<div class="codehilite"><pre><span></span><code>sudo su hadoop
cd ~
curl -o schema.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/schema.avsc
aws s3 cp schema.avsc s3://mrworkshop-youraccountID-dayone/schema/schema.avsc
</code></pre></div>

<p>Run the blocks of the notebook "apache-hudi-on-amazon-emr-datasource-pyspark-demo.ipynb". Replace "youraccountID" in the S3 paths within the notebook with your AWS event engine account ID.</p>
<h3 id="apache-hudi-with-sparksql-dmls">Apache Hudi with SparkSQL DMLs<a class="headerlink" href="#apache-hudi-with-sparksql-dmls" title="Permanent link">&para;</a></h3>
<p>From EMR 6.5.0, you can write Hudi datasets using simple SQL statements. Let's look at an example.</p>
<p>From the EMR Studio workspace Jupyterlab session, go to workshop-repo -&gt; files -&gt; notebook -&gt; apache-hudi-on-amazon-emr-dml.ipynb. Run all the blocks of this notebook. Replace "youraccountID" in the S3 paths within the notebook with your AWS event engine account ID.</p>
<p>Detailed instructions are within the notebook.</p>
<h3 id="apache-hudi-with-spark-deltastreamer">Apache Hudi with Spark Deltastreamer<a class="headerlink" href="#apache-hudi-with-spark-deltastreamer" title="Permanent link">&para;</a></h3>
<p>Hudi provides a utility called Deltastreamer for creating and manipulating Hudi datasets without the need to write any Spark code. For this activity, let us copy a few files to the S3 location. Run the following commands in your EMR leader node session created using Session Manager or SSH.</p>
<div class="codehilite"><pre><span></span><code>sudo su hadoop
cd ~
curl -o source-schema-json.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/source-schema-json.avsc
curl -o target-schema-json.avsc https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/schema/target-schema-json.avsc
curl -o json-deltastreamer.properties https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/properties/json-deltastreamer.properties
curl -o json-deltastreamer_upsert.properties https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/properties/json-deltastreamer_upsert.properties
curl -o apache-hudi-on-amazon-emr-deltastreamer-python-demo.py https://raw.githubusercontent.com/vasveena/amazon-emr-ttt-workshop/main/files/script/apache-hudi-on-amazon-emr-deltastreamer-python-demo.py
</code></pre></div>

<p>Replace youraccountID with event engine AWS account ID in the files json-deltastreamer.properties, json-deltastreamer_upsert.properties and apache-hudi-on-amazon-emr-deltastreamer-python-demo.py. You can do so using sed command below. Replace 707263692290 with your event engine account ID.</p>
<div class="codehilite"><pre><span></span><code>sed -i &#39;s|youraccountID|707263692290|g&#39; json-deltastreamer.properties
sed -i &#39;s|youraccountID|707263692290|g&#39; json-deltastreamer_upsert.properties
sed -i &#39;s|youraccountID|707263692290|g&#39; apache-hudi-on-amazon-emr-deltastreamer-python-demo.py
</code></pre></div>

<p>Now, copy the four files to your S3 location. Replace youraccountID with event engine AWS account ID.</p>
<div class="codehilite"><pre><span></span><code>aws s3 cp source-schema-json.avsc s3://mrworkshop-youraccountID-dayone/hudi-ds/config/
aws s3 cp target-schema-json.avsc s3://mrworkshop-youraccountID-dayone/hudi-ds/config/
aws s3 cp json-deltastreamer.properties s3://mrworkshop-youraccountID-dayone/hudi-ds/config/
aws s3 cp json-deltastreamer_upsert.properties s3://mrworkshop-youraccountID-dayone/hudi-ds/config/
</code></pre></div>

<p>Now let's generate some Fake data for the purpose of this workshop. We will use <a href="https://faker.readthedocs.io/en/master/">Faker</a> library for that. Install Faker with the below command.</p>
<div class="codehilite"><pre><span></span><code>pip3 install Faker
pip3 install boto3
</code></pre></div>

<p>Run the Python program to generate Fake data under respective S3 locations. This takes a few minutes to complete.</p>
<div class="codehilite"><pre><span></span><code>python3 apache-hudi-on-amazon-emr-deltastreamer-python-demo.py
</code></pre></div>

<p>Once done, make sure the inputdata and update prefixes are populated with JSON data files. You can copy one file using “aws s3 cp” on the EMR leader node session to inspect the data. Replace youraccountID with event engine AWS account ID.</p>
<div class="codehilite"><pre><span></span><code>aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds/inputdata
aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds/updates
</code></pre></div>

<p><img alt="Hudi - 3" src="../images/hudi-3.png" /></p>
<p>Copy the Hudi utilities bundle to HDFS.</p>
<div class="codehilite"><pre><span></span><code>hadoop fs -copyFromLocal /usr/lib/hudi/hudi-utilities-bundle.jar hdfs:///user/hadoop/
</code></pre></div>

<p>Let's submit DeltaStreamer step to the EMR cluster. You can submit this step on EC2 JumpHost or leader node of EMR cluster "EMR-Spark-Hive-Presto". Since we have the EMR leader node session active, let us use it to run the command.</p>
<p>Modify Add Steps Command for Bulk Insert Operation. Change the --cluster-id's value to your EMR cluster "EMR-Spark-Hive-Presto" cluster ID (Obtained from AWS Management Console -&gt; Amazon EMR Console -&gt; EMR-Spark-Hive-Presto -&gt; Summary tab. Looks like j-XXXXXXXXX). Replace youraccountID with event engine AWS account ID.</p>
<div class="codehilite"><pre><span></span><code><span class="n">aws</span><span class="w"> </span><span class="n">emr</span><span class="w"> </span><span class="n">add</span><span class="o">-</span><span class="n">steps</span><span class="w"> </span><span class="o">--</span><span class="n">cluster</span><span class="o">-</span><span class="n">id</span><span class="w"> </span><span class="n">j</span><span class="o">-</span><span class="n">XXXXXXXXX</span><span class="w"> </span><span class="o">--</span><span class="n">steps</span><span class="w"> </span><span class="n">Type</span><span class="o">=</span><span class="n">Spark</span><span class="p">,</span><span class="n">Name</span><span class="o">=</span><span class="s">&quot;Deltastreamer COW - Bulk Insert&quot;</span><span class="p">,</span><span class="n">ActionOnFailure</span><span class="o">=</span><span class="n">CONTINUE</span><span class="p">,</span><span class="n">Args</span><span class="o">=</span><span class="p">[</span><span class="o">--</span><span class="n">jars</span><span class="p">,</span><span class="nl">hdfs:</span><span class="c1">///user/hadoop/*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///user/hadoop/hudi-utilities-bundle.jar,--props,s3://mrworkshop-youraccountID-dayone/hudi-ds/config/json-deltastreamer.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://mrworkshop-707263692290-dayone/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,BULK_INSERT] --region us-east-1</span>
</code></pre></div>

<p><img alt="Hudi - 4" src="../images/hudi-4.png" /></p>
<p>You will get an EMR Step ID in return. You will see the corresponding Hudi Deltastreamer step being submitted to your cluster (AWS Management Console -&gt; Amazon EMR Console -&gt; EMR-Spark-Hive-Presto -&gt; Steps). It will take about 2 minutes to complete.</p>
<p><img alt="Hudi - 5" src="../images/hudi-5.png" /></p>
<p>Check the S3 location for Hudi files. Replace youraccountID with event engine AWS account ID.</p>
<div class="codehilite"><pre><span></span><code>aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1/
</code></pre></div>

<p><img alt="Hudi - 6" src="../images/hudi-6.png" /></p>
<p>Let's go to the hive CLI on EMR leader node by typing "hive". Let's run the following command to create a table. Replace youraccountID with event engine AWS account ID.</p>
<div class="codehilite"><pre><span></span><code><span class="k">CREATE</span><span class="w"> </span><span class="n">EXTERNAL</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n n-Quoted">`profile_cow`</span><span class="p">(</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`_hoodie_commit_time`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`_hoodie_commit_seqno`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`_hoodie_record_key`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`_hoodie_partition_path`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`_hoodie_file_name`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`Name`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`phone`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`job`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`company`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`ssn`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`street_address`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`dob`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`email`</span><span class="w"> </span><span class="k">string</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n n-Quoted">`ts`</span><span class="w"> </span><span class="k">string</span><span class="p">)</span><span class="w"></span>
<span class="k">ROW</span><span class="w"> </span><span class="k">FORMAT</span><span class="w"> </span><span class="n">SERDE</span><span class="w"></span>
<span class="w">  </span><span class="s1">&#39;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe&#39;</span><span class="w"></span>
<span class="k">STORED</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">INPUTFORMAT</span><span class="w"></span>
<span class="w">  </span><span class="s1">&#39;org.apache.hudi.hadoop.HoodieParquetInputFormat&#39;</span><span class="w"></span>
<span class="n">OUTPUTFORMAT</span><span class="w"></span>
<span class="w">  </span><span class="s1">&#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat&#39;</span><span class="w"></span>
<span class="n">LOCATION</span><span class="w"></span>
<span class="w">  </span><span class="s1">&#39;s3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1/&#39;</span><span class="p">;</span><span class="w"></span>
</code></pre></div>

<p>Select a record from this table and copy the value of hoodie_record_key and street_address to a notepad.</p>
<div class="codehilite"><pre><span></span><code><span class="k">select</span><span class="w"> </span><span class="n n-Quoted">`_hoodie_commit_time`</span><span class="p">,</span><span class="w"> </span><span class="n n-Quoted">`_hoodie_record_key`</span><span class="p">,</span><span class="w"> </span><span class="n">street_address</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">profile_cow</span><span class="w"> </span><span class="k">limit</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"></span>
</code></pre></div>

<p><img alt="Hudi - 7" src="../images/hudi-7.png" /></p>
<p>Exit from hive.</p>
<div class="codehilite"><pre><span></span><code><span class="k">exit</span><span class="c1">;</span>
</code></pre></div>

<p>Now, let's do upsert operation with Hudi Deltastreamer. Change the --cluster-id's value to your EMR cluster "EMR-Spark-Hive-Presto" cluster ID (Obtained from AWS Management Console -&gt; Amazon EMR Console -&gt; EMR-Spark-Hive-Presto -&gt; Summary tab. Looks like j-XXXXXXXXX). Replace youraccountID with event engine AWS account ID.</p>
<div class="codehilite"><pre><span></span><code><span class="n">aws</span><span class="w"> </span><span class="n">emr</span><span class="w"> </span><span class="n">add</span><span class="o">-</span><span class="n">steps</span><span class="w"> </span><span class="o">--</span><span class="n">cluster</span><span class="o">-</span><span class="n">id</span><span class="w"> </span><span class="n">j</span><span class="o">-</span><span class="n">XXXXXXXXX</span><span class="w"> </span><span class="o">--</span><span class="n">steps</span><span class="w"> </span><span class="n">Type</span><span class="o">=</span><span class="n">Spark</span><span class="p">,</span><span class="n">Name</span><span class="o">=</span><span class="s">&quot;Deltastreamer COW - Upsert&quot;</span><span class="p">,</span><span class="n">ActionOnFailure</span><span class="o">=</span><span class="n">CONTINUE</span><span class="p">,</span><span class="n">Args</span><span class="o">=</span><span class="p">[</span><span class="o">--</span><span class="n">jars</span><span class="p">,</span><span class="nl">hdfs:</span><span class="c1">///user/hadoop/*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///user/hadoop/hudi-utilities-bundle.jar,--props,s3://mrworkshop-youraccountID-dayone/hudi-ds/config/json-deltastreamer_upsert.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,UPSERT] --region us-east-1</span>
</code></pre></div>

<p><img alt="Hudi - 9" src="../images/hudi-9.png" /></p>
<p>You will get an EMR Step ID in return. You will see the corresponding Hudi Deltastreamer step being submitted to your cluster (AWS Management Console -&gt; Amazon EMR Console -&gt; EMR-Spark-Hive-Presto -&gt; Steps). Wait for the step to complete (~1 minute).</p>
<p><img alt="Hudi - 8" src="../images/hudi-8.png" /></p>
<p>Let us check the street_address for the same _hoodie_record_key. Run the following query in hive CLI on the EMR leader node. Replace value of "_hoodie_record_key" in the where clause with the one you obtained from previous select query.</p>
<div class="codehilite"><pre><span></span><code><span class="k">select</span><span class="w"> </span><span class="n n-Quoted">`_hoodie_commit_time`</span><span class="p">,</span><span class="w"> </span><span class="n">street_address</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">profile_cow</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="n n-Quoted">`_hoodie_record_key`</span><span class="o">=</span><span class="s1">&#39;00000b94-1500-4f10-bd10-d6393ba24643&#39;</span><span class="p">;</span><span class="w"></span>
</code></pre></div>

<p>Notice the change in commit time and street_address.</p>
<p><img alt="Hudi - 10" src="../images/hudi-10.png" /></p>
<h3 id="change-data-capture-with-hudi-deltastreamer">Change Data Capture with Hudi Deltastreamer<a class="headerlink" href="#change-data-capture-with-hudi-deltastreamer" title="Permanent link">&para;</a></h3>
<p>Go to RDS Web console and open the database that was created. Copy the endpoint of this database.</p>
<p><img alt="Hudi - 13" src="../images/hudi-13.png" /></p>
<p>Login to your EC2 JumpHost using Session Manager or SSH and run the following command to connect to your DB. Replace "dbendpoint" value with the endpoint you copied from the RDS console.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">sudo</span><span class="w"> </span><span class="nv">yum</span><span class="w"> </span><span class="nv">install</span><span class="w"> </span><span class="o">-</span><span class="nv">y</span><span class="w"> </span><span class="nv">mysql</span><span class="w"></span>
<span class="nv">mysql</span><span class="w"> </span><span class="o">-</span><span class="nv">h</span><span class="w"> </span><span class="nv">dbendpoint</span><span class="w"> </span><span class="o">-</span><span class="nv">uadmin</span><span class="w"> </span><span class="o">-</span><span class="nv">pTest123</span><span class="p">$</span><span class="w"></span>
</code></pre></div>

<p><img alt="Hudi - 14" src="../images/hudi-14.png" /></p>
<p>Once you are logged in to your database, run the following commands in the MySQL session to create a DB table.</p>
<div class="codehilite"><pre><span></span><code><span class="n">call</span><span class="w"> </span><span class="n">mysql</span><span class="o">.</span><span class="n">rds_set_configuration</span><span class="p">(</span><span class="s1">&#39;binlog retention hours&#39;</span><span class="p">,</span><span class="w"> </span><span class="mi">24</span><span class="p">);</span><span class="w"></span>

<span class="n">create</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">dev</span><span class="o">.</span><span class="n">retail_transactions</span><span class="p">(</span><span class="w"></span>
<span class="n">tran_id</span><span class="w"> </span><span class="n">INT</span><span class="p">,</span><span class="w"></span>
<span class="n">tran_date</span><span class="w"> </span><span class="n">DATE</span><span class="p">,</span><span class="w"></span>
<span class="n">store_id</span><span class="w"> </span><span class="n">INT</span><span class="p">,</span><span class="w"></span>
<span class="n">store_city</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w"></span>
<span class="n">store_state</span><span class="w"> </span><span class="nb">char</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span><span class="w"></span>
<span class="n">item_code</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w"></span>
<span class="n">quantity</span><span class="w"> </span><span class="n">INT</span><span class="p">,</span><span class="w"></span>
<span class="n">total</span><span class="w"> </span><span class="n">FLOAT</span><span class="p">);</span><span class="w"></span>
</code></pre></div>

<p>Once the table is created, run the below queries to insert data into this table.</p>
<div class="codehilite"><pre><span></span><code>insert into dev.retail_transactions values(1,&#39;2019-03-17&#39;,1,&#39;CHICAGO&#39;,&#39;IL&#39;,&#39;XXXXXX&#39;,5,106.25);
insert into dev.retail_transactions values(2,&#39;2019-03-16&#39;,2,&#39;NEW YORK&#39;,&#39;NY&#39;,&#39;XXXXXX&#39;,6,116.25);
insert into dev.retail_transactions values(3,&#39;2019-03-15&#39;,3,&#39;SPRINGFIELD&#39;,&#39;IL&#39;,&#39;XXXXXX&#39;,7,126.25);
insert into dev.retail_transactions values(4,&#39;2019-03-17&#39;,4,&#39;SAN FRANCISCO&#39;,&#39;CA&#39;,&#39;XXXXXX&#39;,8,136.25);
insert into dev.retail_transactions values(5,&#39;2019-03-11&#39;,1,&#39;CHICAGO&#39;,&#39;IL&#39;,&#39;XXXXXX&#39;,9,146.25);
insert into dev.retail_transactions values(6,&#39;2019-03-18&#39;,1,&#39;CHICAGO&#39;,&#39;IL&#39;,&#39;XXXXXX&#39;,10,156.25);
insert into dev.retail_transactions values(7,&#39;2019-03-14&#39;,2,&#39;NEW YORK&#39;,&#39;NY&#39;,&#39;XXXXXX&#39;,11,166.25);
insert into dev.retail_transactions values(8,&#39;2019-03-11&#39;,1,&#39;CHICAGO&#39;,&#39;IL&#39;,&#39;XXXXXX&#39;,12,176.25);
insert into dev.retail_transactions values(9,&#39;2019-03-10&#39;,4,&#39;SAN FRANCISCO&#39;,&#39;CA&#39;,&#39;XXXXXX&#39;,13,186.25);
insert into dev.retail_transactions values(10,&#39;2019-03-13&#39;,1,&#39;CHICAGO&#39;,&#39;IL&#39;,&#39;XXXXXX&#39;,14,196.25);
insert into dev.retail_transactions values(11,&#39;2019-03-14&#39;,5,&#39;CHICAGO&#39;,&#39;IL&#39;,&#39;XXXXXX&#39;,15,106.25);
insert into dev.retail_transactions values(12,&#39;2019-03-15&#39;,6,&#39;CHICAGO&#39;,&#39;IL&#39;,&#39;XXXXXX&#39;,16,116.25);
insert into dev.retail_transactions values(13,&#39;2019-03-16&#39;,7,&#39;CHICAGO&#39;,&#39;IL&#39;,&#39;XXXXXX&#39;,17,126.25);
insert into dev.retail_transactions values(14,&#39;2019-03-16&#39;,7,&#39;CHICAGO&#39;,&#39;IL&#39;,&#39;XXXXXX&#39;,17,126.25);
commit;
</code></pre></div>

<p>We will now use AWS DMS to start pushing this data to S3.</p>
<p>Go to the DMS Web Console -&gt; Endpoints -&gt; hudidmsource. Check if the connection is successful. If not, test the connection again.</p>
<p><img alt="Hudi - 16" src="../images/hudi-16.png" /></p>
<p>Start the Database migration task hudiload.</p>
<p><img alt="Hudi - 15" src="../images/hudi-15.png" /></p>
<p>Once the task state changes from Running to "Load complete, replication ongoing", check the below S3 location for deposited files. Replace youraccountID with AWS event engine account ID.</p>
<div class="codehilite"><pre><span></span><code>aws s3 ls s3://mrworkshop-dms-youraccountID-dayone/dmsdata/dev/retail_transactions/
</code></pre></div>

<p>Now login to the EMR leader node of the cluster "EMR-Spark-Hive-Presto" using Session Manager or SSH and run the following commands. Replace youraccountID with AWS event engine account ID.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">sudo</span> <span class="nv">su</span> <span class="nv">hadoop</span>
<span class="nv">cd</span> <span class="o">~</span>
<span class="nv">aws</span> <span class="nv">s3</span> <span class="nv">mv</span> <span class="nv">s3</span>:<span class="o">//</span><span class="nv">mrworkshop</span><span class="o">-</span><span class="nv">dms</span><span class="o">-</span><span class="nv">youraccountID</span><span class="o">-</span><span class="nv">dayone</span><span class="o">/</span><span class="nv">dmsdata</span><span class="o">/</span><span class="nv">dev</span><span class="o">/</span><span class="nv">retail_transactions</span><span class="o">/</span> <span class="nv">s3</span>:<span class="o">//</span><span class="nv">mrworkshop</span><span class="o">-</span><span class="nv">dms</span><span class="o">-</span><span class="nv">youraccountID</span><span class="o">-</span><span class="nv">dayone</span><span class="o">/</span><span class="nv">dmsdata</span><span class="o">/</span><span class="nv">data</span><span class="o">-</span><span class="nv">full</span><span class="o">/</span><span class="nv">dev</span><span class="o">/</span><span class="nv">retail_transactions</span><span class="o">/</span>  <span class="o">--</span><span class="nv">exclude</span> <span class="s2">&quot;</span><span class="s">*</span><span class="s2">&quot;</span> <span class="o">--</span><span class="k">include</span> <span class="s2">&quot;</span><span class="s">LOAD*.parquet</span><span class="s2">&quot;</span> <span class="o">--</span><span class="nv">recursive</span>
</code></pre></div>

<p>With the full table dump available in the data-full S3 folder, we will now use the Hudi Deltastreamer utility on the EMR cluster to populate the Hudi dataset on S3.</p>
<p>Run the following command directly on leader node. Replace youraccountID with AWS event engine account ID.</p>
<div class="codehilite"><pre><span></span><code><span class="n">spark</span><span class="o">-</span><span class="n">submit</span><span class="w"> </span><span class="o">--</span><span class="k">class</span><span class="w"> </span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hudi</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">deltastreamer</span><span class="o">.</span><span class="n">HoodieDeltaStreamer</span><span class="w">  </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">jars</span><span class="w"> </span><span class="n">hdfs</span><span class="p">:</span><span class="o">///</span><span class="n">user</span><span class="o">/</span><span class="n">hadoop</span><span class="o">/*.</span><span class="n">jar</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="k">master</span><span class="w"> </span><span class="n">yarn</span><span class="w"> </span><span class="o">--</span><span class="n">deploy</span><span class="o">-</span><span class="n">mode</span><span class="w"> </span><span class="n">client</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">conf</span><span class="w"> </span><span class="n">spark</span><span class="o">.</span><span class="n">serializer</span><span class="o">=</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">serializer</span><span class="o">.</span><span class="n">KryoSerializer</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">conf</span><span class="w"> </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">hive</span><span class="o">.</span><span class="n">convertMetastoreParquet</span><span class="o">=</span><span class="bp">false</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">hudi</span><span class="o">/</span><span class="n">hudi</span><span class="o">-</span><span class="n">utilities</span><span class="o">-</span><span class="n">bundle</span><span class="o">.</span><span class="n">jar</span><span class="w">  </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">table</span><span class="o">-</span><span class="n">type</span><span class="w"> </span><span class="n">COPY_ON_WRITE</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">source</span><span class="o">-</span><span class="n">ordering</span><span class="o">-</span><span class="n">field</span><span class="w"> </span><span class="n">dms_received_ts</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">props</span><span class="w"> </span><span class="n">s3</span><span class="p">:</span><span class="o">//</span><span class="n">mrworkshop</span><span class="o">-</span><span class="n">dms</span><span class="o">-</span><span class="n">youraccountID</span><span class="o">-</span><span class="n">dayone</span><span class="o">/</span><span class="n">properties</span><span class="o">/</span><span class="n">dfs</span><span class="o">-</span><span class="n">source</span><span class="o">-</span><span class="n">retail</span><span class="o">-</span><span class="n">transactions</span><span class="o">-</span><span class="n">full</span><span class="o">.</span><span class="n">properties</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">source</span><span class="o">-</span><span class="k">class</span><span class="w"> </span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hudi</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">sources</span><span class="o">.</span><span class="n">ParquetDFSSource</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">path</span><span class="w"> </span><span class="n">s3</span><span class="p">:</span><span class="o">//</span><span class="n">mrworkshop</span><span class="o">-</span><span class="n">dms</span><span class="o">-</span><span class="n">youraccountID</span><span class="o">-</span><span class="n">dayone</span><span class="o">/</span><span class="n">hudi</span><span class="o">/</span><span class="n">retail_transactions</span><span class="w"> </span><span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">table</span><span class="w"> </span><span class="n">hudiblogdb</span><span class="o">.</span><span class="n">retail_transactions</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">transformer</span><span class="o">-</span><span class="k">class</span><span class="w"> </span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hudi</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">SqlQueryBasedTransformer</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">payload</span><span class="o">-</span><span class="k">class</span><span class="w"> </span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hudi</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">AWSDmsAvroPayload</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">schemaprovider</span><span class="o">-</span><span class="k">class</span><span class="w"> </span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hudi</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">schema</span><span class="o">.</span><span class="n">FilebasedSchemaProvider</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">enable</span><span class="o">-</span><span class="n">hive</span><span class="o">-</span><span class="n">sync</span><span class="w"></span>
</code></pre></div>

<p>Once this job completes, check the Hudi table by logging into Spark SQL or Athena console.</p>
<div class="codehilite"><pre><span></span><code>spark-sql --conf &quot;spark.serializer=org.apache.spark.serializer.KryoSerializer&quot; --conf &quot;spark.sql.hive.convertMetastoreParquet=false&quot; --jars hdfs:///user/hadoop/*.jar  
</code></pre></div>

<p>Run the query:</p>
<div class="codehilite"><pre><span></span><code>select * from hudiblogdb.retail_transactions order by tran_id
</code></pre></div>

<p>You should see the same data in the table as the MySQL database with a few columns added by Hudi deltastreamer.</p>
<p><img alt="Hudi - 19" src="../images/hudi-19.png" /></p>
<p>Now let's run some DML statements on our MySQL database and take these changes through to the Hudi dataset. Run the following commands in MySQL session from your EC2 JumpHost.</p>
<div class="codehilite"><pre><span></span><code>insert into dev.retail_transactions values(15,&#39;2022-03-22&#39;,7,&#39;CHICAGO&#39;,&#39;IL&#39;,&#39;XXXXXX&#39;,17,126.25);
update dev.retail_transactions set store_city=&#39;SPRINGFIELD&#39; where tran_id=12;
delete from dev.retail_transactions where tran_id=2;
commit;
</code></pre></div>

<p>Exit from the MySQL session. In a few minutes, you see a new .parquet file created under s3://mrworkshop-dms-youraccountID-dayone/dmsdata/dev/retail_transactions/ folder in the S3 bucket. CDC data is being captured by our DMS replication task. You can see the changes in the DMS replication task under "Table statistics".</p>
<p><img alt="Hudi - 18" src="../images/hudi-18.png" /></p>
<p>Now, lets take the incremental changes we made to Hudi. Run the following command on EMR leader node. Replace youraccountID with your AWS event engine account ID.</p>
<div class="codehilite"><pre><span></span><code><span class="n">spark</span><span class="o">-</span><span class="n">submit</span><span class="w"> </span><span class="o">--</span><span class="k">class</span><span class="w"> </span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hudi</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">deltastreamer</span><span class="o">.</span><span class="n">HoodieDeltaStreamer</span><span class="w">  </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">jars</span><span class="w"> </span><span class="n">hdfs</span><span class="p">:</span><span class="o">///</span><span class="n">user</span><span class="o">/</span><span class="n">hadoop</span><span class="o">/*.</span><span class="n">jar</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="k">master</span><span class="w"> </span><span class="n">yarn</span><span class="w"> </span><span class="o">--</span><span class="n">deploy</span><span class="o">-</span><span class="n">mode</span><span class="w"> </span><span class="n">client</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">conf</span><span class="w"> </span><span class="n">spark</span><span class="o">.</span><span class="n">serializer</span><span class="o">=</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">serializer</span><span class="o">.</span><span class="n">KryoSerializer</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">conf</span><span class="w"> </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">hive</span><span class="o">.</span><span class="n">convertMetastoreParquet</span><span class="o">=</span><span class="bp">false</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">hudi</span><span class="o">/</span><span class="n">hudi</span><span class="o">-</span><span class="n">utilities</span><span class="o">-</span><span class="n">bundle</span><span class="o">.</span><span class="n">jar</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">table</span><span class="o">-</span><span class="n">type</span><span class="w"> </span><span class="n">COPY_ON_WRITE</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">source</span><span class="o">-</span><span class="n">ordering</span><span class="o">-</span><span class="n">field</span><span class="w"> </span><span class="n">dms_received_ts</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">props</span><span class="w"> </span><span class="n">s3</span><span class="p">:</span><span class="o">//</span><span class="n">mrworkshop</span><span class="o">-</span><span class="n">dms</span><span class="o">-</span><span class="n">youraccountID</span><span class="o">-</span><span class="n">dayone</span><span class="o">/</span><span class="n">properties</span><span class="o">/</span><span class="n">dfs</span><span class="o">-</span><span class="n">source</span><span class="o">-</span><span class="n">retail</span><span class="o">-</span><span class="n">transactions</span><span class="o">-</span><span class="n">incremental</span><span class="o">.</span><span class="n">properties</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">source</span><span class="o">-</span><span class="k">class</span><span class="w"> </span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hudi</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">sources</span><span class="o">.</span><span class="n">ParquetDFSSource</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">path</span><span class="w"> </span><span class="n">s3</span><span class="p">:</span><span class="o">//</span><span class="n">mrworkshop</span><span class="o">-</span><span class="n">dms</span><span class="o">-</span><span class="n">youraccountID</span><span class="o">-</span><span class="n">dayone</span><span class="o">/</span><span class="n">hudi</span><span class="o">/</span><span class="n">retail_transactions</span><span class="w"> </span><span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">table</span><span class="w"> </span><span class="n">hudiblogdb</span><span class="o">.</span><span class="n">retail_transactions</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">transformer</span><span class="o">-</span><span class="k">class</span><span class="w"> </span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hudi</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">SqlQueryBasedTransformer</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">payload</span><span class="o">-</span><span class="k">class</span><span class="w"> </span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hudi</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">AWSDmsAvroPayload</span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">schemaprovider</span><span class="o">-</span><span class="k">class</span><span class="w"> </span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hudi</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">schema</span><span class="o">.</span><span class="n">FilebasedSchemaProvider</span><span class="w"> </span>\<span class="w"></span>
<span class="w">  </span><span class="o">--</span><span class="n">enable</span><span class="o">-</span><span class="n">hive</span><span class="o">-</span><span class="n">sync</span><span class="w"> </span>\<span class="w"></span>
<span class="o">--</span><span class="n">checkpoint</span><span class="w"> </span><span class="mi">0</span><span class="w"></span>
</code></pre></div>

<p>Once finished, check the Hudi table by logging into Spark SQL or Athena console.</p>
<div class="codehilite"><pre><span></span><code>spark-sql --conf &quot;spark.serializer=org.apache.spark.serializer.KryoSerializer&quot; --conf &quot;spark.sql.hive.convertMetastoreParquet=false&quot; --jars hdfs:///user/hadoop/*.jar  
</code></pre></div>

<p>Run the query:</p>
<div class="codehilite"><pre><span></span><code>select * from hudiblogdb.retail_transactions order by tran_id
</code></pre></div>

<p>You should see the changes you made in the MySQL table.</p>
<h3 id="apache-hudi-with-spark-structured-streaming">Apache Hudi with Spark Structured Streaming<a class="headerlink" href="#apache-hudi-with-spark-structured-streaming" title="Permanent link">&para;</a></h3>
<p>This exercise will show how you can write real time Hudi data sets using Spark Structured Streaming. For this exercise, we will use real-time NYC Metro Subway data using <a href="https://api.mta.info/#/landing">MTA API</a>.</p>
<p>Keep the EMR Session Manager or SSH session active. In a new browser tab, create a new SSM session for EC2 instance "JumpHost" (or SSH into EC2 instance "JumpHost"). i.e., under the <a href="https://console.aws.amazon.com/ec2/home?region=us-east-1#Instances:">EC2 console</a> select the EC2 instance with name "JumpHost". Click on "Connect" -&gt; Session Manager -&gt; Connect.</p>
<p>Switch to EC2 user and go to home directory</p>
<div class="codehilite"><pre><span></span><code>sudo su ec2-user
cd ~
</code></pre></div>

<p>Run the following commands in EC2 instance to get the values of ZookeeperConnectString and BootstrapBrokerString.</p>
<div class="codehilite"><pre><span></span><code>clusterArn=`aws kafka list-clusters --region us-east-1 | jq &#39;.ClusterInfoList[0].ClusterArn&#39;`
echo <span class="nv">$clusterArn</span>
bs=$(echo &quot;aws kafka get-bootstrap-brokers --cluster-arn <span class="cp">${</span><span class="n">clusterArn</span><span class="cp">}</span> --region us-east-1&quot;  | bash | jq &#39;.BootstrapBrokerString&#39;)
bs_update=$(echo <span class="nv">$bs</span> | sed &quot;s|,|\&#39;,\&#39;|g&quot; | sed &quot;s|\&quot;|&#39;|g&quot;)
zs=$(echo &quot;aws kafka describe-cluster --cluster-arn <span class="nv">$clusterArn</span>&quot; --region us-east-1 | bash | jq &#39;.ClusterInfo.ZookeeperConnectString&#39;)
</code></pre></div>

<p>Create two Kafka topics.</p>
<div class="codehilite"><pre><span></span><code>echo &quot;/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-topics.sh --create --zookeeper $zs --replication-factor 3 --partitions 1 --topic trip_update_topic&quot; | bash

echo &quot;/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-topics.sh --create --zookeeper $zs --replication-factor 3 --partitions 1 --topic trip_status_topic&quot; | bash
</code></pre></div>

<p>Install packages required by Kafka client on the JumpHost instance session (via SSH or AWS SSM).</p>
<div class="codehilite"><pre><span></span><code>pip3 install protobuf
pip3 install kafka-python
pip3 install --upgrade gtfs-realtime-bindings
pip3 install underground
pip3 install pathlib
pip3 install requests
</code></pre></div>

<p>Run the below command to modify the bootstrap servers in the file train_arrival_producer.py on the JumpHost's /home/ec2-user/ directory. Change</p>
<div class="codehilite"><pre><span></span><code>sudo sed -i &quot;s|&#39;bootstrapserverstring&#39;|$bs_update|g&quot; /home/ec2-user/train_arrival_producer.py
</code></pre></div>

<p>Export API key on the same session.</p>
<div class="codehilite"><pre><span></span><code><span class="k">export</span><span class="w"> </span><span class="n">MTA_API_KEY</span><span class="o">=</span><span class="n">UskS0iAsK06DtSffbgqNi8hlDvApPR833wydQAHG</span><span class="w"></span>
</code></pre></div>

<p>Run the Kafka producer client and terminate the process using Ctrl + C after 10 seconds.</p>
<div class="codehilite"><pre><span></span><code>python3 train_arrival_producer.py
</code></pre></div>

<p>You can verify that the Kafka topics are being written to using the following commands</p>
<div class="codehilite"><pre><span></span><code>echo &quot;/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-console-consumer.sh --bootstrap-server $bs --topic trip_update_topic --from-beginning&quot; | bash
</code></pre></div>

<p>After a few seconds exit using Ctrl + C.</p>
<div class="codehilite"><pre><span></span><code>echo &quot;/home/ec2-user/kafka/kafka_2.12-2.2.1/bin/kafka-console-consumer.sh --bootstrap-server $bs --topic trip_status_topic --from-beginning&quot; | bash
</code></pre></div>

<p>After a few seconds exit using Ctrl + C.</p>
<p>Now let's configure Spark consumer on EMR leader node using Session Manager or SSH. 9). SSH into the leader node of EMR cluster "EMR-Spark-Hive-Presto" (or use AWS Session Manager). Download Spark dependencies in EMR leader node session.</p>
<div class="codehilite"><pre><span></span><code>sudo su hadoop
cd ~

cd /usr/lib/spark/jars
sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.12/3.0.1/spark-streaming-kafka-0-10_2.12-3.0.1.jar
sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.1/spark-sql-kafka-0-10_2.12-3.0.1.jar
sudo wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.2.1/kafka-clients-2.2.1.jar
sudo wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.12/3.0.1/spark-streaming-kafka-0-10-assembly_2.12-3.0.1.jar
sudo wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar
</code></pre></div>

<p>In same session, provide all access to all HDFS folders. You can scope access per user if desired.</p>
<div class="codehilite"><pre><span></span><code>hdfs dfs -chmod 777 /
</code></pre></div>

<p>Switch to the SSH/SSM session of EC2 instance “JumpHost”. Get the bootstrap string from the below command on the EC2 JumpHost session.</p>
<div class="codehilite"><pre><span></span><code>echo $bs
</code></pre></div>

<p>Example output:
"b-3.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092,b-1.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092,b-2.mskcluster.i9x8j1.c4.kafka.us-east-1.amazonaws.com:9092"</p>
<p>Copy your bootstrap servers output to a notepad.</p>
<p>Run the Kafka producer program again and keep it running.</p>
<div class="codehilite"><pre><span></span><code>python3 train_arrival_producer.py
</code></pre></div>

<p>Now, go to the EMR Studio's JupyterLab workspace and open workshop-repo/files/notebook/amazon-emr-spark-streaming-apache-hudi-demo.ipynb. Make sure that "Spark" kernel is selected.</p>
<p>Replace the broker string with the value of bootstrap servers you copied to your notepad. Replace youraccountID with event engine AWS account ID. Instructions are in the notebook.</p>
<p>Once you have made the changes, run all the cell blocks. Spark streaming job runs every 30 seconds. You can increase the duration if you want to. Based on the time of the day you run this code, the results may vary. After sometime, query this table using hive or even Athena.</p>
<p>Go to the AWS Web Console -&gt; Athena -&gt; Explore the query editor. Since this would be your first time using Athena console, you need to go to the Settings -&gt; Manage and add your Query result location like -&gt; s3://mrworkshop-youraccountID-dayone/athena/.</p>
<p><img alt="Hudi - 11" src="../images/hudi-11.png" /></p>
<p>Now, you can run the following queries once in every 2 minutes or so to see the live changes. You can also build live dashboards using Amazon Quicksight.</p>
<div class="codehilite"><pre><span></span><code>select count(*) from hudi_trips_streaming_table;

select tripId, numoffuturestops from hudi_trips_streaming_table;
</code></pre></div>

<p>After inspecting, stop your Python Kafka producer on the EC2 JumpHost session using Ctrl + C.</p>
<p>On EMR Studio workspace session, click on the stop icon <img alt="Hudi - 12" src="../images/hudi-12.png" /> to stop the Spark Structured Streaming job.</p>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../../smstudio/exercise/" class="md-footer__link md-footer__link--prev" aria-label="Previous: 3 - Sagemaker Studio Integration" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              3 - Sagemaker Studio Integration
            </div>
          </div>
        </a>
      
      
        
        <a href="../../icebrg/exercise/" class="md-footer__link md-footer__link--next" aria-label="Next: 2 - Apache Iceberg on Amazon EMR" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              2 - Apache Iceberg on Amazon EMR
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["tabs"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../../assets/javascripts/workers/search.092fa1f6.min.js"}</script>
    
    
      <script src="../../../assets/javascripts/bundle.5a9542cf.min.js"></script>
      
    
  </body>
</html>