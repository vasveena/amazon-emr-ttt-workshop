# Hudi shuffle properties
hoodie.upsert.shuffle.parallelism=10
hoodie.insert.shuffle.parallelism=10
hoodie.bulkinsert.shuffle.parallelism=10

# Key generator props
hoodie.datasource.write.recordkey.field=id

hoodie.datasource.hive_sync.enable=true

# Extractor class for partitioned dataset
#hoodie.datasource.write.partitionpath.field=dob
#hoodie.datasource.hive_sync.partition_fields=year,month
#hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor
#hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.ComplexKeyGenerator


# Extractor class for Non-partitioned dataset
hoodie.datasource.write.partitionpath.field=company
hoodie.datasource.hive_sync.partition_fields=company
hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.keygen.NonPartitionedExtractor
hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.NonpartitionedKeyGenerator

hoodie.deltastreamer.source.dfs.root=s3://mrworkshop-youraccountID-dayone/hudi-ds/updates

hoodie.parquet.compression.ratio=0.5
hoodie.parquet.max.file.size=120
hoodie.parquet.small.file.limit=10

# Schema provider props (change to absolute path based on your installation)
hoodie.deltastreamer.schemaprovider.source.schema.file=s3://mrworkshop-youraccountID-dayone/hudi-ds/config/source-schema-json.avsc
hoodie.deltastreamer.schemaprovider.target.schema.file=s3://mrworkshop-youraccountID-dayone/hudi-ds/config/target-schema-json.avsc
