{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Apache Hudi Deltastreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Apache Hudi Deltastreamer\n",
    "HoodieDeltaStreamer utility is part of hudi-utilities-bundle that provides a way to ingest data from sources such as DFS or Kafka.\n",
    "\n",
    "In this notebook, you will learn to use DeltaStreamer Utility to bulk insert data into a Hudi Dataset as a Copy on Write(CoW) table and perform batch upsert. \n",
    "\n",
    "We will run queries in hudi-cli and SparkSQL to verify the tables and subsequent updates are incorporated into our datalake on Amazon S3\n",
    "\n",
    "Let's get started !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Faker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Profile Generator\n",
    "\n",
    "Fake profile generator uses Python's Faker [https://faker.readthedocs.io/en/master/index.html] library. Let's define a method to generate a number of random person profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import boto3\n",
    "import io\n",
    "from io import StringIO\n",
    "from faker import Faker\n",
    "from faker.providers import date_time, credit_card\n",
    "from json import dumps\n",
    "\n",
    "\n",
    "# Intialize Faker library and S3 client\n",
    "fake = Faker() \n",
    "fake.add_provider(date_time)\n",
    "fake.add_provider(credit_card)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Write the fake profile data to a S3 bucket\n",
    "# Replace with your own bucket\n",
    "s3_bucket = \"mrworkshop-youraccountID-dayone\"\n",
    "s3_load_prefix = 'hudi-ds/inputdata/'\n",
    "s3_update_prefix = 'hudi-ds/updates/'\n",
    "\n",
    "# Number of records in each file and number of files\n",
    "# Adjust per your need - this produces 40MB files\n",
    "#num_records = 150000\n",
    "#num_files = 50\n",
    "\n",
    "num_records = 10000\n",
    "num_files = 15\n",
    "\n",
    "def generate_bulk_data():\n",
    "    '''\n",
    "    Generates bulk profile data\n",
    "    '''\n",
    "    # Generate number of files equivalent to num_files\n",
    "    for i in range (num_files):\n",
    "        fake_profile_data = fake_profile_generator(num_records, fake)\n",
    "        fakeIO = StringIO()\n",
    "        filename = 'profile_' + str(i + 1) + '.json'\n",
    "        s3key = s3_load_prefix + filename \n",
    "        fakeIO.write(str(''.join(dumps_lines(fake_profile_data))))\n",
    "        s3object = s3.Object(s3_bucket, s3key)\n",
    "        s3object.put(Body=(bytes(fakeIO.getvalue().encode('UTF-8'))))\n",
    "        fakeIO.close()\n",
    "\n",
    "def generate_updates():\n",
    "    '''\n",
    "    Generates updates for the profiles\n",
    "    '''\n",
    "    #\n",
    "    # We will make updates to records in randomly picked files\n",
    "    #\n",
    "    random_file_list = []\n",
    "    for i in range (1, num_files):\n",
    "        random_file_list.append('profile_' + str(i) + '.json')\n",
    "    for f in random_file_list:\n",
    "        #print(f)\n",
    "        s3key = s3_load_prefix + f\n",
    "        obj = s3.Object(s3_bucket, s3key)\n",
    "        profile_data = obj.get()['Body'].read().decode('utf-8')\n",
    "        #s3_profile_list = json.loads(profile_data)\n",
    "        stringIO_data = io.StringIO(profile_data)\n",
    "        data = stringIO_data.readlines()\n",
    "        #Its time to use json module now.\n",
    "        json_data = list(map(json.loads, data))\n",
    "        fakeIO = StringIO()\n",
    "        s3key = s3_update_prefix + f\n",
    "        fake_profile_data = []\n",
    "        for rec in json_data:\n",
    "            # Let's generate a new address\n",
    "            #print (\"old address: \" + rec['street_address'])\n",
    "            rec['street_address'] = fake.address()\n",
    "            #print (\"new address: \" + rec['street_address'])\n",
    "            fake_profile_data.append(rec)       \n",
    "        fakeIO.write(str(''.join(dumps_lines(fake_profile_data))))\n",
    "        s3object = s3.Object(s3_bucket, s3key)\n",
    "        s3object.put(Body=(bytes(fakeIO.getvalue().encode('UTF-8'))))\n",
    "        fakeIO.close()\n",
    "\n",
    "def fake_profile_generator(length, fake, new_address=\"\"):\n",
    "    \"\"\"\n",
    "    Generates fake profiles\n",
    "    \"\"\"\n",
    "    for x in range (length):       \n",
    "        yield {'Name': fake.name(),\n",
    "               'phone': fake.phone_number(),\n",
    "               'job': fake.job(),\n",
    "               'company': fake.company(),\n",
    "               'ssn': fake.ssn(),\n",
    "               'street_address': (new_address if new_address else fake.address()),\n",
    "               'dob': (fake.date_of_birth(tzinfo=None, minimum_age=21, maximum_age=105).isoformat()),\n",
    "               'email': fake.email(),\n",
    "               'ts': (fake.date_time_between(start_date='-10y', end_date='now', tzinfo=None).isoformat()),\n",
    "               'credit_card': fake.credit_card_number(),\n",
    "               'record_id': fake.pyint(),\n",
    "               'id': fake.uuid4()}\n",
    "        \n",
    "def dumps_lines(objs):\n",
    "    for obj in objs:\n",
    "        yield json.dumps(obj, separators=(',',':')) + '\\n'   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the data generator\n",
    "\n",
    "Following code kicks off the fake data generator to produce files each with certain records (configurable) in JSON format. The files are written to a specified S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_bulk_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the generated data:\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds/inputdata/\n",
    "2022-03-01 06:42:36    3685908 profile_1.json\n",
    "2022-03-01 06:44:18    3685807 profile_10.json\n",
    "2022-03-01 06:44:30    3684892 profile_11.json\n",
    "2022-03-01 06:44:42    3684254 profile_12.json\n",
    "2022-03-01 06:44:53    3684155 profile_13.json\n",
    "2022-03-01 06:45:05    3685178 profile_14.json\n",
    "2022-03-01 06:45:16    3685062 profile_15.json\n",
    "2022-03-01 06:42:47    3683295 profile_2.json\n",
    "2022-03-01 06:42:58    3686567 profile_3.json\n",
    "2022-03-01 06:43:10    3683613 profile_4.json\n",
    "2022-03-01 06:43:21    3686654 profile_5.json\n",
    "2022-03-01 06:43:32    3685491 profile_6.json\n",
    "2022-03-01 06:43:44    3683970 profile_7.json\n",
    "2022-03-01 06:43:55    3685578 profile_8.json\n",
    "2022-03-01 06:44:06    3685117 profile_9.json\n",
    "```\n",
    "\n",
    "## Copy Hudi Libraries on the EMR Cluster and create Hive table\n",
    "\n",
    "0. For the following steps to work, you should have launched the EMR cluster with appropriate permissions set for **Systems Manager Session Manager** \n",
    "1. From the AWS Console, type SSM in the search box and navigate to the **Amazon System Manager console**\n",
    "2. On the left hand side, select **Session Manager** from **Instances and Nodes** section\n",
    "3. Click on the start session and you should see two EC2 instances listed \n",
    "4. Select instance-id of the **EMR's Master** Node and click on **Start session**\n",
    "5. From the terminal type the following to change to user *ec2-user*\n",
    " \n",
    "```bash\n",
    "sh-4.2$ sudo su hadoop\n",
    "hadoop@ip-10-0-2-73 /]$ cd\n",
    "hdfs dfs -mkdir -p /apps/hudi/lib\n",
    "hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar /apps/hudi/lib/spark-avro.jar\n",
    "hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-utilities-bundle.jar /apps/hudi/lib/hudi-utilities-bundle.jar\n",
    "hdfs dfs -copyFromLocal /usr/lib/spark/jars/httpclient-4.5.9.jar /apps/hudi/lib/httpclient-4.5.9.jar\n",
    "hdfs dfs -copyFromLocal /usr/lib/spark/jars/httpcore-4.4.11.jar /apps/hudi/lib/httpcore-4.4.11.jar\n",
    "hdfs dfs -ls /apps/hudi/lib/\n",
    "Found 5 items\n",
    "-rw-r--r--   1 hadoop hadoop     774384 2021-10-11 02:51 /apps/hudi/lib/httpclient-4.5.9.jar\n",
    "-rw-r--r--   1 hadoop hadoop     326874 2021-10-11 02:51 /apps/hudi/lib/httpcore-4.4.11.jar\n",
    "-rw-r--r--   1 hadoop hadoop   35041795 2021-10-11 02:51 /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "-rw-r--r--   1 hadoop hadoop   39996793 2021-10-11 02:51 /apps/hudi/lib/hudi-utilities-bundle.jar\n",
    "-rw-r--r--   1 hadoop hadoop     161984 2021-10-11 02:51 /apps/hudi/lib/spark-avro.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DeltaStreamer to write a Copy on Write (COW) table\n",
    "\n",
    "We will now run the DeltaStreamer utility as an EMR Step to write the above JSON formatted data into a Hudi dataset. To do that, we will need the following:\n",
    "\n",
    "* Properties file on localfs or dfs, with configurations for Hudi client, schema provider, key generator and data source \n",
    "* Schema file for source dataset\n",
    "* Schema file for target dataset\n",
    "\n",
    "To run DeltaStreamer\n",
    "\n",
    "```\n",
    "! ~/.local/bin/aws emr add-steps --cluster-id j-1GMG9EJ4Z4ZL0 --steps Type=Spark,Name=\"Deltastreamer COW - Bulk Insert\",ActionOnFailure=CONTINUE,Args=[--jars,hdfs:///apps/hudi/lib/*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///apps/hudi/lib/hudi-utilities-bundle.jar,--props,s3://my-bucket/hudi-ds/config/json-deltastreamer.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://my-bucket/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,BULK_INSERT] --region us-east-1\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Replace the following values in the above command in the text editor\n",
    "\n",
    "1. --cluster-id with the value you got from previous step\n",
    "2. For --props value replace xxxx part in hudi-workshop-xxxx with the S3 bucket name \n",
    "3. For -- target-base-path value with the S3 bucket name\n",
    "4. After replacing the values, copy the entire commmand and run it in the next cell\n",
    "5. If the values are replaced correctly, you should see a step id displayed as the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install awscli --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ~/.local/bin/aws emr add-steps --cluster-id j-1GMG9EJ4Z4ZL0 --steps Type=Spark,Name=\"Deltastreamer COW - Bulk Insert\",ActionOnFailure=CONTINUE,Args=[--jars,hdfs:///apps/hudi/lib/*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///apps/hudi/lib/hudi-utilities-bundle.jar,--props,s3://mrworkshop-youraccountID-dayone/hudi-ds/config/json-deltastreamer.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,BULK_INSERT] --region us-east-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Hudi Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check the S3 path:\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1/\n",
    "                           PRE .hoodie/\n",
    "2022-03-01 06:49:05          0 .hoodie_$folder$\n",
    "2022-03-01 06:49:27         93 .hoodie_partition_metadata\n",
    "2022-03-01 06:49:30    2488921 15aaf95c-38c1-4889-9987-cdc0e8e7f913-0_7-4-39_20220301064915.parquet\n",
    "2022-03-01 06:49:31    2259709 55662f83-a4b3-4278-b5d0-4176c1146ac7-0_0-4-32_20220301064915.parquet\n",
    "2022-03-01 06:49:29    2467117 5ed0070b-55a6-4743-8aab-2e97d57d28f6-0_5-4-37_20220301064915.parquet\n",
    "2022-03-01 06:49:29    2231503 7d24eabb-7fe6-4e7a-b2fc-9582caef059e-0_9-4-41_20220301064915.parquet\n",
    "2022-03-01 06:49:31    2383519 a07bea06-3671-45c1-90ef-038e1f60e012-0_4-4-36_20220301064915.parquet\n",
    "2022-03-01 06:49:30    2165923 a787d255-cd60-46c0-a8d1-f5405a3ac5de-0_3-4-35_20220301064915.parquet\n",
    "2022-03-01 06:49:30    2352220 ae944d48-0379-4974-854b-46b2f2a641af-0_6-4-38_20220301064915.parquet\n",
    "2022-03-01 06:49:29    2070634 b463dd72-044e-45a5-b520-87e79c397fd9-0_2-4-34_20220301064915.parquet\n",
    "2022-03-01 06:49:31    2021565 cdd10a9f-1c6a-45ae-b65c-12b24c63360a-0_8-4-40_20220301064915.parquet\n",
    "2022-03-01 06:49:29    2354644 f70ef867-2e07-4120-95ae-1703201a4067-0_1-4-33_20220301064915.parquet\n",
    "```\n",
    "\n",
    "To query the Hudi dataset you can do one of the following\n",
    "\n",
    "- Navigate to the another sparkmagic notebook and run queries in Spark using SparkMagic cell\n",
    "- SSH to the master node (you can also SSM if you launched your cluster with SSM permissions) and run queries using Hive/Presto\n",
    "- Head to the Hue console on Amazon EMR and run queries\n",
    "- Query using Amazon Athena or Redshift spectrum (preferred)\n",
    "\n",
    "Let us use Athena to query\n",
    "\n",
    "```\n",
    "\n",
    "In Athena console: \n",
    "\n",
    "select * from profile_cow limit 2;\n",
    "\n",
    "\t_hoodie_commit_time\t_hoodie_commit_seqno\t_hoodie_record_key\t_hoodie_partition_path\t_hoodie_file_name\tname\tphone\tjob\tcompany\tssn\tstreet_address\tdob\temail\tts\n",
    "1\t20220301064915\t20220301064915_6_2\t95d748fc-158d-44b0-85b6-ad198e7ad2f1\t\t15219e18-c613-405a-9f80-3ebac5e7a2a3-0_6-22-136_20220301064915.parquet\tJoshua Johnson\t(165)401-1609x877\tAccountant, chartered\tGallegos, Patel and Perez\t675-97-0588\t82503 Morgan Cliff Apt. 310 South Eddie, DE 38645\t1937-04-30\tjacob89@example.org\t2015-06-05T00:38:23\n",
    "2\t20220301064915\t20220301064915_6_4\t95d74b34-5d9d-4e75-b5aa-a70d73167cbd\t\t15219e18-c613-405a-9f80-3ebac5e7a2a3-0_6-22-136_20220301064915.parquet\tKeith Chen\t576-351-8011x7651\tEcologist\tBoyd-Jones\t382-35-5590\t76979 Robert Summit North Ashleymouth, HI 73317\t1973-04-23\tkflores@example.net\t2020-06-14T06:17:54\n",
    "\n",
    "Now, lets make a note of street_address in one of these two records -> \"82503 Morgan Cliff Apt. 310 South Eddie, DE 38645\"\n",
    "\n",
    "select _hoodie_commit_time, street_address from profile_cow where _hoodie_record_key='95d748fc-158d-44b0-85b6-ad198e7ad2f1';\n",
    "\n",
    "_hoodie_commit_time\tstreet_address\n",
    "1\t20220301064915\t82503 Morgan Cliff Apt. 310 South Eddie, DE 38645\n",
    "\n",
    "```\n",
    "\n",
    "Lets now run an upsert to observe the change in records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_updates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the records in updates/ location.\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds/updates/\n",
    "2022-03-01 06:58:10    3686930 profile_1.json\n",
    "2022-03-01 06:58:35    3686555 profile_10.json\n",
    "2022-03-01 06:58:38    3686528 profile_11.json\n",
    "2022-03-01 06:58:41    3684902 profile_12.json\n",
    "2022-03-01 06:58:44    3683917 profile_13.json\n",
    "2022-03-01 06:58:47    3685412 profile_14.json\n",
    "2022-03-01 06:58:12    3683398 profile_2.json\n",
    "2022-03-01 06:58:15    3686330 profile_3.json\n",
    "2022-03-01 06:58:18    3685814 profile_4.json\n",
    "2022-03-01 06:58:21    3686473 profile_5.json\n",
    "2022-03-01 06:58:24    3687483 profile_6.json\n",
    "2022-03-01 06:58:27    3684895 profile_7.json\n",
    "2022-03-01 06:58:29    3685616 profile_8.json\n",
    "2022-03-01 06:58:32    3683469 profile_9.json\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DeltaStreamer to apply updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the Deltastreamer again to run upserts using the updates generated in the previous step.\n",
    "\n",
    "```\n",
    "\n",
    "! ~/.local/bin/aws emr add-steps --cluster-id j-XXXXXXX --steps Type=Spark,Name=\"Deltastreamer Profile Upserts\",ActionOnFailure=CONTINUE,Args=[--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///apps/hudi/lib/hudi-utilities-bundle.jar,--props,s3://<my-bucket>/hudi-ds/config/json-deltastreamer.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://<my-bucket>/hudi-ds/output/profile-test15-out,--target-table,profile_test15_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,UPSERT] --region us-east-1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ~/.local/bin/aws emr add-steps --cluster-id j-1GMG9EJ4Z4ZL0 --steps Type=Spark,Name=\"Deltastreamer COW\",ActionOnFailure=CONTINUE,Args=[--jars,hdfs:///apps/hudi/lib/*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///apps/hudi/lib/hudi-utilities-bundle.jar,--props,s3://mrworkshop-youraccountID-dayone/config/json-deltastreamer_upsert.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,UPSERT] --region us-east-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the updated Hudi Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the S3 path of output location. Notice the new Parquet files. \n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1/\n",
    "                           PRE .hoodie/\n",
    "2022-03-01 06:49:05          0 .hoodie_$folder$\n",
    "2022-03-01 06:49:27         93 .hoodie_partition_metadata\n",
    "2022-03-01 07:04:59    2494102 15aaf95c-38c1-4889-9987-cdc0e8e7f913-0_7-22-135_20220301070424.parquet\n",
    "2022-03-01 06:49:30    2488921 15aaf95c-38c1-4889-9987-cdc0e8e7f913-0_7-4-39_20220301064915.parquet\n",
    "2022-03-01 06:49:31    2259709 55662f83-a4b3-4278-b5d0-4176c1146ac7-0_0-4-32_20220301064915.parquet\n",
    "2022-03-01 07:04:57    2263906 55662f83-a4b3-4278-b5d0-4176c1146ac7-0_1-22-129_20220301070424.parquet\n",
    "2022-03-01 07:04:59    2470890 5ed0070b-55a6-4743-8aab-2e97d57d28f6-0_4-22-132_20220301070424.parquet\n",
    "2022-03-01 06:49:29    2467117 5ed0070b-55a6-4743-8aab-2e97d57d28f6-0_5-4-37_20220301064915.parquet\n",
    "2022-03-01 07:04:57    2235852 7d24eabb-7fe6-4e7a-b2fc-9582caef059e-0_9-22-137_20220301070424.parquet\n",
    "2022-03-01 06:49:29    2231503 7d24eabb-7fe6-4e7a-b2fc-9582caef059e-0_9-4-41_20220301064915.parquet\n",
    "2022-03-01 07:04:58    2387713 a07bea06-3671-45c1-90ef-038e1f60e012-0_3-22-131_20220301070424.parquet\n",
    "2022-03-01 06:49:31    2383519 a07bea06-3671-45c1-90ef-038e1f60e012-0_4-4-36_20220301064915.parquet\n",
    "2022-03-01 06:49:30    2165923 a787d255-cd60-46c0-a8d1-f5405a3ac5de-0_3-4-35_20220301064915.parquet\n",
    "2022-03-01 07:04:57    2169821 a787d255-cd60-46c0-a8d1-f5405a3ac5de-0_6-22-134_20220301070424.parquet\n",
    "2022-03-01 06:49:30    2352220 ae944d48-0379-4974-854b-46b2f2a641af-0_6-4-38_20220301064915.parquet\n",
    "2022-03-01 07:04:58    2355854 ae944d48-0379-4974-854b-46b2f2a641af-0_8-22-136_20220301070424.parquet\n",
    "2022-03-01 07:04:56    2074694 b463dd72-044e-45a5-b520-87e79c397fd9-0_2-22-130_20220301070424.parquet\n",
    "2022-03-01 06:49:29    2070634 b463dd72-044e-45a5-b520-87e79c397fd9-0_2-4-34_20220301064915.parquet\n",
    "2022-03-01 07:04:59    2025729 cdd10a9f-1c6a-45ae-b65c-12b24c63360a-0_5-22-133_20220301070424.parquet\n",
    "2022-03-01 06:49:31    2021565 cdd10a9f-1c6a-45ae-b65c-12b24c63360a-0_8-4-40_20220301064915.parquet\n",
    "2022-03-01 07:04:57    2358794 f70ef867-2e07-4120-95ae-1703201a4067-0_0-22-128_20220301070424.parquet\n",
    "2022-03-01 06:49:29    2354644 f70ef867-2e07-4120-95ae-1703201a4067-0_1-4-33_20220301064915.parquet\n",
    "\n",
    "```\n",
    "\n",
    "Let's query an upserted record. \n",
    "\n",
    "```\n",
    "select _hoodie_commit_time, street_address from profile_cow where _hoodie_record_key='95d748fc-158d-44b0-85b6-ad198e7ad2f1';\n",
    "\n",
    "_hoodie_commit_time    street_address\n",
    "1    20220301070424    82503 Morgan Cliff Apt. 310 South Eddie, DE 38645    # Old address \n",
    "2    20220301064915\t   35740 Young Orchard Suite 147 South Williamport, MT 82610   # Our recent update \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Now lets check out Hudi CLI\n",
    "\n",
    "hudi:person_profile_cow->commits show\n",
    "2022-03-01 07:11:45,875 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20220301070424__commit__COMPLETED]}\n",
    "2022-03-01 07:11:45,918 INFO s3n.S3NativeFileSystem: Opening 's3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1/.hoodie/20220301070424.commit' for reading\n",
    "2022-03-01 07:11:46,265 INFO s3n.S3NativeFileSystem: Opening 's3://mrworkshop-youraccountID-dayone/hudi-ds-output/person-profile-out1/.hoodie/20220301064915.commit' for reading\n",
    "\n",
    "```\n",
    "╔════════════════╤═════════════════════╤═══════════════════╤═════════════════════╤══════════════════════════╤═══════════════════════╤══════════════════════════════╤══════════════╗\n",
    "║ CommitTime     │ Total Bytes Written │ Total Files Added │ Total Files Updated │ Total Partitions Written │ Total Records Written │ Total Update Records Written │ Total Errors ║\n",
    "╠════════════════╪═════════════════════╪═══════════════════╪═════════════════════╪══════════════════════════╪═══════════════════════╪══════════════════════════════╪══════════════╣\n",
    "║ 20220301070424 │ 21.8 MB             │ 0                 │ 10                  │ 1                        │ 150000                │ 140000                       │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20220301064915 │ 21.7 MB             │ 10                │ 0                   │ 1                        │ 150000                │ 0                            │ 0            ║\n",
    "╚════════════════╧═════════════════════╧═══════════════════╧═════════════════════╧══════════════════════════╧═══════════════════════╧══════════════════════════════╧══════════════╝\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
