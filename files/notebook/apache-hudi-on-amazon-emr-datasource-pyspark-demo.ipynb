{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert, Update and Delete data on S3 Datalake using Apache Hudi and Amazon EMR - Demo\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Copy On Write](#Copy-On-Write)<br>\n",
    "    2.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)<br>\n",
    "    2.2 [Batch Upsert some records](#Batch-Upsert-some-records)<br>\n",
    "    2.3 [Deleting Records](#Deleting-Records)<br>\n",
    "3. [Rollback](#Rollback)<br>\n",
    "4. [Time travel with Hudi](#Time-travel-with-Hudi) \n",
    "5. [Merge on Read](#Merge-On-Read)<br>\n",
    "    5.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)<br>\n",
    "    5.2 [Batch Upsert some records](#Batch-Upsert-some-records)<br>\n",
    "    5.3 [Compaction for MOR tables](#Compaction-for-MOR-tables) <br>\n",
    "6. [Working with Partitioned Tables](#Working-with-Partitioned-Tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates using PySpark on [Apache Hudi](https://aws.amazon.com/emr/features/hudi/) on Amazon EMR to insert/upsert/delete records to an S3 data lake.\n",
    "\n",
    "Here are some good reference links to read later:\n",
    "\n",
    "* [Apache Hudi concepts](https://hudi.apache.org/concepts.html)\n",
    "* [How Hudi Works](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-how-it-works.html)\n",
    "\n",
    "This notebook covers the following concepts when writing Copy-On-Write and Merge-On-Read tables to an S3 Datalake:\n",
    "\n",
    "- Write Hudi Spark jobs in PySpark.\n",
    "- Bulk Insert the Initial Dataset.\n",
    "- Write a MultiKey Partitioned table as well as a Non-Partitioned table.\n",
    "- Tune the Bulk Insert write performance as per expected number of target files.\n",
    "- Sync the Hudi tables to the Hive/Glue Catalog.\n",
    "- Upsert some records to a Hudi table.\n",
    "- Delete from records from a Hudi table.\n",
    "- Understand how Hudi Commit Retention policy works.\n",
    "- Time travel with Hudi incremental tables using incremental and point-in-time queries\n",
    "- Perform Insert/Upsert/Delete operations for Merge-on-Read table and understand the difference between MOR and COW tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by initializing the Spark Session to connect this notebook to our Spark EMR cluster:\n",
    "\n",
    "Note that the files hudi-spark-bundle.jar and spark-avro.jar are copied into HDFS.\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar hdfs:///user/hadoop/\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar hdfs:///user/hadoop/\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/spark/jars/httpclient-4.5.9.jar hdfs:///user/hadoop/\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/spark/jars/httpcore-4.4.11.jar hdfs:///user/hadoop/\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.12.31.jar hdfs:///user/hadoop/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///user/hadoop/aws-java-sdk-bundle-1.12.31.jar,hdfs:///user/hadoop/httpcore-4.4.11.jar,hdfs:///user/hadoop/httpclient-4.5.9.jar,hdfs:////user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\", \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Following are the configuration items we will use in this demo\n",
    "\n",
    "Make sure to point the target parameter to your S3 bucket. Replace youraccountID with your event engine AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# REPLACE \"youraccountID\" with your event engine AWS account ID\n",
    "\n",
    "config = {\n",
    "    \"table_name\": \"hudi_trips_table\",\n",
    "    \"target\": \"s3://mrworkshop-youraccountID-dayone/hudi/hudi_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants for Python to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General Constants\n",
    "HUDI_FORMAT = \"org.apache.hudi\"\n",
    "TABLE_NAME = \"hoodie.table.name\"\n",
    "RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\"\n",
    "\n",
    "# Hive Constants\n",
    "HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "# Partition Constants\n",
    "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.keygen.NonpartitionedKeyGenerator\"\n",
    "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "\n",
    "#Incremental Constants\n",
    "VIEW_TYPE_OPT_KEY=\"hoodie.datasource.view.type\"\n",
    "BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create JSON data and Spark dataframe from this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Generates Data\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_json_data(start, count, dest):\n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    data = [{\"trip_id\": i, \"tstamp\": time_stamp, \"route_id\": chr(65 + (i % 10)), \"destination\": dest[i%10]} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "# Creates the Dataframe\n",
    "def create_json_df(spark, data):\n",
    "    sc = spark.sparkContext\n",
    "    return spark.read.json(sc.parallelize(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Insert the Initial Dataset\n",
    "\n",
    "Let's generate 2M records to load into our Data Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df1 = create_json_df(spark, get_json_data(0, 2000000, dest))\n",
    "print(df1.count())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And write the data to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the number of files in S3. Expected number of files is 3 files as BULK_INSERT_PARALLELISM is set to 3. \n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID \n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2022-03-01 05:28:13    0 Bytes .hoodie_$folder$\n",
    "2022-03-01 05:28:25   93 Bytes .hoodie_partition_metadata\n",
    "2022-03-01 05:28:33    4.6 MiB 387ba1b1-854d-4add-b294-ccbdeb1d61d2-0_1-8-132_20220301052810.parquet\n",
    "2022-03-01 05:28:34    4.5 MiB a4e39bef-3ff1-4c0e-a4d2-853575468021-0_2-8-133_20220301052810.parquet\n",
    "2022-03-01 05:28:34    4.8 MiB b51530d5-d7a9-41c7-9e3e-63a7a6ec8098-0_0-8-131_20220301052810.parquet\n",
    "\n",
    "Total Objects: 5\n",
    "   Total Size: 13.9 MiB\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "Let's inspect the table created and query the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Glue data catalog for a new Hudi table called hudi_trips_table. Check the schema and notice the new columns added by Hudi to keep track of commits and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df2.count()\n",
    "df2.show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the Hive table as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from \"+config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Upsert some records\n",
    "\n",
    "Let's modify a few records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name'] +\" where trip_id between 1000000 and 1000009\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "upsert_dest = [\"Boston\", \"Boston\", \"Boston\", \"Boston\", \"Boston\",\"Boston\",\"Boston\",\"Boston\",\"Boston\",\"Boston\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3.select(\"trip_id\",\"route_id\",\"tstamp\",\"destination\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have changed the destination and timestamp for trip IDs 1000000 to 1000010. Now, let's upsert the changes to S3. Note that the operation now is \"Upsert\" as opposed to BulkInsert for the initial load:\n",
    "\n",
    "```\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id between 999996 and 1000013\").show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the commit. \n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID.\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2022-03-01 05:28:13    0 Bytes .hoodie_$folder$\n",
    "2022-03-01 05:28:25   93 Bytes .hoodie_partition_metadata\n",
    "2022-03-01 05:28:33    4.6 MiB 387ba1b1-854d-4add-b294-ccbdeb1d61d2-0_1-8-132_20220301052810.parquet\n",
    "2022-03-01 05:28:34    4.5 MiB a4e39bef-3ff1-4c0e-a4d2-853575468021-0_2-8-133_20220301052810.parquet\n",
    "2022-03-01 05:31:37    4.8 MiB b51530d5-d7a9-41c7-9e3e-63a7a6ec8098-0_0-55-728_20220301053126.parquet\n",
    "2022-03-01 05:28:34    4.8 MiB b51530d5-d7a9-41c7-9e3e-63a7a6ec8098-0_0-8-131_20220301052810.parquet\n",
    "\n",
    "Total Objects: 6\n",
    "   Total Size: 18.7 MiB\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we upserted some records, let us try to insert 10 new records into the table. We will use same upsert option. As primary keys 2000000 to 2000009 do not exist in the table, the records will be inserted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "insert_dest = [\"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\"]\n",
    "df5 = create_json_df(spark, get_json_data(2000000, 10, insert_dest))\n",
    "df5.count()\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(df5.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df6=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Records.\n",
    "\n",
    "Apache Hudi supports implementing two types of deletes on data stored in Hudi datasets, by enabling the user to specify a different record payload implementation.\n",
    "\n",
    "* **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. This can be simply achieved by ensuring the appropriate fields are nullable in the dataset schema and simply upserting the dataset after setting these fields to null.\n",
    "    \n",
    "* **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the dataset. \n",
    "\n",
    "Let's now execute some hard delete operations on our dataset which will remove the records from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete the 10 records with the \"Syracuse\" destination we added to the table. Note that the only change is the single line that set the hoodie.datasource.write.payload.class to org.apache.hudi.EmptyHoodieRecordPayload to delete the records.\n",
    "\n",
    "```\n",
    ".option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(df5.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records > 2000000 no longer exist in our table.\n",
    "\n",
    "Let's observe the number of files in S3. Expected : 6 files (initial files (3) + one upsert + one insert + one delete = 6)\n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID \n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2022-03-01 05:28:13    0 Bytes .hoodie_$folder$\n",
    "2022-03-01 05:28:25   93 Bytes .hoodie_partition_metadata\n",
    "2022-03-01 05:28:33    4.6 MiB 387ba1b1-854d-4add-b294-ccbdeb1d61d2-0_1-8-132_20220301052810.parquet\n",
    "2022-03-01 05:33:09    4.5 MiB a4e39bef-3ff1-4c0e-a4d2-853575468021-0_0-141-1576_20220301053259.parquet\n",
    "2022-03-01 05:32:44    4.5 MiB a4e39bef-3ff1-4c0e-a4d2-853575468021-0_0-98-1180_20220301053232.parquet\n",
    "2022-03-01 05:28:34    4.5 MiB a4e39bef-3ff1-4c0e-a4d2-853575468021-0_2-8-133_20220301052810.parquet\n",
    "2022-03-01 05:31:37    4.8 MiB b51530d5-d7a9-41c7-9e3e-63a7a6ec8098-0_0-55-728_20220301053126.parquet\n",
    "2022-03-01 05:28:34    4.8 MiB b51530d5-d7a9-41c7-9e3e-63a7a6ec8098-0_0-8-131_20220301052810.parquet\n",
    "\n",
    "Total Objects: 8\n",
    "   Total Size: 27.8 MiB\n",
    "\n",
    "```\n",
    "\n",
    "In our example, we set number of commits to retain as 10. So, maximum only 10 new files can be created on top of our bulk insert files. i.e., 13 files in total. If we had set the commits_to_retain as 2, the number of files created will not increase beyond initial files(3) + commits_to_retain(2) = 5 files. This is because Hudi Cleaning Policy will delete older files when writing based on the commit retain policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollback\n",
    "\n",
    "Let's say we want to roll back the last delete we made. Run the below command on EMR leader node session (using Session Manager or SSH).\n",
    "\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID.\n",
    "\n",
    "connect --path s3://mrworkshop-youraccountID-dayone/hudi/hudi_trips_table/\n",
    "\n",
    "```\n",
    "hudi:hudi_trips_table->commits show\n",
    "2022-03-01 05:34:49,072 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20220301053259__commit__COMPLETED]}\n",
    "2022-03-01 05:34:49,096 INFO s3n.S3NativeFileSystem: Opening 's3://mrworkshop-youraccountID-dayone/hudi/hudi_trips_table/.hoodie/20220301053259.commit' for reading\n",
    "2022-03-01 05:34:49,456 INFO s3n.S3NativeFileSystem: Opening 's3://mrworkshop-youraccountID-dayone/hudi/hudi_trips_table/.hoodie/20220301053232.commit' for reading\n",
    "2022-03-01 05:34:49,563 INFO s3n.S3NativeFileSystem: Opening 's3://mrworkshop-youraccountID-dayone/hudi/hudi_trips_table/.hoodie/20220301053126.commit' for reading\n",
    "2022-03-01 05:34:49,632 INFO s3n.S3NativeFileSystem: Opening 's3://mrworkshop-youraccountID-dayone/hudi/hudi_trips_table/.hoodie/20220301052810.commit' for reading\n",
    "╔════════════════╤═════════════════════╤═══════════════════╤═════════════════════╤══════════════════════════╤═══════════════════════╤══════════════════════════════╤══════════════╗\n",
    "║ CommitTime     │ Total Bytes Written │ Total Files Added │ Total Files Updated │ Total Partitions Written │ Total Records Written │ Total Update Records Written │ Total Errors ║\n",
    "╠════════════════╪═════════════════════╪═══════════════════╪═════════════════════╪══════════════════════════╪═══════════════════════╪══════════════════════════════╪══════════════╣\n",
    "║ 20220301053259 │ 4.5 MB              │ 0                 │ 1                   │ 1                        │ 650365                │ 0                            │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20220301053232 │ 4.5 MB              │ 0                 │ 1                   │ 1                        │ 650375                │ 0                            │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20220301053126 │ 4.8 MB              │ 0                 │ 1                   │ 1                        │ 682498                │ 10                           │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20220301052810 │ 13.9 MB             │ 3                 │ 0                   │ 1                        │ 2000000               │ 0                            │ 0            ║\n",
    "╚════════════════╧═════════════════════╧═══════════════════╧═════════════════════╧══════════════════════════╧═══════════════════════╧══════════════════════════════╧══════════════╝\n",
    "```\n",
    "\n",
    "REPLACE the --commit value with the first CommitTime returned by the above command. \n",
    "\n",
    "hudi:hudi_trips_table->commit rollback --commit 20220301053259\n",
    "    \n",
    "Now let us check what happened to the records we deleted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel with Hudi \n",
    "\n",
    "Now, let us time travel with Hudi (query previous commits) with incremental and point-in-time queries\n",
    "\n",
    "First, lets check out incremental queries\n",
    "\n",
    "In EMR cluster, let us check Hudi CLI and check how commits look right now.\n",
    "\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID \n",
    "\n",
    "connect --path s3://mrworkshop-youraccountID-dayone/hudi/hudi_trips_table/\n",
    "\n",
    "commits show\n",
    "\n",
    "```\n",
    "╔════════════════╤═════════════════════╤═══════════════════╤═════════════════════╤══════════════════════════╤═══════════════════════╤══════════════════════════════╤══════════════╗\n",
    "║ CommitTime     │ Total Bytes Written │ Total Files Added │ Total Files Updated │ Total Partitions Written │ Total Records Written │ Total Update Records Written │ Total Errors ║\n",
    "╠════════════════╪═════════════════════╪═══════════════════╪═════════════════════╪══════════════════════════╪═══════════════════════╪══════════════════════════════╪══════════════╣\n",
    "║ 20220301053232 │ 4.5 MB              │ 0                 │ 1                   │ 1                        │ 650375                │ 0                            │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20220301053126 │ 4.8 MB              │ 0                 │ 1                   │ 1                        │ 682498                │ 10                           │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20220301052810 │ 13.9 MB             │ 3                 │ 0                   │ 1                        │ 2000000               │ 0                            │ 0            ║\n",
    "╚════════════════╧═════════════════════╧═══════════════════╧═════════════════════╧══════════════════════════╧═══════════════════════╧══════════════════════════════╧══════════════╝\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets check out point-in-time queries. i.e., query from a specific commit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits = spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_table order by commitTime\").collect()\n",
    "print(\"commits: \")\n",
    "for elem in commits: print (elem) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Lets check out the first state. i.e., before we updated the trip to Boston\n",
    "\n",
    "beginTime = \"000\" #Represents all commits > this time.\n",
    "endTime = commits[-3][0] # first commit\n",
    "#print(\"end time: \"+endTime)\n",
    "\n",
    "# incrementally query data\n",
    "incViewDF = spark.read.format(\"org.apache.hudi\") \\\n",
    "                   .option(VIEW_TYPE_OPT_KEY, VIEW_TYPE_INCREMENTAL_OPT_VAL) \\\n",
    "                   .option(BEGIN_INSTANTTIME_OPT_KEY, beginTime) \\\n",
    "                   .option(END_INSTANTTIME_OPT_KEY, endTime) \\\n",
    "                   .load(config[\"target\"]) \n",
    "\n",
    "#incViewDF.show(5)\n",
    "incViewDF.registerTempTable(\"hudi_incr_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select `_hoodie_commit_time`, trip_id, route_id, destination, tstamp from  hudi_incr_table where trip_id between 999996 and 1000013\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the records from 1000000 to 1000009 display first state of our table before we upserted the trip destination to Boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge On Read \n",
    "\n",
    "The default table type is Copy-On-Write which is best suited for read-heavy workloads with modest writes. Copy-On-Write creates commit files with original data + the new changes during writing itself. While this increases latency on writes, this set up makes it more manageable for faster read.\n",
    "\n",
    "For near real-time applications that mandate quick upserts, MERGE_ON_READ table type would be better suited. MOR table stores incoming upserts for each file group, onto a row based delta log (In Avro file format). This log is then merged with the existing Parquet file using a a compactor during reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# REPLACE youraccountID with event engine AWS account ID \n",
    "\n",
    "config = {\n",
    "    \"table_name\": \"hudi_mor_trips_table\",\n",
    "    \"table_name_rt\": \"hudi_mor_trips_table_rt\",\n",
    "    \"target\": \"s3://mrworkshop-youraccountID-dayone/hudi/hudi_mor_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STORAGE_TYPE_OPT_KEY=\"hoodie.datasource.write.storage.type\"\n",
    "COMPACTION_INLINE_OPT_KEY=\"hoodie.compact.inline\"\n",
    "COMPACTION_MAX_DELTA_COMMITS_OPT_KEY=\"hoodie.compact.inline.max.delta.commits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mor_dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df2 = create_json_df(spark, get_json_data(0, 2000000, mor_dest))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk insert will take the same time as COW as this is the first write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df2.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"1\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the number of files \n",
    "\n",
    "Let us check the contents of S3 path. Bulk insert operation on Copy-On-Write and Merge-On-Read tables is identical in terms of performance. \n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID.\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2022-03-01 05:40:47          0 .hoodie_$folder$\n",
    "2022-03-01 05:40:53         93 .hoodie_partition_metadata\n",
    "2022-03-01 05:41:00    4668560 426b8846-8e9f-470b-9b46-859d6c32877b-0_2-184-2214_20220301054046.parquet\n",
    "2022-03-01 05:41:01    4841603 4eeaa768-d945-42a9-8f5d-aab7abacb9a2-0_1-184-2213_20220301054046.parquet\n",
    "2022-03-01 05:41:01    4974502 4fd9dbe9-2575-4b61-9808-787b6cf8880f-0_0-184-2212_20220301054046.parquet\n",
    "\n",
    "```\n",
    "\n",
    "Notice the delta commits \n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID.\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi/hudi_mor_trips_table/.hoodie/\n",
    "                           PRE .aux/\n",
    "2022-03-01 05:40:48          0 .aux_$folder$\n",
    "2022-03-01 05:40:47          0 .temp_$folder$\n",
    "2022-03-01 05:41:01       3508 20220301054046.deltacommit\n",
    "2022-03-01 05:40:49          0 20220301054046.deltacommit.inflight\n",
    "2022-03-01 05:40:49          0 20220301054046.deltacommit.requested\n",
    "2022-03-01 05:40:47          0 archived_$folder$\n",
    "2022-03-01 05:40:48        370 hoodie.properties\n",
    "\n",
    "```\n",
    "\n",
    "This is the first commit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to upsert some records into MOR table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_dest = [\"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.count()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"1\")\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+ \"_ro where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets query the real-time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+ config['table_name_rt'] + \" where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the S3 path again. There is no change in number of Parquet files after upsert operation unlike Copy-On-Write tables. \n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID.\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2022-03-01 05:42:04       2071 .4fd9dbe9-2575-4b61-9808-787b6cf8880f-0_20220301054046.log.1_0-218-2541\n",
    "2022-03-01 05:40:47          0 .hoodie_$folder$\n",
    "2022-03-01 05:40:53         93 .hoodie_partition_metadata\n",
    "2022-03-01 05:41:00    4668560 426b8846-8e9f-470b-9b46-859d6c32877b-0_2-184-2214_20220301054046.parquet\n",
    "2022-03-01 05:41:01    4841603 4eeaa768-d945-42a9-8f5d-aab7abacb9a2-0_1-184-2213_20220301054046.parquet\n",
    "2022-03-01 05:41:01    4974502 4fd9dbe9-2575-4b61-9808-787b6cf8880f-0_0-184-2212_20220301054046.parquet\n",
    "\n",
    "```\n",
    "\n",
    "Now, let us compact using Hudi CLI \n",
    "\n",
    "On EMR:\n",
    "\n",
    "```\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID.\n",
    "\n",
    "hudi->connect --path s3://mrworkshop-youraccountID-dayone/hudi/hudi_mor_trips_table/\n",
    "\n",
    "hudi:hudi_mor_trips_table->compactions show all\n",
    "╔═════════════════════════╤═══════╤═══════════════════════════════╗\n",
    "║ Compaction Instant Time │ State │ Total FileIds to be Compacted ║\n",
    "╠═════════════════════════╧═══════╧═══════════════════════════════╣\n",
    "║ (empty)                                                         ║\n",
    "╚═════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "```\n",
    "\n",
    "Notice there are no compactions for our mor table. Let us manually schedule and run a compaction. \n",
    "\n",
    "```\n",
    "\n",
    "hudi:hudi_mor_trips_table->compaction schedule --hoodieConfigs hoodie.datasource.hive_sync.partition_extractor_class=com.uber.hoodie.NonpartitionedKeyGenerator,hoodie.compact.inline.max.delta.commits=1\n",
    "\n",
    "Attempted to schedule compaction for 20220301054343\n",
    "\n",
    "Exit the Hudi CLI (Ctrl+C) and re-enter \n",
    "\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID.\n",
    "\n",
    "hudi:hudi_mor_trips_table->connect --path s3://mrworkshop-youraccountID-dayone/hudi/hudi_mor_trips_table/\n",
    "\n",
    "hudi:hudi_mor_trips_table->compactions show all\n",
    "2022-03-01 05:44:42,756 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20220301054343__compaction__REQUESTED]}\n",
    "2022-03-01 05:44:42,779 INFO s3n.S3NativeFileSystem: Opening 's3://mrworkshop-youraccountID-dayone/hudi/hudi_mor_trips_table/.hoodie/.aux/20220301054343.compaction.requested' for reading\n",
    "╔═════════════════════════╤═══════════╤═══════════════════════════════╗\n",
    "║ Compaction Instant Time │ State     │ Total FileIds to be Compacted ║\n",
    "╠═════════════════════════╪═══════════╪═══════════════════════════════╣\n",
    "║ 20220301054343          │ REQUESTED │ 1                             ║\n",
    "╚═════════════════════════╧═══════════╧═══════════════════════════════╝\n",
    "\n",
    "REPLACE youraccountID with your event engine AWS account ID.\n",
    "REPLACE --compactionInstant value with what you received when you ran \"compactions show all\"\n",
    "\n",
    "hudi:hudi_mor_trips_table->compaction run --parallelism 10 --sparkMemory 4G --retry 2 --schemaFilePath s3://mrworkshop-youraccountID-dayone/schema/schema.avsc --compactionInstant 20220301054343\n",
    "\n",
    "Compaction successfully completed for 20220301054343\n",
    "\n",
    "```\n",
    "\n",
    "You can also schedule a compaction using \"compaction schedule\" option. \n",
    "\n",
    "Now, if we check the S3 path, we will see the delta commit has been merged into a new file \n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID.\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2022-03-01 05:42:04       2071 .4fd9dbe9-2575-4b61-9808-787b6cf8880f-0_20220301054046.log.1_0-218-2541\n",
    "2022-03-01 05:40:47          0 .hoodie_$folder$\n",
    "2022-03-01 05:40:53         93 .hoodie_partition_metadata\n",
    "2022-03-01 05:41:00    4668560 426b8846-8e9f-470b-9b46-859d6c32877b-0_2-184-2214_20220301054046.parquet\n",
    "2022-03-01 05:41:01    4841603 4eeaa768-d945-42a9-8f5d-aab7abacb9a2-0_1-184-2213_20220301054046.parquet\n",
    "2022-03-01 05:48:02    5405713 4fd9dbe9-2575-4b61-9808-787b6cf8880f-0_0-0-0_20220301054343.parquet\n",
    "2022-03-01 05:41:01    4974502 4fd9dbe9-2575-4b61-9808-787b6cf8880f-0_0-184-2212_20220301054046.parquet\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Partitioned Tables\n",
    "\n",
    "Let's do the same thing with Partitioned Tables. For the sake of this demo, we will be making route_id as partition field. You can also have a nested partition structure like yyyy/mm/dd which is more common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE youraccountID with event engine AWS account ID \n",
    "\n",
    "config = {\n",
    "    \"table_name\": \"hudi_partitioned_trips_table\",\n",
    "    \"target\": \"s3://mrworkshop-youraccountID-dayone/hudi/hudi_partitioned_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"2\",\n",
    "    \"partition_keys\" : \"route_id\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df1 = create_json_df(spark, get_json_data(0, 2000000, part_dest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the partitionKey column to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "hudiTablePartitionKey=\"route_id\"\n",
    "df1 = df1.withColumn(hudiTablePartitionKey,concat(lit(\"route_id=\"),col(\"route_id\")))\n",
    "df1.select(hudiTablePartitionKey).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now write out the data to S3. Notice that the Hive Partition Extractor class has changed in the statement below:\n",
    "\n",
    "```\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"route_id\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 6)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"route_id\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the partitions fields are present in our Hive table from Glue catalog\n",
    "\n",
    "```\n",
    "PARTITIONED BY (`route_id` STRING)\n",
    "```\n",
    "\n",
    "Let's now query the data and group by the the partition columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select route_id, count(*) as num_trips from \"+config['table_name']+\" group by route_id order by route_id\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the S3 path\n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID \n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi/hudi_partitioned_trips_table/\n",
    "                           PRE .hoodie/\n",
    "                           PRE route_id=A/\n",
    "                           PRE route_id=B/\n",
    "                           PRE route_id=C/\n",
    "                           PRE route_id=D/\n",
    "                           PRE route_id=E/\n",
    "                           PRE route_id=F/\n",
    "                           PRE route_id=G/\n",
    "                           PRE route_id=H/\n",
    "                           PRE route_id=I/\n",
    "                           PRE route_id=J/\n",
    "2022-03-01 05:50:23          0 .hoodie_$folder$\n",
    "2022-03-01 05:50:29          0 route_id=A_$folder$\n",
    "2022-03-01 05:50:29          0 route_id=B_$folder$\n",
    "2022-03-01 05:50:31          0 route_id=C_$folder$\n",
    "2022-03-01 05:50:29          0 route_id=D_$folder$\n",
    "2022-03-01 05:50:29          0 route_id=E_$folder$\n",
    "2022-03-01 05:50:31          0 route_id=F_$folder$\n",
    "2022-03-01 05:50:29          0 route_id=G_$folder$\n",
    "2022-03-01 05:50:29          0 route_id=H_$folder$\n",
    "2022-03-01 05:50:31          0 route_id=I_$folder$\n",
    "2022-03-01 05:50:34          0 route_id=J_$folder$\n",
    "```\n",
    "\n",
    "Under each partition, there will be partition metadata\n",
    "\n",
    "REPLACE youraccountID with event engine AWS account ID \n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://mrworkshop-youraccountID-dayone/hudi/hudi_partitioned_trips_table/route_id=A/\n",
    "2022-03-01 05:50:29         93 .hoodie_partition_metadata\n",
    "2022-03-01 05:50:32    1730659 b3ea79a8-d0ea-48f2-ae02-3ffea2275744-0_0-245-2900_20220301055022.parquet\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other operations Upsert etc. behave the same way on Partitioned tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
