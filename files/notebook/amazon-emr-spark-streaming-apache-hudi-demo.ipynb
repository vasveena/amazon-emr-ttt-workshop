{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a88e1a85-d221-4343-b865-be93184fe79f",
   "metadata": {},
   "source": [
    "# Building real time live incremental data lake on S3 using Apache Hudi + Spark structured streaming + Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28c432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///user/hadoop/aws-java-sdk-bundle-1.12.31.jar,hdfs:///user/hadoop/httpcore-4.4.11.jar,hdfs:///user/hadoop/httpclient-4.5.9.jar,hdfs:////user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\", \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7a4fa",
   "metadata": {},
   "source": [
    "You can also run the commands on Spark shell\n",
    "\n",
    "spark-shell --jars hdfs:///user/hadoop/aws-java-sdk-bundle-1.12.31.jar,hdfs:///user/hadoop/httpcore-4.4.11.jar,hdfs:///user/hadoop/httpclient-4.5.9.jar,hdfs:////user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar --conf spark.sql.hive.convertMetastoreParquet=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbcc07c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// General Constants\n",
    "val HUDI_FORMAT = \"org.apache.hudi\"\n",
    "val TABLE_NAME = \"hoodie.table.name\"\n",
    "val RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "val PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "val OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "val BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "val UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "val BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "val UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "val S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "val HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "val KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "val HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "val PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "val EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\"\n",
    "val TABLE_TYPE_OPT_KEY=\"hoodie.datasource.write.table.type\"\n",
    "\n",
    "// Hive Constants\n",
    "val HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "val HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "val HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "val HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "val HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "// Partition Constants\n",
    "val NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "val MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "val KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "val NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.keygen.NonpartitionedKeyGenerator\"\n",
    "val COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "val PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "\n",
    "//Incremental Constants\n",
    "val VIEW_TYPE_OPT_KEY=\"hoodie.datasource.view.type\"\n",
    "val BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "val VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "val END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d82cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n",
    "import org.apache.spark.sql.types.{LongType, StringType, StructField, StructType}\n",
    "import org.apache.spark.sql.ForeachWriter\n",
    "import org.apache.spark.sql.catalyst.encoders.RowEncoder\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.kafka.clients.producer.{ProducerConfig, KafkaProducer, ProducerRecord}\n",
    "import java.util.HashMap\n",
    "import spark.implicits._\n",
    "import org.apache.hudi._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2358d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val trip_update_topic = \"trip_update_topic\"\n",
    "val trip_status_topic = \"trip_status_topic\"\n",
    "val broker = \"yourbootstrapbrokers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74faca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "object MTASubwayTripUpdates extends Serializable {\n",
    "\n",
    "    val props = new HashMap[String, Object]()\n",
    "    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, broker)\n",
    "    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n",
    "      \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\n",
    "      \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "    @transient var producer : KafkaProducer[String, String] = null\n",
    "    var msgId : Long = 1\n",
    "    @transient var joined_query : StreamingQuery = null\n",
    "    @transient var joined_query_s3 : StreamingQuery = null\n",
    "\n",
    "    val spark = SparkSession.builder.appName(\"MSK streaming Example\").getOrCreate()\n",
    "    \n",
    "\n",
    "    def start() = {\n",
    "        //Start producer for kafka\n",
    "        producer = new KafkaProducer[String, String](props)\n",
    "\n",
    "        //Create a datastream from trip update topic\n",
    "        val trip_update_df = spark.readStream.format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", broker)\n",
    "        .option(\"subscribe\", trip_update_topic)\n",
    "        .option(\"startingOffsets\", \"latest\").option(\"failOnDataLoss\",\"false\").load()\n",
    "\n",
    "        //Create a datastream from trip status topic\n",
    "        val trip_status_df = spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", broker)\n",
    "        .option(\"subscribe\", trip_status_topic)\n",
    "        .option(\"startingOffsets\", \"latest\").option(\"failOnDataLoss\",\"false\").load()\n",
    "\n",
    "        // define schema of data\n",
    "\n",
    "        val trip_update_schema = new StructType()\n",
    "        .add(\"trip\", new StructType().add(\"tripId\",\"string\").add(\"startTime\",\"string\").add(\"startDate\",\"string\").add(\"routeId\",\"string\"))\n",
    "        .add(\"stopTimeUpdate\",ArrayType(new StructType().add(\"arrival\",new StructType().add(\"time\",\"string\")).add(\"stopId\",\"string\").add(\"departure\",new StructType().add(\"time\",\"string\"))))\n",
    "\n",
    "        val trip_status_schema = new StructType()\n",
    "        .add(\"trip\", new StructType().add(\"tripId\",\"string\").add(\"startTime\",\"string\").add(\"startDate\",\"string\").add(\"routeId\",\"string\")).add(\"currentStopSequence\",\"integer\").add(\"currentStatus\", \"string\").add(\"timestamp\", \"string\").add(\"stopId\",\"string\")\n",
    "\n",
    "        // covert datastream into a datasets and apply schema\n",
    "        val trip_update_ds = trip_update_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").as[(String, String)]\n",
    "        val trip_update_ds_schema = trip_update_ds\n",
    "        .select(from_json($\"value\", trip_update_schema).as(\"data\")).select(\"data.*\")\n",
    "        trip_update_ds_schema.printSchema()\n",
    "\n",
    "        val trip_status_ds = trip_status_df\n",
    "        .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").as[(String, String)]\n",
    "        val trip_status_ds_schema = trip_status_ds\n",
    "        .select(from_json($\"value\", trip_status_schema).as(\"data\")).select(\"data.*\")\n",
    "        trip_status_ds_schema.printSchema()\n",
    "\n",
    "        val trip_status_ds_unnest = trip_status_ds_schema\n",
    "        .select(\"trip.*\",\"currentStopSequence\",\"currentStatus\",\"timestamp\",\"stopId\")\n",
    "\n",
    "        val trip_update_ds_unnest = trip_update_ds_schema\n",
    "        .select($\"trip.*\", $\"stopTimeUpdate.arrival.time\".as(\"arrivalTime\"), \n",
    "                $\"stopTimeUpdate.departure.time\".as(\"depatureTime\"), $\"stopTimeUpdate.stopId\")\n",
    "\n",
    "        val trip_update_ds_unnest2 = trip_update_ds_unnest\n",
    "        .withColumn(\"numOfFutureStops\", size($\"arrivalTime\"))\n",
    "        .withColumnRenamed(\"stopId\",\"futureStopIds\")\n",
    "\n",
    "        val joined_ds = trip_update_ds_unnest2\n",
    "        .join(trip_status_ds_unnest, Seq(\"tripId\",\"routeId\",\"startTime\",\"startDate\"))\n",
    "        .withColumn(\"startTime\",(col(\"startTime\").cast(\"timestamp\")))\n",
    "        .withColumn(\"currentTs\",from_unixtime($\"timestamp\".divide(1000)))\n",
    "        .drop(\"startDate\").drop(\"timestamp\")\n",
    "\n",
    "        joined_ds.printSchema()\n",
    "        \n",
    "        def myFunc( batchDF:DataFrame, batchID:Long ) : Unit = {\n",
    "            batchDF.persist()\n",
    "            batchDF.write.format(\"org.apache.hudi\")\n",
    "                .option(TABLE_TYPE_OPT_KEY, \"COPY_ON_WRITE\")\n",
    "                .option(PRECOMBINE_FIELD_OPT_KEY, \"currentTs\")\n",
    "                .option(RECORDKEY_FIELD_OPT_KEY, \"tripId\")\n",
    "                .option(TABLE_NAME, \"hudi_trips_streaming_table\")\n",
    "                .option(UPSERT_PARALLELISM, 200)\n",
    "                .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "                .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "                .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "                .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "                .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "                .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "                .mode(SaveMode.Append)\n",
    "                .save(\"s3://mrworkshop-youraccountID-dayone/hudi/streaming_output/\")\n",
    "            \n",
    "            batchDF.unpersist()\n",
    "        }\n",
    "        \n",
    "        val query = joined_ds.writeStream.queryName(\"lab3\")\n",
    "        .trigger(Trigger.ProcessingTime(\"30 seconds\"))\n",
    "        .foreachBatch(myFunc _)\n",
    "    \n",
    "      .option(\"checkpointLocation\", \"/user/hadoop/checkpoint\")\n",
    "      .start()\n",
    "        \n",
    "      query.awaitTermination()\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MTASubwayTripUpdates.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b46273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
