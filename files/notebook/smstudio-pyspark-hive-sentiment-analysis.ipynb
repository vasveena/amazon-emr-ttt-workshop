{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This notebook does the following:\n",
    "\n",
    "* Demonstrates how you can visually connect Amazon SageMaker Studio Sparkmagic kernel to an EMR cluster\n",
    "* Explore and query data from a Hive table \n",
    "* Use the data locally\n",
    "* Provides resources that demonstrate how to use the local data for ML including using SageMaker Processing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "When using PySpark kernel notebooks, there is no need to create a SparkContext or a HiveContext; those are all created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:\n",
    "- SparkContext (sc)\n",
    "- HiveContext (sqlContext)\n",
    "\n",
    "----------\n",
    "### PySpark magics \n",
    "\n",
    "The PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (e.g. `%%MAGIC` <args>). The magic command must be the first word in a code cell and allow for multiple lines of content. You can’t put comments before a cell magic.\n",
    "\n",
    "For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html).\n",
    "    \n",
    "### Connection to EMR Cluster\n",
    "\n",
    "In the cell below, the code block is autogenerated. You can generate this code by clicking on the \"Cluster\" link on the top of the notebook and select the EMR cluster \"EMR-Spark-Hive-Presto\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read emr cluster(j-BNO7MEC3SMZR) details\n",
      "Initiating EMR connection..\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1647930094658_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-80-0-11-130.ec2.internal:20888/proxy/application_1647930094658_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-80-0-8-132.ec2.internal:8042/node/containerlogs/container_1647930094658_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "{\"namespace\": \"sagemaker-analytics\", \"cluster_id\": \"j-BNO7MEC3SMZR\", \"error_message\": null, \"success\": true, \"service\": \"emr\", \"operation\": \"connect\"}\n"
     ]
    }
   ],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics\n",
    "%sm_analytics emr connect --cluster-id j-BNO7MEC3SMZR --auth-type None  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the %%local magic to run your code locally on the Jupyter server without going to Spark. When you use %%local all subsequent lines in the cell will be executed locally. The code in the cell must be valid Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo Notebook\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "print(\"Demo Notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session information (%%info)\n",
    "\n",
    "Livy is an open source REST server for Spark. When you execute a code cell in a sparkmagic notebook, it creates a Livy session to execute your code. `%%info` magic will display the current Livy session information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1647930094658_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-80-0-11-130.ec2.internal:20888/proxy/application_1647930094658_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-80-0-8-132.ec2.internal:8042/node/containerlogs/container_1647930094658_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will use the HiveContext to query Hive and look at the databases and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|   hudidb|\n",
      "+---------+\n",
      "\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+"
     ]
    }
   ],
   "source": [
    "dbs = sqlContext.sql(\"show databases\")\n",
    "dbs.show()\n",
    "\n",
    "tables = sqlContext.sql(\"show tables\")\n",
    "tables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will query the movie_reviews table and get the data into a spark dataframe. You can visualize the data from the remote cluster locally in the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movie_reviews = sqlContext.sql(\"select * from movie_reviews\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data size and size of each class (positive and negative) and visualize it. You can see that we have a balanced dataset with equal number on both classes (25000 each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50001, 2)\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "| positive|25000|\n",
      "|sentiment|    1|\n",
      "| negative|25000|\n",
      "+---------+-----+"
     ]
    }
   ],
   "source": [
    "# Shape\n",
    "print((movie_reviews.count(), len(movie_reviews.columns)))\n",
    "# count of both positive and negative sentiments\n",
    "movie_reviews.groupBy('sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_reviews = movie_reviews.filter(movie_reviews.sentiment == 'positive').collect()\n",
    "neg_reviews = movie_reviews.filter(movie_reviews.sentiment == 'negative').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/lib64/python3.7/site-packages/numpy']"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Matplotlib requires numpy>=1.17; you have 1.16.5\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/matplotlib/__init__.py\", line 208, in <module>\n",
      "    _check_versions()\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/matplotlib/__init__.py\", line 204, in _check_versions\n",
      "    raise ImportError(f\"Matplotlib requires {modname}>={minver}; \"\n",
      "ImportError: Matplotlib requires numpy>=1.17; you have 1.16.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_counts(positive,negative):\n",
    "    plt.rcParams['figure.figsize']=(6,6)\n",
    "    plt.bar(0,positive,width=0.6,label='Positive Reviews',color='Green')\n",
    "    plt.bar(2,negative,width=0.6,label='Negative Reviews',color='Red')\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Type of Review')\n",
    "    plt.tick_params(\n",
    "        axis='x',          \n",
    "        which='both',      \n",
    "        bottom=False,      \n",
    "        top=False,         \n",
    "        labelbottom=False) \n",
    "    plt.show()\n",
    "    \n",
    "plot_counts(len(pos_reviews),len(neg_reviews))\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Let's inspect length of reviews using the pyspark.sql.functions module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Length of Review|\n",
      "+----------------+\n",
      "|               6|\n",
      "|            1761|\n",
      "|             998|\n",
      "|             926|\n",
      "|             748|\n",
      "|            1317|\n",
      "|             656|\n",
      "|             726|\n",
      "|             934|\n",
      "|             681|\n",
      "|             176|\n",
      "|             578|\n",
      "|             937|\n",
      "|            2227|\n",
      "|             662|\n",
      "|             275|\n",
      "|             830|\n",
      "|             720|\n",
      "|            1322|\n",
      "|             639|\n",
      "+----------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "reviewlengthDF = movie_reviews.select(length('review').alias('Length of Review')) \n",
    "reviewlengthDF.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also execute SparkSQL queries using the %%sql magic and save results to a local data frame. This allows for quick data exploration. Max rows returned by default is 2500. You can set the max rows by using the -n argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9739a3166742378610af95c6b7b43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687aae6f1825410b82cf2dab06ef0b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql -o movie_reviews_sparksql_df -n 10\n",
    "select * from movie_reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access and explore the data in the dataframe locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138a07cfd6514de5b10aa634b8ebc6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76a25d74b554549936fd4e1b7309849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local \n",
    "movie_reviews_sparksql_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Session logs (%%logs)\n",
    "\n",
    "You can get the logs of your current Livy session to debug any issues you encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "\n",
      "stderr: \n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 86) (ip-80-0-9-217.ec2.internal, executor 4, partition 11, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 19.0 in stage 8.0 (TID 87) (ip-80-0-8-132.ec2.internal, executor 5, partition 19, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 20.0 in stage 8.0 (TID 88) (ip-80-0-8-132.ec2.internal, executor 5, partition 20, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 21.0 in stage 8.0 (TID 89) (ip-80-0-8-132.ec2.internal, executor 5, partition 21, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 22.0 in stage 8.0 (TID 90) (ip-80-0-8-132.ec2.internal, executor 5, partition 22, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 23.0 in stage 8.0 (TID 91) (ip-80-0-8-132.ec2.internal, executor 5, partition 23, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 24.0 in stage 8.0 (TID 92) (ip-80-0-8-132.ec2.internal, executor 5, partition 24, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 25.0 in stage 8.0 (TID 93) (ip-80-0-8-132.ec2.internal, executor 5, partition 25, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 26.0 in stage 8.0 (TID 94) (ip-80-0-8-132.ec2.internal, executor 5, partition 26, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 27.0 in stage 8.0 (TID 95) (ip-80-0-8-132.ec2.internal, executor 5, partition 27, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ip-80-0-9-217.ec2.internal:38457 (size: 9.0 KiB, free: 4.7 GiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ip-80-0-9-217.ec2.internal:36451 (size: 9.0 KiB, free: 2.3 GiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ip-80-0-8-132.ec2.internal:41281 (size: 9.0 KiB, free: 9.7 GiB)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 28.0 in stage 8.0 (TID 96) (ip-80-0-9-217.ec2.internal, executor 3, partition 28, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 76) in 122 ms on ip-80-0-9-217.ec2.internal (executor 3) (1/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 29.0 in stage 8.0 (TID 97) (ip-80-0-9-217.ec2.internal, executor 3, partition 29, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 73) in 125 ms on ip-80-0-9-217.ec2.internal (executor 3) (2/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 30.0 in stage 8.0 (TID 98) (ip-80-0-9-217.ec2.internal, executor 3, partition 30, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 79) in 128 ms on ip-80-0-9-217.ec2.internal (executor 3) (3/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 31.0 in stage 8.0 (TID 99) (ip-80-0-9-217.ec2.internal, executor 3, partition 31, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 70) in 130 ms on ip-80-0-9-217.ec2.internal (executor 3) (4/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 24.0 in stage 8.0 (TID 92) in 129 ms on ip-80-0-8-132.ec2.internal (executor 5) (5/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 15.0 in stage 8.0 (TID 78) in 136 ms on ip-80-0-8-132.ec2.internal (executor 5) (6/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 26.0 in stage 8.0 (TID 94) in 137 ms on ip-80-0-8-132.ec2.internal (executor 5) (7/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 20.0 in stage 8.0 (TID 88) in 140 ms on ip-80-0-8-132.ec2.internal (executor 5) (8/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 16.0 in stage 8.0 (TID 81) in 143 ms on ip-80-0-8-132.ec2.internal (executor 5) (9/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 23.0 in stage 8.0 (TID 91) in 142 ms on ip-80-0-8-132.ec2.internal (executor 5) (10/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 72) in 147 ms on ip-80-0-8-132.ec2.internal (executor 5) (11/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 18.0 in stage 8.0 (TID 85) in 146 ms on ip-80-0-8-132.ec2.internal (executor 5) (12/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 27.0 in stage 8.0 (TID 95) in 147 ms on ip-80-0-8-132.ec2.internal (executor 5) (13/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 75) in 151 ms on ip-80-0-8-132.ec2.internal (executor 5) (14/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 69) in 154 ms on ip-80-0-8-132.ec2.internal (executor 5) (15/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 31.0 in stage 8.0 (TID 99) in 24 ms on ip-80-0-9-217.ec2.internal (executor 3) (16/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 17.0 in stage 8.0 (TID 83) in 153 ms on ip-80-0-8-132.ec2.internal (executor 5) (17/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 19.0 in stage 8.0 (TID 87) in 153 ms on ip-80-0-8-132.ec2.internal (executor 5) (18/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 21.0 in stage 8.0 (TID 89) in 155 ms on ip-80-0-8-132.ec2.internal (executor 5) (19/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 25.0 in stage 8.0 (TID 93) in 155 ms on ip-80-0-8-132.ec2.internal (executor 5) (20/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 30.0 in stage 8.0 (TID 98) in 30 ms on ip-80-0-9-217.ec2.internal (executor 3) (21/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 22.0 in stage 8.0 (TID 90) in 157 ms on ip-80-0-8-132.ec2.internal (executor 5) (22/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 28.0 in stage 8.0 (TID 96) in 44 ms on ip-80-0-9-217.ec2.internal (executor 3) (23/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 29.0 in stage 8.0 (TID 97) in 44 ms on ip-80-0-9-217.ec2.internal (executor 3) (24/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 86) in 179 ms on ip-80-0-9-217.ec2.internal (executor 4) (25/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 84) in 180 ms on ip-80-0-9-217.ec2.internal (executor 4) (26/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 71) in 184 ms on ip-80-0-9-217.ec2.internal (executor 4) (27/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 68) in 187 ms on ip-80-0-9-217.ec2.internal (executor 4) (28/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 74) in 186 ms on ip-80-0-9-217.ec2.internal (executor 4) (29/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 80) in 186 ms on ip-80-0-9-217.ec2.internal (executor 4) (30/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 77) in 188 ms on ip-80-0-9-217.ec2.internal (executor 4) (31/32)\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 82) in 189 ms on ip-80-0-9-217.ec2.internal (executor 4) (32/32)\n",
      "22/03/22 08:09:57 INFO YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "22/03/22 08:09:57 INFO DAGScheduler: ResultStage 8 (collect at <stdin>:1) finished in 0.196 s\n",
      "22/03/22 08:09:57 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/03/22 08:09:57 INFO YarnScheduler: Killing all running tasks in stage 8: Stage finished\n",
      "22/03/22 08:09:57 INFO DAGScheduler: Job 5 finished: collect at <stdin>:1, took 0.200414 s\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ip-80-0-11-130.ec2.internal:44219 in memory (size: 14.2 KiB, free: 912.2 MiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ip-80-0-8-132.ec2.internal:41281 in memory (size: 14.2 KiB, free: 9.7 GiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_6_piece0 on ip-80-0-11-130.ec2.internal:44219 in memory (size: 9.0 KiB, free: 912.2 MiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_6_piece0 on ip-80-0-8-132.ec2.internal:41281 in memory (size: 9.0 KiB, free: 9.7 GiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_6_piece0 on ip-80-0-9-217.ec2.internal:36451 in memory (size: 9.0 KiB, free: 2.3 GiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_6_piece0 on ip-80-0-9-217.ec2.internal:38457 in memory (size: 9.0 KiB, free: 4.7 GiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ip-80-0-11-130.ec2.internal:44219 in memory (size: 14.2 KiB, free: 912.2 MiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ip-80-0-9-217.ec2.internal:38457 in memory (size: 14.2 KiB, free: 4.7 GiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ip-80-0-9-217.ec2.internal:36451 in memory (size: 14.2 KiB, free: 2.3 GiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ip-80-0-11-130.ec2.internal:44219 in memory (size: 14.0 KiB, free: 912.3 MiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ip-80-0-9-217.ec2.internal:36451 in memory (size: 14.0 KiB, free: 2.3 GiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ip-80-0-9-217.ec2.internal:38457 in memory (size: 14.0 KiB, free: 4.7 GiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ip-80-0-8-132.ec2.internal:41281 in memory (size: 14.0 KiB, free: 9.7 GiB)\n",
      "22/03/22 08:09:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(sentiment#44) generates partition filter: ((sentiment.count#259 - sentiment.nullCount#258) > 0)\n",
      "22/03/22 08:09:57 INFO DefaultCachedBatchSerializer: Predicate (sentiment#44 = negative) generates partition filter: ((sentiment.lowerBound#257 <= negative) AND (negative <= sentiment.upperBound#256))\n",
      "22/03/22 08:09:57 INFO SparkContext: Starting job: collect at <stdin>:2\n",
      "22/03/22 08:09:57 INFO DAGScheduler: Got job 6 (collect at <stdin>:2) with 32 output partitions\n",
      "22/03/22 08:09:57 INFO DAGScheduler: Final stage: ResultStage 9 (collect at <stdin>:2)\n",
      "22/03/22 08:09:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/03/22 08:09:57 INFO DAGScheduler: Missing parents: List()\n",
      "22/03/22 08:09:57 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[31] at collect at <stdin>:2), which has no missing parents\n",
      "22/03/22 08:09:57 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 19.7 KiB, free 911.9 MiB)\n",
      "22/03/22 08:09:57 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 911.9 MiB)\n",
      "22/03/22 08:09:57 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ip-80-0-11-130.ec2.internal:44219 (size: 9.0 KiB, free: 912.3 MiB)\n",
      "22/03/22 08:09:57 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1484\n",
      "22/03/22 08:09:57 INFO DAGScheduler: Submitting 32 missing tasks from ResultStage 9 (MapPartitionsRDD[31] at collect at <stdin>:2) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "22/03/22 08:09:57 INFO YarnScheduler: Adding task set 9.0 with 32 tasks resource profile 0\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 100) (ip-80-0-9-217.ec2.internal, executor 3, partition 0, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 12.0 in stage 9.0 (TID 101) (ip-80-0-8-132.ec2.internal, executor 5, partition 12, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 102) (ip-80-0-9-217.ec2.internal, executor 4, partition 4, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 103) (ip-80-0-9-217.ec2.internal, executor 3, partition 1, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 13.0 in stage 9.0 (TID 104) (ip-80-0-8-132.ec2.internal, executor 5, partition 13, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 105) (ip-80-0-9-217.ec2.internal, executor 4, partition 5, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 106) (ip-80-0-9-217.ec2.internal, executor 3, partition 2, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 14.0 in stage 9.0 (TID 107) (ip-80-0-8-132.ec2.internal, executor 5, partition 14, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 6.0 in stage 9.0 (TID 108) (ip-80-0-9-217.ec2.internal, executor 4, partition 6, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 109) (ip-80-0-9-217.ec2.internal, executor 3, partition 3, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 15.0 in stage 9.0 (TID 110) (ip-80-0-8-132.ec2.internal, executor 5, partition 15, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 7.0 in stage 9.0 (TID 111) (ip-80-0-9-217.ec2.internal, executor 4, partition 7, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 16.0 in stage 9.0 (TID 112) (ip-80-0-8-132.ec2.internal, executor 5, partition 16, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 8.0 in stage 9.0 (TID 113) (ip-80-0-9-217.ec2.internal, executor 4, partition 8, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 17.0 in stage 9.0 (TID 114) (ip-80-0-8-132.ec2.internal, executor 5, partition 17, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 9.0 in stage 9.0 (TID 115) (ip-80-0-9-217.ec2.internal, executor 4, partition 9, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()\n",
      "22/03/22 08:09:57 INFO TaskSetManager: Starting task 18.0 in stage 9.0 (TID 116) (ip-80-0-8-132.ec2.internal, executor 5, partition 18, PROCESS_LOCAL, 4888 bytes) taskResourceAssignments Map()"
     ]
    }
   ],
   "source": [
    "%%logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SageMaker Studio sparkmagic kernel, you can train machine learning models in the Spark cluster using the *SageMaker Spark library*. SageMaker Spark is an open source Spark library for Amazon SageMaker. For examples, \n",
    "see [here](https://github.com/aws/sagemaker-spark#example-using-sagemaker-spark-with-any-sagemaker-algorithm)\n",
    "\n",
    "In this notebook however, we will use SageMaker experiments, trial and estimator to train a model and deploy the model using SageMaker realtime endpoint hosting\n",
    "\n",
    "In the next cell, we will install the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting sagemaker-experiments\n",
      "  Downloading sagemaker_experiments-0.1.35-py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 126 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.16.27 in /opt/conda/lib/python3.7/site-packages (from sagemaker-experiments) (1.20.23)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.23 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.23.23)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.16.27->sagemaker-experiments) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.16.27->sagemaker-experiments) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.23->boto3>=1.16.27->sagemaker-experiments) (1.16.0)\n",
      "Installing collected packages: sagemaker-experiments\n",
      "Successfully installed sagemaker-experiments-0.1.35\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Name: sagemaker\n",
      "Version: 2.70.0\n",
      "Summary: Open source library for training and deploying models on Amazon SageMaker.\n",
      "Home-page: https://github.com/aws/sagemaker-python-sdk/\n",
      "Author: Amazon Web Services\n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: attrs, boto3, google-pasta, importlib-metadata, numpy, packaging, pandas, pathos, protobuf, protobuf3-to-dict, smdebug-rulesconfig\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "import sys\n",
    "!{sys.executable} -m pip install sagemaker-experiments \n",
    "!{sys.executable} -m pip show sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import libraries and set global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import sagemaker\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "from time import strftime, gmtime\n",
    "import json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "sess = boto3.Session()\n",
    "region_name = sess.region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_runtime = boto3.Session().client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will create a new S3 bucket that will be used for storing the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "stsclient = boto3.client(\"sts\", region_name=region_name)\n",
    "s3client = boto3.client(\"s3\", region_name=region_name)\n",
    "\n",
    "aws_account_id = stsclient.get_caller_identity()[\"Account\"] \n",
    "bucket = \"sagemaker-studio-pyspark-{}-{}\".format(region_name, aws_account_id)\n",
    "key = \"sentiment/movie_reviews.csv\"\n",
    "smprocessing_input = \"s3://{}/{}\".format(bucket, key)\n",
    "\n",
    "try:\n",
    "    if region_name==\"us-east-1\":\n",
    "           s3client.create_bucket(Bucket=bucket)\n",
    "    else:\n",
    "           s3client.create_bucket(Bucket=bucket, CreateBucketConfiguration={\n",
    "                'LocationConstraint': region_name})\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    message = e.response['Error']['Message']\n",
    "    if error_code == 'BucketAlreadyOwnedByYou':\n",
    "        print ('A bucket with the same name already exists in your account - using the same bucket.')\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Error->{}:{}\".format(error_code, message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the following variables to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'bucket' as 'bucket' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i bucket -t str -n bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'key' as 'key' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i key -t str -n key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the spark dataframe received by querying the hive table (using the sqlContext.sql above) to a pandas dataframe and upload the data to the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Pandas >= 0.23.2 must be installed; however, it was not found.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py\", line 62, in toPandas\n",
      "    require_minimum_pandas_version()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/utils.py\", line 34, in require_minimum_pandas_version\n",
      "    \"it was not found.\" % minimum_pandas_version) from raised_error\n",
      "ImportError: Pandas >= 0.23.2 must be installed; however, it was not found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_reviews_df = movie_reviews.toPandas()\n",
    "import boto3\n",
    "from io import StringIO\n",
    "csv_buffer = StringIO()\n",
    "movie_reviews_df.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, key).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data and feature engineering\n",
    "\n",
    "#### Amazon SageMaker Processing jobs using the Scikit-learn Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process data and feature engineering\n",
    "Amazon SageMaker Processing jobs using the Scikit-learn Processor\n",
    "With Amazon SageMaker Processing jobs, you can leverage a simplified, managed experience to run data pre- or post-processing and model evaluation workloads on the Amazon SageMaker platform.\n",
    "\n",
    "A processing job downloads input from Amazon Simple Storage Service (Amazon S3), then uploads outputs to Amazon S3 during or after the processing job.\n",
    "\n",
    "The cell below shows how to run scikit-learn scripts using a Docker image provided and maintained by SageMaker to preprocess data.\n",
    "\n",
    "Note: We will use a \"ml.m5.xlarge\" instance as the instance type for sagemaker processing, training and model hosting. If you don't have access to this instance type and see a \"ResourceLimitExceeded\" error, use another instance type that you have access to. You can also request a service limit increase using AWS Support Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "instance_type_smprocessing=\"ml.m5.xlarge\"\n",
    "instance_type_smtraining=\"ml.m5.xlarge\"\n",
    "instance_type_smendpoint=\"ml.m5.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type=instance_type_smprocessing,\n",
    "                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-studio-pyspark-us-east-1-763050260265/sentiment/movie_reviews.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "code preprocessing.py wasn't found. Please make sure that the file exists.\n                    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5d8215392d7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                ProcessingOutput(output_name='validation_data',\n\u001b[1;32m     11\u001b[0m                                                 source='/opt/ml/processing/validation')],\n\u001b[0;32m---> 12\u001b[0;31m                       \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'--train-test-split-ratio'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0.2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                      )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, code, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         )\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36m_normalize_args\u001b[0;34m(self, job_name, arguments, inputs, outputs, code, kms_key)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_current_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0minputs_with_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_include_code_in_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkms_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mnormalized_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_with_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkms_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mnormalized_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36m_include_code_in_inputs\u001b[0;34m(self, inputs, code, kms_key)\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0mcode\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mProcessingInput\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \"\"\"\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0muser_code_s3_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_user_code_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkms_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0muser_script_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_user_code_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36m_handle_user_code_url\u001b[0;34m(self, code, kms_key)\u001b[0m\n\u001b[1;32m    622\u001b[0m                     \"\"\"code {} wasn't found. Please make sure that the file exists.\n\u001b[1;32m    623\u001b[0m                     \"\"\".format(\n\u001b[0;32m--> 624\u001b[0;31m                         \u001b[0mcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m                     )\n\u001b[1;32m    626\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: code preprocessing.py wasn't found. Please make sure that the file exists.\n                    "
     ]
    }
   ],
   "source": [
    "%%local\n",
    "print(smprocessing_input)\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code='preprocessing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=smprocessing_input,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='validation_data',\n",
    "                                                source='/opt/ml/processing/validation')],\n",
    "                      arguments=['--train-test-split-ratio', '0.2']\n",
    "                     )\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'validation_data':\n",
    "        preprocessed_validation_data = output['S3Output']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "print(preprocessed_training_data)\n",
    "print(preprocessed_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "prefix = 'blazingtext/supervised' \n",
    "s3_train_data = preprocessed_training_data\n",
    "s3_validation_data = preprocessed_validation_data\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a SageMaker model\n",
    "#### Amazon SageMaker Experiments\n",
    "\n",
    "Amazon SageMaker Experiments allows us to keep track of model training; organize related models together; and log model configuration, parameters, and metrics to reproduce and iterate on previous models and compare models. \n",
    "Let's create the experiment, trial, and train the model. To reduce cost, the training code below uses spot instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "sm_session = sagemaker.session.Session()\n",
    "\n",
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "sentiment_experiment = Experiment.create(experiment_name=\"sentimentdetection-{}\".format(create_date), \n",
    "                                              description=\"Detect sentiment in text\", \n",
    "                                              sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "trial = Trial.create(trial_name=\"sentiment-trial-blazingtext-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())), \n",
    "                     experiment_name=sentiment_experiment.experiment_name,\n",
    "                     sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "train_use_spot_instances = False\n",
    "train_max_run=3600\n",
    "train_max_wait = 3600 if train_use_spot_instances else None\n",
    "\n",
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         instance_count=1, \n",
    "                                         instance_type=instance_type_smtraining,\n",
    "                                         volume_size = 30,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sm_session,\n",
    "                                         use_spot_instances=train_use_spot_instances,\n",
    "                                         max_run=train_max_run,\n",
    "                                         max_wait=train_max_wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.005328,\n",
    "                            vector_dim=286,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "train_data = sagemaker.inputs.TrainingInput(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.inputs.TrainingInput(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "%%time\n",
    "\n",
    "bt_model.fit(data_channels, \n",
    "             experiment_config={\n",
    "                      \"ExperimentName\": sentiment_experiment.experiment_name, \n",
    "                      \"TrialName\": trial.trial_name,\n",
    "                      \"TrialComponentDisplayName\": \"BlazingText-Training\",\n",
    "                  },\n",
    "             logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model and get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "text_classifier = bt_model.deploy(initial_instance_count = 1, instance_type = instance_type_smendpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "review = [\"please give this one a miss br br kristy swanson and the rest of the cast\"\n",
    "          \"rendered terrible performances the show is flat flat flat br br\"\n",
    "          \"i don't know how michael madison could have allowed this one on his plate\"\n",
    "          \"he almost seemed to know this wasn't going to work out\"\n",
    "          \"and his performance was quite lacklustre so all you madison fans give this a miss\"]\n",
    "tokenized_review = [' '.join(t.split(\" \")) for t in review]\n",
    "#For retrieving the top k predictions, you can set k in the configuration\n",
    "payload = {\"instances\" : tokenized_review}\n",
    "bt_endpoint_name = text_classifier.endpoint_name\n",
    "response = sm_runtime.invoke_endpoint(EndpointName=bt_endpoint_name,\n",
    "                                      ContentType = 'application/json',\n",
    "                                      Body=json.dumps(payload))\n",
    "output = json.loads(response['Body'].read().decode('utf-8'))\n",
    "#make the output readable \n",
    "import copy\n",
    "predictions = copy.deepcopy(output) \n",
    "for output in predictions:\n",
    "    output['label'] = output['label'][0][9:].upper() \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "#Clean up resources created as part of this notebook\n",
    "#delete endpoint\n",
    "text_classifier.delete_endpoint()\n",
    "# empty s3 bucket we created\n",
    "s3_bucket_to_remove = \"s3://{}\".format(bucket)\n",
    "!aws s3 rm {s3_bucket_to_remove} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cleanup -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "PySpark (SparkMagic)",
   "language": "python",
   "name": "pysparkkernel__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-sparkmagic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
